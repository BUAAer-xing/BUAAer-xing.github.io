---
authors:
    name: BUAAer-xing
    title: 北航HPC硕士生
    url: https://github.com/BUAAer-xing
    image_url: https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/icon.png
---

# 在主机中定义复杂结构体指针无法通过内存拷贝直接到设备内存中

## 错误代码

结构体的定义：

```cpp
// 通过CSR格式进行矩阵存储元素的结构体
struct CSRmatrix
{
    int *row_ptr;     // 指向每一行第一个非零元素在values中的位置的数组
    int *col_indices; // 指向每个非零元素的列索引的数组
    double *values;   // 存储所有非零元素值的数组
    int numRows;      // 矩阵的行数
    int numCols;      // 矩阵的列数
    int numNonzeros;  // 非零元素的总数
};
```

主体代码：

```cpp
#include <iostream> 
#include <vector> 
#include <string> 
#include <hip/hip_runtime.h>

#include "cx_utils/cx_utils.h"

using namespace std;

__global__ void spmv_kernel(CSRmatrix *csrMatrix, double *vec, double *res) { 
	int i = blockIdx.x * blockDim.x + threadIdx.x; 
	if (i < csrMatrix->numRows) { 
		res[i] = 0; 
		for (int j = csrMatrix->row_ptr[i]; j < csrMatrix->row_ptr[i + 1]; j++) {
			 res[i] = res[i] + csrMatrix->values[j] * vec[csrMatrix->col_indices[j]]; 
		 }
	} 	
}

int main() {

	double current_time = get_time();
	
	string filename = "../data/1138_bus.mtx"; // 假设你有一个名为 matrix.mtx 的文件
	vector<COOmatrix> coo_mtx;
	int numRows, numCols, nonzeros;
	
	if (read_mtx(filename, coo_mtx, numRows, numCols, nonzeros))
	{
	    cout << "矩阵读取成功！" << endl;
	    cout << "矩阵尺寸: " << numRows << " x " << numCols << ", 非零元素数量: " << nonzeros << endl;
	
	    CSRmatrix csr_mtx = convert_COO_to_CSR(coo_mtx, numRows, numCols);
	
	    double h_vec[numCols];
	    double h_res[numRows];
	    double *d_vec, *d_res;
	    CSRmatrix *d_csr;
	
	    for (int i = 0; i < numCols; i++)
	    {
	        h_vec[i] = 1.0;
	    }
	
	    hipMalloc(&d_vec, numCols * sizeof(double));
	    hipMalloc(&d_res, numRows * sizeof(double));
	    hipMalloc(&d_csr, sizeof(CSRmatrix));
	    hipMemcpy(d_vec, h_vec, numCols * sizeof(double), hipMemcpyHostToDevice);
	    hipMemcpy(d_csr, &csr_mtx, sizeof(CSRmatrix), hipMemcpyHostToDevice);
	
	    int blockSize = 256;
	    int gridSize = (numRows + blockSize - 1) / blockSize;
	
	    spmv_kernel<<<gridSize, blockSize>>>(d_csr, d_vec, d_res);
	
	    hipDeviceSynchronize();
	
	    hipMemcpy(h_res, d_res, numRows * sizeof(double), hipMemcpyDeviceToHost);
	
	    for (int i = 0; i < numRows; i++)
	    {
	        cout << h_res[i] << endl;
	    }
	
	    hipFree(d_vec);
	    hipFree(d_res);
	}
	return 0;
}
```

### 错误输出

```shell
Invalid address access: 0x7ffc45259000, Error code: 1. 
Aborted
```


### 错误原因

在CUDA或HIP编程中，遇到无效地址错误通常是因为**试图访问未正确分配或传递到设备（GPU）的内存**。问题很可能出现在如何处理`CSRmatrix`结构及其成员的内存分配和传递上。

1. `CSRmatrix`结构体的定义中包含四个主要成员：`numRows`（矩阵的行数），`row_ptr`（行指针数组），`values`（非零值数组），和`col_indices`（列索引数组）。
2. 当我们将`CSRmatrix`的一个实例`csr_mtx`从主机内存复制到了设备内存（`d_csr`）中时，`CSRmatrix`内部的指针（如`row_ptr`，`values`和`col_indices`）指向的数据实际上还在主机内存中（也就是说，在拷贝时，只会拷贝结构体中的具体数据，对结构体中指针指向的数据并不会进行拷贝）。这意味着，当GPU试图通过这些指针访问数据时，会遇到无效地址错误，因为这些指针对设备来说没有意义。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240229223613.png)


### 解决方案

1. **单独为`CSRmatrix`中的每个指针成员分配设备内存**。这包括`row_ptr`，`values`，和`col_indices`。
2. **将这些数组从主机复制到设备内存**。

## 更改后的代码

```cpp
#include <iostream>
#include <vector>
#include <string>
#include <hip/hip_runtime.h>

#include "cx_utils/cx_utils.h"

using namespace std;

__global__ void spmv_kernel(int *row_ptr, int *col_indices, double *values, double *vec, double *res, int *numRows)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < *numRows)
    {
        double sum = 0.0;
        for (int j = row_ptr[i]; j < row_ptr[i + 1]; j++)
        {
            sum += values[j] * vec[col_indices[j]];
        }
        res[i] = sum;
    }
}

int main()
{

    double current_time = get_time();

    string filename = "../data/1138_bus.mtx"; // 假设你有一个名为 matrix.mtx 的文件
    vector<COOmatrix> coo_mtx;
    int numRows, numCols, nonzeros;

    if (read_mtx(filename, coo_mtx, numRows, numCols, nonzeros))
    {
        cout << "矩阵读取成功！" << endl;
        cout << "矩阵尺寸: " << numRows << " x " << numCols << ", 非零元素数量: " << nonzeros << endl;

        CSRmatrix csr_mtx = convert_COO_to_CSR(coo_mtx, numRows, numCols);

        double h_vec[numCols];
        double h_res[numRows];
        double *d_vec, *d_res;
        int *d_row_ptr, *d_col_indices, *d_numRows;
        double *d_values;

        for (int i = 0; i < numCols; i++)
        {
            h_vec[i] = 1.0;
        }

        hipMalloc(&d_vec, numCols * sizeof(double));
        hipMalloc(&d_res, numRows * sizeof(double));
        hipMalloc(&d_row_ptr, (numRows + 1) * sizeof(int));
        hipMalloc(&d_col_indices, nonzeros * sizeof(int));
        hipMalloc(&d_values, nonzeros * sizeof(double));
        hipMalloc(&d_numRows, sizeof(int));

        hipMemcpy(d_vec, h_vec, numCols * sizeof(double), hipMemcpyHostToDevice);
        hipMemcpy(d_row_ptr, csr_mtx.row_ptr, (numRows + 1) * sizeof(int), hipMemcpyHostToDevice);
        hipMemcpy(d_col_indices, csr_mtx.col_indices, nonzeros * sizeof(int), hipMemcpyHostToDevice);
        hipMemcpy(d_values, csr_mtx.values, nonzeros * sizeof(double), hipMemcpyHostToDevice);
        hipMemcpy(d_numRows, &numRows, sizeof(int), hipMemcpyHostToDevice);
        
        int blockSize = 256;
        int gridSize = (numRows + blockSize - 1) / blockSize;

        spmv_kernel<<<gridSize, blockSize>>>(d_row_ptr, d_col_indices, d_values, d_vec, d_res, d_numRows);

        hipDeviceSynchronize();

        hipMemcpy(h_res, d_res, numRows * sizeof(double), hipMemcpyDeviceToHost);

        for (int i = 0; i < numRows; i++)
        {
            cout << h_res[i] << endl;
        }

        hipFree(d_vec);
        hipFree(d_res);
    }
    return 0;
}


```
