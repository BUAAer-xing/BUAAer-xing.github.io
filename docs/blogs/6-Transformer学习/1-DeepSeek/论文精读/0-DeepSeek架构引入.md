
## 1-DeepSeek LLM

该论文介绍了 DeepSeek LLM 系列模型，旨在通过系统研究扩展法则（scaling laws）来优化模型训练。研究表明，<font color='red'><b>合理的模型与数据规模配比对于提升性能至关重要</b></font>。DeepSeek LLM 67B 模型在多个基准测试中表现优异，尤其在代码、数学和推理任务上超过了 LLaMA-2 70B，并在开放式评估中优于 GPT-3.5。

**核心贡献**：探索大语言模型的缩放规律，提出基于长期主义的开源模型优化方案。
- **架构设计**：
    - 基于LLaMA架构改进，采用Pre-Norm、SwiGLU激活函数和RoPE位置编码，67B版本引入分组查询注意力（GQA）以降低推理成本158。
    - 分层设计：7B模型30层，67B模型95层，通过深度调整增强语义捕捉能力。
- **训练优化**：
    - 多阶段学习率调度器（预热-稳态-分阶衰减），结合梯度裁剪提升稳定性。
    - 使用2万亿token的高质量双语数据集，通过去重、过滤和混合策略优化数据分布。
- **性能表现**：
    - 67B模型在代码、数学和推理任务中超越LLaMA-2 70B，中文开放性评估接近GPT-4。
    - 通过监督微调（SFT）和直接偏好优化（DPO）对齐后，聊天模型在MT-Bench中媲美GPT-3.5。

## 2-DeepSeek MoE

该论文提出了 DeepSeekMoE 架构，旨在实现专家模块的极致专化。<font color='red'><b>通过细粒度的专家划分和共享专家隔离策略，模型在保持性能的同时显著减少了计算资源的消耗</b></font>。实验结果显示，DeepSeekMoE 在相同计算预算下优于传统的 MoE 架构，如 GShard。

**核心贡献**：提出高性能混合专家架构（DeepSeekMoE），解决传统MoE的知识冗余问题。
- **架构创新**：
    - **细粒度专家分割**：将单个专家拆分为更小的子专家（如将FFN中间层维度缩减为1/m），提升知识专业化程度。
    - **共享专家隔离**：固定部分专家始终激活，捕捉跨上下文通用知识，减少冗余。
- **训练效率**：
    - 对比传统MoE，参数利用率更高，训练成本降低42.5%，KV缓存减少93.3%。
    - 支持动态路由策略（如贪心选择专家组），优化资源分配。


## 3-DeepSeek V2

DeepSeek V2 是一个具有 2360 亿总参数的稀疏激活模型，每个 token 激活 210 亿参数，支持最长 128K 的上下文长度。<font color='red'><b>该模型引入了多头潜在注意力机制（MLA）和 DeepSeekMoE 架构，实现了高效的推理和经济的训练</b></font>。与 DeepSeek 67B 相比，V2 在性能提升的同时，训练成本降低了 42.5%，KV 缓存减少了 93.3%，生成吞吐量提高了 5.76 倍。

**核心贡献**：实现经济高效的MoE模型，兼顾性能与推理成本。
- **架构亮点**：
    - **MLA注意力机制**：通过低秩键值压缩减少KV缓存至同级别密集模型的1/5-1/100，支持128K长上下文。
    - **DeepSeekMoE**：总参数量2360亿，每token激活21亿参数，引入细粒度专家分割与共享专家。
- **性能与成本**：
    - 训练成本为GPT-4的1/20，生成吞吐量提升5.76倍，中文综合能力（AlignBench）超越LLaMA。
    - API定价仅为GPT-4 Turbo的1/100，推动大模型价格战。


## 4-DeepSeek V3

DeepSeek V3 将模型规模扩展至 6710 亿参数，每个 token 激活 370 亿参数，训练数据量达到 14.8 万亿 token。在继承 V2 架构的基础上，<font color='red'><b>V3 引入了无辅助损失的负载均衡策略和多 token 预测训练目标，进一步提升了模型性能</b></font>。此外，采用 <font color='red'><b>FP8 混合精度训练和高性能计算协同设计</b></font>，实现了成本效益的训练过程。

**核心贡献**：开源模型的性能逼近闭源模型，突破训练成本限制。
- **技术创新**：
    - **多词元预测（MTP）**：通过预测多个未来token提升训练效率，生成速度从20 TPS提升至60 TPS。
    - **无辅助负载均衡**：动态监控专家负载，避免资源倾斜39。
- **训练优化**：
    - 使用14.8万亿token预训练，结合<font color='red'><b>FP8混合精度与DualPipe流水线并行</b></font>，总训练成本557万美元（仅需2048块H800 GPU）。
- **性能表现**：
    - 数学任务（如MATH-500）得分90.2，超越Qwen 2.5-72B；中文事实性知识测试领先闭源模型39。

## 5-DeepSeek R1

DeepSeek R1 是一个专注于推理能力的模型，<font color='red'><b>采用纯强化学习（RL）方法进行训练，完全不依赖监督数据</b></font>。训练过程分为四个阶段，包括冷启动、面向推理的强化学习、拒绝采样与监督微调，以及最终的模型蒸馏。实验表明，R1 在数学、代码和推理任务上表现出色，性能可与 OpenAI 的 o1 模型相媲美，且训练成本仅为后者的 1/20。此外，R1 的训练方法促进了小型高性能模型的开发，推动了高效 AI 模型的研究方向。

**核心贡献**：首次验证纯强化学习（RL）可独立提升推理能力，无需监督微调（SFT）。
- **训练方法**：
    - **纯RL训练（R1-Zero）**：通过组相对策略优化（GRPO）和自我博弈，在AIME基准测试中pass@1达71%。
    - **多阶段RL（R1）**：结合冷启动SFT与强化学习，解决可读性问题，MATH-500得分97.3%。
- **模型扩展**：
    - 支持蒸馏技术，将推理能力迁移至小模型（如Qwen-7B），在资源受限场景下表现优异。
    - 开源MIT许可证，提供GGML、GGUF等多种格式，支持本地部署与API调用。

