
## 1-DeepSeek产生背景

从第一天的FlashMLA，到第二天的DeepEP到今天的DeepGEMM，**这些工作都是基于一款特定的英伟达-H800**，H的来源于Hopper架构。而这款GPU的特殊之处就在于它是<font color='red'><b>中国特供</b></font>的，而这个特供可不是什么好词，而是因为美国芯片限制法案。

而国外的AI大厂情况完全不一样，完全没有禁售，可以说Nvidia造多少他们能买到多少。

| 公司      | 主力模型                  | 主力芯片                                                  |
|-----------|----------------------------|-----------------------------------------------------------|
| OpenAI    | ChatGPT / GPT-4             | [Nvidia A100](https://www.nvidia.com/en-us/data-center/a100/) & H100 GPUs |
| Anthropic | Claude                     | Nvidia GPUs (A100/H100)                                   |
| xAI       | Grok (and future iterations) | [Nvidia H100](https://www.nvidia.com/en-us/data-center/h100/) GPUs (with planned expansion to H200/Blackwell) |
| Google    | Gemini                     | Google's in-house TPU                                     |

他们在这种弹药充足的情况下，根本不会想着去压榨英伟达性能，而是加大pre-train投入，比如Grok3，世界上目前GPU用量最大的模型（貌似是20万块），硬件投入是DeepSeek的数倍，性能提升并不明显。快是真的快，**不过卡只要多，谁都快到飞起**。

<center> <font > <font color='red' size='5'><b>DeepSeek最大程度的压榨英伟达显卡的性能</b></font> </font> </center>

而DeepSeek是真不一样，开源了三套专门针对于阉割版H800的性能优化方法，
- FlashMLA 能在显存有限的情况下高效处理长文
- DeepEP 则类似超高速的网络，让多台电脑迅速协同工作
- 而 DeepGEMM 则像一个极简但强大的超级计算器，能够快速完成大规模数学运算，从而大幅提升 AI 模型的训练和推理速度
<font color='red'><b><center> <font face='微软雅黑' size='5' color='red'> 未来对于软硬件皆通的人才需求会更大 </font> </center></b></font>
第二个和第三个工作的核心都是在压榨英伟达的性能，而纯粹搞大模型的人其实对一个领域并不是很了解。而只有这种，**既懂AI大模型，又懂显卡这种硬件的复合型人才**，才是这一波大模型爆发的关键先生。可以说真正做到了比英伟达还要懂英伟达，英伟达这家公司的确伟大，但重要的是它就是个卖铲子的，而铲子怎么用，如何优化，他们可能在未来要更多的仰仗DeepSeek这样的公司。

因为对于英伟达来说，OpenAI这类型公司其实对于芯片的态度就是不够就买，很少会从底层去优化，真的没必要。而DeepSeek这样的公司，**软硬都搞，几个算法和优化下来，1万块芯片甚至可以当几万，甚至10万块**，真的是对于AGI的超级加速。


## 1-五大项目贡献总结

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250429143258.png)

| 天数    | 项目名称            | 核心贡献与特点                                                                                         |
| ----- | --------------- | ----------------------------------------------------------------------------------------------- |
| Day 1 | FlashMLA        | 针对 NVIDIA Hopper GPU 优化的高性能解码内核；支持 BF16 与分块 KV 缓存，实现高内存带宽（3000 GB/s）与高算力（580 TFLOPS），专注于处理交长序列。 |
| Day 2 | DeepEP          | 专为 MoE（混合专家）模型设计的高性能通信库；利用 FP8 压缩与 NVLink 技术，实现节点间外高带宽传输，降低通信延迟，为大规模训练提供支持。                     |
| Day 3 | DeepGEMM        | 基于 FP8 的通用矩阵乘法库，仅用 300 行代码实现；支持常规密集乘运算和 MoE 模型分组计算，有效提升运算效率，突破传统工程库的性能瓶颈。                       |
| Day 4 | DualPipe & EPLB | 分布式训练优化方案：DualPipe 实现双向流水线（“一边做饭一边洗碗”），EPLB 实现专家载均衡，显著减少训练中的“气泡”时间，提高整体训练效率。                    |
| Day 5 | 3FS 文件系统        | 高性能分布式文件系统，通过 SSD+RDMA 技术实现高吞吐（6.6 TiB/s）与低延迟数据存取，保障数据一致性，为大数据处理和 AI 推理提供支持。                    |

## 2-FlashMLA

FlashMLA 是 DeepSeek 推出的高效解码内核，专为 NVIDIA Hopper 架构（如 H100/H800）GPU 优化，旨在加速大语言模型（LLM）在自回归解码阶段的推理性能，特别适用于处理可变长度序列的场景。

<font color='red'><b><center> <font color='red' size='5'> 通过手撸CUDA和PTX的Kernel来实现MLA，相当于是直接优化的MLA kernel，当然也可以说是算子融合！！！ </font> </center></b></font>

其主要优化措施包括：
1. **低秩分解的多头潜在注意力机制（MLA）**
	- FlashMLA 引入了低秩分解的 MLA 机制，通过对 Key 和 Value 进行低维压缩，显著减少了 KV 缓存的内存占用，降低了计算复杂度，提升了长序列处理的效率。
2. **分页式 KV 缓存机制**
	- 采用块大小为 64 的分页 KV 缓存，有效解决了传统 KV 缓存的内存碎片化问题，提高了显存利用率，支持高效处理不同长度的序列数据。
3. **针对 Hopper GPU 的深度优化**
	- FlashMLA 充分利用 Hopper 架构的高带宽内存和 Tensor Core，结合 CUDA 核心的优化，实现了高达 3000 GB/s 的内存带宽和 580 TFLOPS 的计算性能，显著提升了推理效率。
4. **支持 BF16 精度计算**
	- 通过支持 BF16 精度，FlashMLA 在保持计算准确度的同时，降低了内存带宽压力，提高了计算效率。
5. **内核级的调度优化**
	- 在新版本中，FlashMLA 通过重构内核调度策略，实现了 CUDA 核心与 Tensor Core 操作的重叠执行，以及内存访问与计算的并行，进一步提升了计算资源的利用率。

这些优化措施使得 FlashMLA 在处理长序列和可变长度输入时，显著降低了延迟和资源占用，提升了推理性能，特别适用于聊天机器人、文档分析和实时翻译等应用场景。

---
**KV Cache**

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250430155436.png)

$$
\text{Attention}(Q_t, K_{1:t}, V_{1:t}) = \text{softmax}\left( \frac{Q_t K_{1:t}^T}{\sqrt{d_k}} \right) V_{1:t}
$$
----
**结构上：多头注意力的 QKV 是如何计算的？**

假设模型隐藏维度为 $d_{model}$，有 h 个注意力头，每个头的维度是 $d_{head} = d_{model} / h$。那么，对于每个输入 X，有：
$$\begin{aligned}
Q_h &= X W_h^Q, \\
K_h &= X W_h^K, \\
V_h &= X W_h^V, \quad \text{for } h = 1,…,H
\end{aligned}$$

所有头的输出被拼接后再经过输出投影：

$$\text{MultiHead}(X) = \text{Concat}(\text{head}_1,…,\text{head}_h) W^O$$

因此：
	•	每个头拥有一组独立的 QKV 投影权重；
	•	每个头计算自己的 Q、K、V；
	•	每个头的注意力是独立计算的；

----

在 Transformer 架构中，KV Cache（Key-Value Cache）是一种优化自回归推理性能的关键技术。在生成任务中，模型逐步生成每个 token，每一步都需要计算当前 token 与之前所有 token 的注意力关系。**由于之前 token 的键（Key）和值（Value）在后续步骤中保持不变，重复计算这些值会导致冗余计算和资源浪费。KV Cache 通过缓存这些键和值，避免了重复计算，从而提升了推理效率**。

具体而言，KV Cache 的工作流程包括两个阶段：
1. **预填充阶段（Prefill Phase）**：在生成第一个 token 时，模型计算并缓存输入序列中所有 token 的键和值。
2. **解码阶段（Decoding Phase）**：对于每个新生成的 token，模型仅需计算其对应的键和值，并将其追加到缓存中。随后，模型使用当前 token 的查询（Query）与缓存中的键和值进行注意力计算，生成下一个 token。
这种缓存机制显著减少了每步生成所需的计算量，提高了推理速度。例如，在使用 GPT-2 模型生成 1000 个 token 时，启用 KV Cache 的平均推理时间为约 11.9 秒，而禁用 KV Cache 时则为约 56.2 秒，显示出明显的性能提升 。

然而，KV Cache 也带来了一些挑战，**特别是在处理长序列和大批量数据时，缓存的键和值会占用大量显存，限制了模型的扩展性**。为此，研究人员提出了多种优化策略，如多查询注意力（MQA）、分组查询注意力（GQA）、跨层注意力（CLA）和多层键值共享（MLKV）等，这些方法通过共享或压缩键和值，减少了缓存的内存占用，同时保持了模型的性能 。

存储对应硬件位置：**KV Cache 在硬件上对应的是显存中的连续内存区域，通常以层级（per-layer）分组管理，通过高效的内存布局和低精度存储格式，配合 Tensor Core 高效执行矩阵运算，以支撑高速、自回归推理任务的需要**

---

## 3-DeepEP

DeepEP 是 DeepSeek 团队为**混合专家模型**（MoE, Mixture of Experts）和**专家并行**（EP, Expert Parallelism）场景设计的高性能通信库，<font color='red'><b>旨在解决大规模分布式训练和推理中的通信瓶颈问题</b></font>。其优化措施主要体现在以下几个方面：

- 首先，DeepEP 提供了高效的 **All-to-All 通信内核**，支持节点内的 NVLink 和节点间的 RDMA 通信，显著提升了数据传输效率。在实际测试中，单节点 NVLink 带宽利用率超过 95%，跨节点 RDMA 延迟仅为 163 微秒，极大地减少了通信延迟 。
- 其次，DeepEP 支持低精度数据类型，如 FP8 和 BF16，降低了通信数据量，进一步提升了通信效率。此外，库中引入了<font color='red'><b>基于 Hook 的通信与计算重叠机制</b></font>，允许在不占用 GPU 流式多处理器（SM）资源的情况下，实现通信和计算的并行执行，提高了整体计算资源的利用率 。
- 在内核优化方面，DeepEP 针对非对称带宽转发场景（如 NVLink 到 RDMA）进行了深度优化，确保在不同通信路径下都能达到高性能。同时，库中还使用了未公开的 PTX 指令 ld.global.nc.L1::no_allocate.L2::256B，**通过绕过 L1 缓存并直接访问 L2 缓存，以 256 字节的事务大小加载数据，进一步提升了内存访问效率** 。
- 在实际应用中，DeepEP 显著提升了 MoE 模型的训练和推理效率。例如，在 H800 GPU 上，使用 DeepEP 的常规内核进行训练时，吞吐量达到了 153 GB/s，接近 NVLink 的理论带宽上限；而在推理阶段，使用纯 RDMA 的低延迟内核，端到端延迟降至微秒级，带宽达到 46 GB/s，接近 RDMA 的理论极限 。

DeepEP 通过高效的通信内核、低精度支持、通信与计算重叠机制以及深度的内核优化，极大地提升了 MoE 模型在大规模分布式环境下的训练和推理效率，为实现高性能的专家并行计算提供了有力支持。

---
**All-to-All通信内核对MoE模型的必要性**

在大规模分布式训练，尤其是**MoE（Mixture of Experts，混合专家）模型**中，**All-to-All 通信内核**是必须的：

在 MoE 模型中，每个 token（或小批次 token）根据路由器（Router）的决策，只会激活少量专家（比如 Top-1 或 Top-2 选择）。这些专家通常跨越多个 GPU 分布。当输入 token 被分配给不同的专家时，为了进行正确的专家计算，**需要把每个 GPU 上属于其他 GPU 上专家的数据发送过去**；同样，处理完后结果还要**再发回对应的 GPU**。这就自然形成了一个典型的 **All-to-All 通信模式**。

具体来说：
- **输入阶段（Dispatch）**：每个 GPU 持有的 token 需要根据路由决策，发送到其他 GPU 上对应的专家处理单元。
- **输出阶段（Gather）**：各个 GPU 处理完自己负责的专家计算后，再把结果按 token 顺序归并回原来的 GPU。 

这一过程就需要执行一次完整的 **All-to-All 通信**，即每个参与节点都需要同时向所有其他节点发送和接收数据。

在这种背景下，为什么一定要专门的 All-to-All 内核呢？主要原因是：
1. **通信量巨大且频繁**：在大规模 MoE 中，每次推理或训练步都需要交换大量激活 token 的特征向量，带宽和延迟直接决定整体吞吐率。
2. **通信模式高度稠密**：每个 GPU 与所有其他 GPU 都有通信需求，简单的点对点（P2P）拷贝或广播（Broadcast）根本无法满足这种稠密全连接通信需求，必须使用 All-to-All 机制。
3. **需要极致优化通信带宽与延迟**：如果 All-to-All 通信没有足够高效，会导致计算单元等待数据，形成**通信瓶颈**，整体训练和推理吞吐量严重下降。因此，通信内核必须充分利用硬件资源（NVLink、PCIe、RDMA），并实现计算与通信的高度重叠。
4. **负载不均问题**：MoE 特有的“稀疏激活”导致数据量不均匀，All-to-All 内核还需要能够处理负载不平衡（Load Imbalance），否则会出现部分 GPU 空闲、部分 GPU 拥堵，进一步降低利用率。
5. **跨节点扩展性要求**：现代大模型（如 DeepSeekMoE-100B 量级）常常跨越数十甚至上百台服务器（节点），All-to-All 内核必须支持跨节点的高速通信，且在大规模环境下保持可扩展性，否则训练和推理的效率无法线性扩展。

因此，**需要 All-to-All 通信内核，是因为 MoE 架构的激活模式天然要求全节点间的大规模、低延迟、高带宽的数据交换。没有高效的 All-to-All 通信，MoE 模型的性能优势就无法发挥**。

---

## 4-DeepGEMM

DeepGEMM 是 DeepSeek 团队为 NVIDIA Hopper 架构（如 H100/H800）GPU 优化的高性能 <font color='red'><b>FP8 通用矩阵乘法</b></font>（GEMM）库，旨在提升大规模 AI 模型，特别是混合专家（MoE）模型的训练和推理效率。其主要优化措施包括：
- 首先，DeepGEMM 采用了轻量级的 Just-In-Time（JIT）编译机制，在运行时根据具体的矩阵形状动态生成和优化 CUDA 内核，避免了传统模板库的复杂性，提高了开发和部署的灵活性。该库仅包含约 300 行核心内核代码，便于理解和维护。
- 其次，为了充分利用 Hopper 架构的 Tensor Core，**DeepGEMM 实现了双层累加机制，结合 CUDA 核心和 Tensor Core 的计算能力，提升了 FP8 低精度计算的数值稳定性和准确性**。此外，库中还引入了精细化的缩放策略，对每 128 通道进行独立缩放，进一步增强了低精度计算的鲁棒性。
- 在内核优化方面，DeepGEMM 对 **FFMA（Fused Multiply-Add）** 指令进行了深入分析和调整，通过修改 <font color='red'><b>SASS 汇编</b></font>中的 yield 和 reuse 位，实现了指令级的调度优化，提升了内核的执行效率。此外，**库中还采用了 Tensor Memory Accelerator（TMA）、软件流水线和 Warp 专用化**等技术，最大化地利用了内存带宽和计算资源。
- 针对 MoE 模型的特点，DeepGEMM 支持分组 GEMM 操作，包括连续分组（contiguous-grouped）和掩码分组（masked-grouped）等形式，优化了小批量矩阵乘法的性能，提升了专家模型的计算效率。在实际测试中，DeepGEMM 在 H800 GPU 上实现了高达 1550 TFLOPS 的计算性能，超过了许多手工优化的库。

DeepGEMM 通过 JIT 编译、双层累加、精细化缩放、指令级调度优化和 MoE 特化支持等多项优化措施，显著提升了 FP8 低精度矩阵乘法的性能和稳定性，为大规模 AI 模型的高效训练和推理提供了有力支持。


## 5-DualPipe&EPLB

DualPipe 和 EPLB 是 DeepSeek 团队为提升大规模模型（如 DeepSeek-V3）训练效率而设计的两项关键优化策略。
- **DualPipe** 旨在通过双向流水线并行减少计算和通信的空闲时间（即“气泡”）
- **EPLB** 则通过智能负载均衡策略优化专家模型（MoE）的资源分配。

### 5.1-DualPipe：双向流水线并行优化

传统的流水线并行（如 1F1B）在执行前向传播和反向传播时存在明显的空闲时间，导致 GPU 资源未被充分利用。DualPipe 引入了<font color='red'><b>双向流水线并行机制</b></font>，使得前向和反向计算可以在不同阶段同时进行，从而<font color='red'><b>实现计算与通信的完全重叠</b></font>，显著**减少了流水线中的“气泡”时间** 。

具体而言，DualPipe 的优化措施包括：
- **双向调度机制**：允许模型的前向传播和反向传播在流水线的不同阶段同时进行，提高了 GPU 的利用率。
- **计算与通信的重叠**：通过精细的任务调度，使得计算和通信可以并行执行，减少了等待时间。
- **内存优化**：尽管 DualPipe 增加了激活值的存储需求，但通过优化内存管理，确保了整体内存使用的高效性 。

### 5.2-EPLB：专家并行负载均衡器

在混合专家模型（MoE）中，不同专家的负载可能因输入数据的分布而不均，导致某些 GPU 过载，而其他 GPU 资源未被充分利用。EPLB（Expert Parallel Load Balancer）通过以下策略实现了负载的动态均衡：
- **专家复制**：对于高负载的专家，EPLB 会创建其副本，并将这些副本分配到负载较低的 GPU 上，以分担计算任务。
- **启发式分配算法**：根据专家的历史负载数据，智能地将专家分配到不同的 GPU 上，确保各 GPU 的负载均衡。
- **节点内专家分组**：尽可能将需要协同工作的专家分配到同一节点，减少跨节点通信的开销 。


## 6-3FS文件系统

3FS（Fire-Flyer File System）是 DeepSeek 团队为 AI 模型训练和推理场景设计的高性能分布式文件系统，专注于高带宽读取和低延迟访问。其优化措施主要体现在以下几个方面：

### **架构设计与一致性保障**

3FS 采用典型的分布式文件系统架构，包含客户端（Client）、集群管理服务（mgmtd）、元数据服务（Meta Service, mds）和数据服务（Storage Service）四大组件。其中，元数据服务使用分布式事务的键值存储系统 FoundationDB，简化了元数据的设计和实现，确保了系统的一致性和高可用性。

在数据存储方面，3FS 使用了 CRAQ（Chain Replication with Apportioned Queries）协议，在保证数据强一致性的同时，优化了读取性能。该协议通过链式复制的方式，将写操作按顺序传递至所有副本，读操作则可在任意副本上进行，从而提高了系统的读取吞吐量。

### **高性能客户端与零拷贝机制**

3FS 提供两种客户端接入方式：FUSE 客户端和 Native 客户端。其中，Native 客户端通过 USRBIO 库实现了用户态与内核态之间的零拷贝数据传输，显著提升了数据读写性能。该机制利用共享内存和 IoRing 技术，减少了数据在用户态和内核态之间的复制开销，从而降低了 I/O 延迟。

### **面向 AI 场景的优化策略**

3FS 在设计上充分考虑了 AI 模型训练和推理的特点，进行了多项针对性的优化：
- **高吞吐量读取**：在 180 节点的集群中，3FS 实现了高达 6.6 TiB/s 的聚合读取吞吐量，满足了大规模模型训练对数据读取速度的极高要求。
- **KVCache 高速访问**：在推理场景下，3FS 支持高效的 KVCache 查找，每个客户端节点的峰值吞吐量超过 40 GiB/s，显著提升了推理性能。
- **智能调度与负载均衡**：3FS 通过智能调度算法，根据各节点的负载情况动态分配任务，确保系统资源的最优利用，提升了整体性能。
- **小文件处理优化**：针对小文件处理性能较差的问题，3FS 引入了 FFRecord 文件格式，将多个小文件合并为一个大文件，减少了元数据操作的开销，提高了小文件的处理效率。

### **端到端无缓存设计**

3FS 采用了端到端无缓存的设计理念，摒弃了传统文件系统中的数据和元数据缓存机制，充分利用 SSD 的高性能特点，减少了缓存一致性维护的开销，提升了系统的整体性能。

























