### **一、自注意力（Self-Attention）**
#### **1. 核心思想**
自注意力用于让模型在处理输入序列的某个位置时，动态关注同一序列中其他位置的关联信息。 

**关键点**：  
- Query（Q）、Key（K）、Value（V）**均来自同一输入序列**。  
- 捕捉序列内部的长距离依赖关系（例如句子中两个相隔较远的词之间的关系）。

#### **2. 应用场景**
- **Transformer编码器**：每个词通过自注意力关注整个输入序列的所有词。  
- **BERT、GPT等预训练模型**：通过自注意力学习上下文表示。

#### **3. 计算步骤**
假设输入序列为矩阵 $X \in \mathbb{R}^{n \times d}$（n 为序列长度，d 为特征维度）：  
1. **生成Q、K、V**：  
   $$
   Q = X W^Q, \quad K = X W^K, \quad V = X W^V  
   $$  
   其中 $W^Q, W^K, W^V \in \mathbb{R}^{d \times d_k}$ 是可学习参数矩阵。

2. **计算注意力分数**：  
   $$
   \text{Attention Score} = \frac{Q K^T}{\sqrt{d_k}}  
   $$  
   （缩放点积注意力，避免梯度消失）

3. **Softmax归一化**：  
   $$
   \text{Attention Weights} = \text{Softmax}(\text{Attention Score})  
   $$

4. **加权求和**：  
   $$
   \text{Output} = \text{Attention Weights} \cdot V  
   $$

#### **4. 多头自注意力（Multi-Head Self-Attention）**
为了增强模型对不同语义子空间的学习能力：  
- 将Q、K、V拆分为多个“头”（例如8个头），每个头独立计算注意力。  
- 拼接所有头的输出并通过线性变换合并：  
  $$
  \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O  
  $$  
  其中，每个头的计算为：  
  $$
  \text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)  
  $$

#### **5. 自注意力的特点**
- **优点**：  
  - 直接建模任意距离的依赖关系（优于RNN/CNN）。  
  - 并行计算，高效处理长序列。  
- **缺点**：  
  - 计算复杂度为 $O(n^2)$，长序列时资源消耗大。  
  - 需要位置编码（Positional Encoding）补充位置信息。

#### **6. 示例**
假设输入句子为：“The cat sat on the mat”。  
自注意力允许“sat”关注到“cat”和“mat”，即使它们在序列中相隔较远。

---

### **二、交叉注意力（Cross-Attention）**
#### **1. 核心思想**
交叉注意力用于**两个不同序列之间的交互**，例如在编码器-解码器结构中，解码器生成目标序列时动态关注编码器的输出。  
**关键点**：  
- **Query（Q）来自一个序列**（如解码器的当前状态）。  
- **Key（K）和 Value（V）来自另一个序列**（如编码器的输出）。

#### **2. 应用场景**
- **Transformer解码器**：解码器生成目标序列时，Q来自解码器的自注意力输出，K和V来自编码器的输出。  
- **多模态任务**：例如图像描述生成（文本Query关注图像特征的Key/Value）。

#### **3. 计算步骤**
假设编码器输出为 $H_{\text{enc}} \in \mathbb{R}^{m \times d}$，解码器输入为 $H_{\text{dec}} \in \mathbb{R}^{n \times d}$：  
1. **生成Q、K、V**：  
   $$
   Q = H_{\text{dec}} W^Q, \quad K = H_{\text{enc}} W^K, \quad V = H_{\text{enc}} W^V  
   $$  
   （注意：Q来自解码器，K和V来自编码器）

2. 后续步骤与自注意力相同：  
   - 计算缩放点积分数 → Softmax → 加权求和。

#### **4. 交叉注意力的特点**
- **优点**：  
  - 建立跨序列的语义关联（如源语言到目标语言）。  
  - 动态选择与当前生成位置最相关的编码器信息。  
- **缺点**：  
  - 依赖编码器的表示质量。  
  - 计算复杂度与编码器序列长度相关（若编码器序列长，可能影响效率）。

#### **5. 示例**
在机器翻译任务中，解码器生成英文单词“bank”时：  
- Q是解码器当前状态（表示“bank”的生成需求）。  
- K和V来自编码器的法语输入序列，模型可能关注“banque”（法语中的“bank”）或“rivière”（法语中的“river”，根据上下文）。

---

### **三、自注意力与交叉注意力的对比**
| **特性**               | **自注意力（Self-Attention）**              | **交叉注意力（Cross-Attention）**          |
|------------------------|---------------------------------------------|--------------------------------------------|
| **输入来源**            | Q、K、V均来自同一序列                       | Q来自序列A，K、V来自序列B                  |
| **主要作用**            | 捕捉序列内部依赖关系                        | 建立两个序列之间的关联                     |
| **典型应用**            | Transformer编码器、BERT                     | Transformer解码器、多模态任务              |
| **计算复杂度**          | $O(n^2)$（n为输入序列长度）               | $O(n m)$（n为Q序列长度，m为K/V序列长度） |
| **是否需要位置编码**    | 是（因自注意力本身不包含位置信息）          | 是（解码器需知道自身位置）                 |

---

### **四、实战中的细节**
#### **1. 自注意力的掩码（Masking）**
- **因果掩码（Causal Mask）**：在解码器的自注意力中，防止当前位置关注未来信息（用于生成任务）。  
#### **2. 交叉注意力的位置**
在Transformer解码器中，交叉注意力层位于：  
1. 解码器自注意力层之后。  
2. 前馈网络之前。  

#### **3. 代码示例（伪代码）**
```python
# 自注意力
def self_attention(X):
    Q = X @ W_Q
    K = X @ W_K
    V = X @ W_V
    scores = (Q @ K.T) / sqrt(d_k)
    weights = softmax(scores)
    output = weights @ V
    return output

# 交叉注意力
def cross_attention(decoder_input, encoder_output):
    Q = decoder_input @ W_Q
    K = encoder_output @ W_K
    V = encoder_output @ W_V
    scores = (Q @ K.T) / sqrt(d_k)
    weights = softmax(scores)
    output = weights @ V
    return output
```

---

### **五、总结**
- **自注意力**：处理单序列内部关系，是Transformer的核心，用于编码上下文表示。  
- **交叉注意力**：连接两个序列，实现跨序列信息交互（如机器翻译、问答系统）。  

理解两者的区别与联系，是掌握Transformer架构的关键。如果有进一步问题，欢迎随时提问！