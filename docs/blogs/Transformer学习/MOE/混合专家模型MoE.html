<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-blogs/Transformer学习/MOE/混合专家模型MoE" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">混合专家模型MoE | BUAAer-xing Blog</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://buaaer-xing.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://buaaer-xing.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://buaaer-xing.github.io/docs/blogs/Transformer学习/MOE/混合专家模型MoE"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="混合专家模型MoE | BUAAer-xing Blog"><meta data-rh="true" name="description" content="0-Introduction"><meta data-rh="true" property="og:description" content="0-Introduction"><link data-rh="true" rel="icon" href="/img/icon.png"><link data-rh="true" rel="canonical" href="https://buaaer-xing.github.io/docs/blogs/Transformer学习/MOE/混合专家模型MoE"><link data-rh="true" rel="alternate" href="https://buaaer-xing.github.io/docs/blogs/Transformer学习/MOE/混合专家模型MoE" hreflang="en"><link data-rh="true" rel="alternate" href="https://buaaer-xing.github.io/docs/blogs/Transformer学习/MOE/混合专家模型MoE" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://YOUR_APP_ID-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"混合专家模型MoE","item":"https://buaaer-xing.github.io/docs/blogs/Transformer学习/MOE/混合专家模型MoE"}]}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="BUAAer-xing Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="BUAAer-xing Blog Atom Feed">




<link rel="search" type="application/opensearchdescription+xml" title="BUAAer-xing Blog" href="/opensearch.xml">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.98cc3bd4.css">
<script src="/assets/js/runtime~main.b1a1434e.js" defer="defer"></script>
<script src="/assets/js/main.8b44110d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/icon.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/icon.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Home</b></a><a class="navbar__item navbar__link" href="/docs/paper_notes_intro">论文笔记</a><a class="navbar__item navbar__link" href="/docs/week_report/week_report_intro">周报汇总</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/blogs_intro">个人博客</a><a class="navbar__item navbar__link" href="/docs/my_papers_intro">发表论文</a><a class="navbar__item navbar__link" href="/blog">相关内容</a><a class="navbar__item navbar__link" href="/resume">个人简历</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/BUAAer-xing/BUAAer-xing.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/blogs_intro">博客说明</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/blogs/Linux学习笔记/Linux基础/虚拟机部分">Linux学习笔记</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/blogs/Fortran语言/笔记/fortran 简介">Fortran语言</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/blogs/高性能计算/高性能计算学习路线/高性能计算的学习路线">高性能计算</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/blogs/CUDA编程学习笔记/Tips/CUDA中的统一虚拟内存">CUDA编程学习笔记</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/docs/blogs/Transformer学习/入门/机器学习与深度学习">Transformer学习</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/blogs/Transformer学习/入门/机器学习与深度学习">入门</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/blogs/Transformer学习/DeepSeek/开源周/DeepSeek开源周总结">DeepSeek</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/docs/blogs/Transformer学习/MOE/混合专家模型MoE">MOE</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/blogs/Transformer学习/MOE/混合专家模型MoE">混合专家模型MoE</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/blogs/Transformer学习/MOE/MoE中的All-to-All算子">MoE中的All-to-All算子</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/blogs/CANN异构架构/概述">CANN异构架构</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Transformer学习</span></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">MOE</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">混合专家模型MoE</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>混合专家模型MoE</h1></header><h2 class="anchor anchorWithStickyNavbar_LWe7" id="0-introduction">0-Introduction<a href="#0-introduction" class="hash-link" aria-label="Direct link to 0-Introduction" title="Direct link to 0-Introduction">​</a></h2>
<p>deepseek的王炸出现，MOE方法更加引起关注。</p>
<p><img decoding="async" loading="lazy" alt="center|400" src="/assets/images/v2-ea05429a94b6c88bf7e86fdfda08c838_1440w-b43037cdf98ba67019fe1d0c818b308b.jpg" width="1080" height="358" class="img_ev3q"></p>
<p>在这文中，将深入探讨 MoE 的核心组件、训练方法，以及在推理过程中需要考量的各种因素。</p>
<p>混合专家模型 (MoE)的特点:</p>
<ul>
<li>与稠密模型相比， <strong>预训练速度更快</strong></li>
<li>与具有相同参数数量的模型相比，具有更快的 <strong>推理速度</strong></li>
<li>📒：需要 <strong><font color="red"><b>大量显存</b></font></strong>，因为所有专家系统都需要加载到内存中</li>
<li>在 <strong>微调方面存在诸多挑战</strong>，但 近期的研究 表明，对混合专家模型进行 <strong>指令调优具有很大的潜力</strong>。</li>
</ul>
<p>为实现大模型的高效训练与推理，研究方向主要有三种：</p>
<ul>
<li>一是从底层架构入手，如将Transformer架构改为基于状态空间模型（SSM）的Mamba架构；</li>
<li>二是优化预训练微调方法，例如《大模型免微调的上下文对齐方法》中提到的URIAL方法，通过少量示例和系统提示对基础LLM进行对齐；</li>
<li>三是采用混合专家模型（Mixture of Experts，MoE）的大而化之处理方式。随着Mixtral 8x7B的推出，其基于MoE的Transformer架构受到广泛关注。</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-什么是混合专家模型">1-什么是混合专家模型？<a href="#1-什么是混合专家模型" class="hash-link" aria-label="Direct link to 1-什么是混合专家模型？" title="Direct link to 1-什么是混合专家模型？">​</a></h2>
<p><font color="red"><b>模型规模是提升模型性能的关键因素之一</b></font>。在有限的计算资源预算下，用更少的训练步数训练一个更大的模型，往往比用更多的步数训练一个较小的模型效果更佳。</p>
<font color="red"><b>📒：要首先保证模型的规模</b></font>
<p>混合专家模型 (MoE) 的一个显著优势是它们能够在远少于稠密模型所需的计算资源下进行有效的预训练。这意味着在相同的计算预算条件下，<strong>可以显著扩大模型或数据集的规模</strong>。特别是在预训练阶段，与稠密模型相比，混合专家模型通常能够更快地达到相同的质量水平。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="11-混合专家模型moe">1.1-混合专家模型MoE<a href="#11-混合专家模型moe" class="hash-link" aria-label="Direct link to 1.1-混合专家模型MoE" title="Direct link to 1.1-混合专家模型MoE">​</a></h3>
<p>MoE提出的前提是如果有一个包括了多个领域知识的复杂问题：该使用什么样的方法来解决呢？<strong>最简单的办法就是把各个领域的专家集合到一起来攻克这个任务，比如事先要把不同的任务先分离出来，这样才便于分发给不同领域的专家，让他们来帮忙处理，最后再汇总结论</strong>。</p>
<p>混合专家模型正是基于这样的理念，它由多个专业化的子模型（即“专家”）组合而成，每一个“专家”都在其擅长的领域内做出贡献。而决定哪个“专家”参与解答特定问题的，是一个称为“<a href="https://zhida.zhihu.com/search?content_id=239283275&amp;content_type=Article&amp;match_order=1&amp;q=%E9%97%A8%E6%8E%A7%E7%BD%91%E7%BB%9C&amp;zhida_source=entity" target="_blank" rel="noopener noreferrer">门控网络</a>”的机制。</p>
<p>作为一种基于 Transformer 架构的模型，混合专家模型主要由两个关键部分组成:</p>
<ul>
<li><strong>稀疏 MoE 层</strong>: 这些层代替了传统 Transformer 模型中的前馈网络 (FFN) 层。MoE 层包含若干“专家”(例如 8 个)，每个专家本身是一个独立的神经网络。在实际应用中，这些专家通常是前馈网络 (FFN)，但它们也可以是更复杂的网络结构，甚至可以是 MoE 层本身，从而形成层级式的 MoE 结构。</li>
<li><strong>门控网络或路由</strong>: 这个部分用于决定哪些令牌 (token) 被发送到哪个专家。例如，在下图中，“More”这个令牌可能被发送到第二个专家，而“Parameters”这个令牌被发送到第一个专家。有时，一个令牌甚至可以被发送到多个专家。<strong>令牌的路由方式是 MoE 使用中的一个关键点，因为路由器由学习的参数组成，并且与网络的其他部分一同进行预训练</strong>。
<img decoding="async" loading="lazy" alt="center|600" src="/assets/images/v2-4b14295694827242663e530e887b137b_1440w-ea4b16a4540dfe18fd73f577642dcede.jpg" width="1080" height="832" class="img_ev3q">
<center> <font face="华文宋体" size="4"> Switch Transformers paper 论文中的 MoE layer </font> </center></li>
</ul>
<p>总结来说，在混合专家模型 (MoE) 中，<font color="red"><b>将传统 Transformer 模型中的每个前馈网络 (FFN) 层替换为 MoE 层</b></font>，其中 MoE 层由两个核心部分组成: <strong>一个门控网络和若干数量的专家</strong>。</p>
<p>尽管混合专家模型 (MoE) 提供了若干显著优势，例如更高效的预训练和与稠密模型相比更快的推理速度，但它们也伴随着一些挑战:</p>
<ul>
<li><strong>训练挑战</strong>: 虽然 MoE 能够实现更高效的计算预训练，但它们在微调阶段往往面临泛化能力不足的问题，长期以来易于引发过拟合现象。</li>
<li><strong>推理挑战</strong>: <font color="red"><b>MoE 模型虽然可能拥有大量参数，但在推理过程中只使用其中的一部分，这使得它们的推理速度快于具有相同数量参数的稠密模型。然而，这种模型需要将所有参数加载到内存中，因此对内存的需求非常高</b></font>。以 Mixtral 8x7B MoE 为例，需要足够的 VRAM 来容纳一个 47B 参数的稠密模型。之所以是 47B 而不是 8 x 7B = 56B，是因为在 MoE 模型中，只有 FFN 层被视为独立的专家，而模型的其他参数是共享的。此外，假设每个token只使用两个专家，那么推理速度 (以 FLOPs 计算) 类似于使用 12B 模型 (而不是 14B 模型)，因为虽然它进行了 2x7B 的矩阵乘法计算，但某些层是共享的。</li>
</ul>
<hr>
<p><strong>MOE面对着内存需求的挑战原因</strong>：</p>
<p>在推理过程中，MoE 模型通过门控机制（Gating Mechanism）选择性地激活部分专家网络（Experts），通常只激活其中的一个或几个专家，从而减少了计算量，提高了推理速度。然而，<strong>为了确保在推理时能够快速访问任意专家的参数，所有专家的参数必须预先加载到内存中</strong>。这意味着，即使在一次推理中只使用了部分专家，整个模型的参数仍需常驻内存，导致内存需求较高。</p>
<p>这种设计在训练阶段尤为明显，因为训练过程中需要频繁地更新所有专家的参数。在推理阶段，为了避免频繁的磁盘 I/O 操作带来的延迟，通常也会将所有参数加载到内存中。因此，<strong>尽管 MoE 模型在计算上是稀疏的，但在内存使用上仍然是稠密的</strong>。</p>
<p>无论是稠密模型还是 MoE（Mixture of Experts）模型，在推理过程中都需要将模型参数加载到内存中。然而，MoE 模型在内存需求方面存在一些独特的挑战，主要体现在以下几个方面：</p>
<ul>
<li>首先，MoE 模型的专家模块（Experts）在推理过程中是动态选择的。也就是说，对于每个输入，模型会根据门控机制选择激活的专家，这种选择可能在每个时间步都不同。因此，<strong>为了避免频繁的磁盘 I/O 操作带来的延迟，通常需要将所有专家的参数预先加载到内存中，以确保推理过程的高效性</strong>。</li>
<li>其次，虽然在一次推理中只使用了部分专家，但由于无法预知下一个时间步将激活哪些专家，因此<strong>必须将所有专家的参数常驻内存</strong>。这种设计在训练阶段尤为明显，因为训练过程中需要频繁地更新所有专家的参数。</li>
<li>此外，<strong>MoE 模型的参数总量通常远大于等效性能的稠密模型</strong>。例如，为了达到与某个稠密模型相同的性能，MoE 模型可能需要更多的参数，这进一步增加了内存需求。</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="12-moe的优点">1.2-Moe的优点<a href="#12-moe的优点" class="hash-link" aria-label="Direct link to 1.2-Moe的优点" title="Direct link to 1.2-Moe的优点">​</a></h3>
<ol>
<li>任务特异性：采用混合专家方法可以有效地充分利用多个专家模型的优势，每个专家都可以专门处理不同的任务或数据的不同部分，<strong>在处理复杂任务时取得更卓越的性能</strong>。<!-- -->
<ul>
<li>各个专家模型能够针对不同的数据分布和模式进行建模，从而显著提升模型的准确性和泛化能力，因此模型可以更好地适应任务的复杂性。</li>
</ul>
</li>
<li>灵活性：混合专家方法展现出卓越的灵活性，能够<strong>根据任务的需求灵活选择并组合适宜的专家模型</strong>。<!-- -->
<ul>
<li>模型的结构允许根据任务的需要动态选择激活的专家模型，实现对输入数据的灵活处理。这使得模型能够适应不同的输入分布和任务场景，提高了模型的灵活性。</li>
</ul>
</li>
<li>高效性：由于只有少数专家模型被激活，大部分模型处于未激活状态，混合专家模型具有很高的<a href="https://zhida.zhihu.com/search?content_id=239283275&amp;content_type=Article&amp;match_order=1&amp;q=%E7%A8%80%E7%96%8F%E6%80%A7&amp;zhida_source=entity" target="_blank" rel="noopener noreferrer">稀疏性</a>。这种<font color="red"><b>稀疏性带来了计算效率的提升</b></font>，因为只有特定的专家模型对当前输入进行处理，减少了计算的开销。</li>
<li>表现能力：每个专家模型可以被设计为更加专业化，能够更好地捕捉输入数据中的模式和关系。整体模型通过组合这些专家的输出，提高了对复杂数据结构的建模能力，从而增强了模型的性能。</li>
<li>可解释性：由于每个专家模型相对独立，因此模型的决策过程更易于解释和理解，为用户提供更高的可解释性，这对于一些对模型决策过程有强解释要求的应用场景非常重要。</li>
<li>适应大规模数据：混合专家方法是处理大规模数据集的理想选择，能够有效地应对数据量巨大和特征复杂的挑战，可以利用稀疏矩阵的高效计算，利用GPU的并行能力计算所有专家层，能够有效地应对海量数据和复杂特征的挑战。</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="13-混合专家模型简史">1.3-混合专家模型简史<a href="#13-混合专家模型简史" class="hash-link" aria-label="Direct link to 1.3-混合专家模型简史" title="Direct link to 1.3-混合专家模型简史">​</a></h3>
<p>混合专家模型 (MoE) 的理念起源于 1991 年的论文 Adaptive Mixture of Local Experts。这个概念与集成学习方法相似，旨在为由多个单独网络组成的系统建立一个监管机制。</p>
<p>在moe中，<strong>每个网络 (被称为“专家”) 处理训练样本的不同子集，专注于输入空间的特定区域</strong>。那么，如何选择哪个专家来处理特定的输入呢？这就是门控网络的任务，它决定了分配给每个专家的权重。在训练过程中，这些专家和门控网 络都同时接受训练，以优化它们的性能和决策能力。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-混合专家模型-moe-的关键研究">2-混合专家模型 (MoE) 的关键研究<a href="#2-混合专家模型-moe-的关键研究" class="hash-link" aria-label="Direct link to 2-混合专家模型 (MoE) 的关键研究" title="Direct link to 2-混合专家模型 (MoE) 的关键研究">​</a></h2>
<ol>
<li><strong>组件专家</strong>: 在早期的 MoE 设置中，整个系统由一个门控网络和多个专家组成。在支持向量机 (SVM) 、高斯过程和其他方法的研究中，MoE 通常被视为整个模型的一部分。后来 MoE 也作为更深层网络的一个组件。这种方法允许将 MoE 嵌入到多层网络中的某一层，使得模型既大又高效。</li>
<li><strong>条件计算</strong>: 传统的神经网络通过每一层处理所有输入数据。后来人们开始探索基于输入令牌动态激活或停用网络组件的方法。</li>
</ol>
<p>这两项研究的融合促进了在自然语言处理 (NLP) 领域对混合专家模型的探索。通过引入稀疏性，在保持极高规模的同时实现了快速的推理速度。但主要集中在翻译领域，但面临着如高通信成本和训练不稳定性等多种挑战。</p>
<p><img decoding="async" loading="lazy" alt="center|500" src="/assets/images/v2-ea0202c0e4eed9b00c2b4b3e9ab505d0_1440w-2a74bc39cc07dd4521914547d1fefaf3.jpg" width="881" height="425" class="img_ev3q">
<center> <font face="华文宋体" size="4"> Outrageously Large Neural Network 论文中的 MoE layer </font> </center></p>
<p>混合专家模型 (MoE) 的引入使得训练具有数千亿甚至万亿参数的模型成为可能，如开源的 1.6 万亿参数的 Switch Transformers 等。这种方法即可以在自然语言处理 (NLP) 领域得到了广泛应用，也可以在计算机视觉领域进行探索。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-moe模型结构">3-MOE模型结构<a href="#3-moe模  型结构" class="hash-link" aria-label="Direct link to 3-MOE模型结构" title="Direct link to 3-MOE模型结构">​</a></h2>
<p>混合专家模型（MoE）是一种稀疏门控制的深度学习模型，它主要由<strong>一组专家模型</strong>和<strong>一个门控模型</strong>组成。MoE的基本理念是将输入数据根据任务类型分割成多个区域，并将每个区域的数据分配一个或多个专家模型。每个专家模型可以专注于处理输入这部分数据，从而提高模型的整体性能。</p>
<p>MoE架构的基本原理非常简单明了，它主要包括两个核心组件：GateNet和Experts。</p>
<ul>
<li><strong>GateNet</strong>的作用在于判定输入样本应该由哪个专家模型接管处理。</li>
<li><strong>Experts</strong>则构成了一组相对独立的专家模型，每个专家负责处理特定的输入子空间。</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="31-门控网络gatenet">3.1-门控网络GateNet<a href="#31-门控网络gatenet" class="hash-link" aria-label="Direct link to 3.1-门控网络GateNet" title="Direct link to 3.1-门控网络GateNet">​</a></h3>
<p>混合专家模型中“门”是一种稀疏门网络，它接收单个数据元素作为输入，然后输出一个权重，这些权重表示每个专家模型对处理输入数据的贡献。<font color="red"><b>一般是通过softmax门控函数通过专家或token对概率分布进行建模，并选择前K个</b></font>。</p>
<p>例如，如果模型有三个专家，输出的概率可能为0.5和0.4、0.1，这意味着第一个专家对处理此数据的贡献为50%，第二个专家为40%，第二个专家为10%，这个时候的K就可以选择为2，我们认为前两个专家模型的建议会更好，可以用于更加精确的回答中，而第三个专家模型的建议可以用于更加富有创意性的答案中。</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>G</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>Softmax</mtext><mo stretchy="false">(</mo><mtext>KeepTopK</mtext><mo stretchy="false">(</mo><mi>H</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">G(x) = \text{Softmax}(\text{KeepTopK}(H(x), k))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">G</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">Softmax</span></span><span class="mopen">(</span><span class="mord text"><span class="mord">KeepTopK</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mclose">))</span></span></span></span></span>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mo stretchy="false">)</mo><mi>i</mi></msub><mo>=</mo><mo stretchy="false">(</mo><mi>x</mi><mo>⋅</mo><msub><mi>W</mi><mi>g</mi></msub><msub><mo stretchy="false">)</mo><mi>i</mi></msub><mo>+</mo><mtext>StandardNormal</mtext><mo stretchy="false">(</mo><mo stretchy="false">)</mo><mo>⋅</mo><mtext>Softplus</mtext><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mi>x</mi><mo>⋅</mo><msub><mi>W</mi><mtext>noise</mtext></msub><msub><mo stretchy="false">)</mo><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H(x)_i = (x \cdot W_g)_i + \text{StandardNormal}() \cdot \text{Softplus}((x \cdot W_{\text{noise}})_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">StandardNormal</span></span><span class="mopen">(</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">Softplus</span></span><span class="mopen">((</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">noise</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>KeepTopK</mtext><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>k</mi><msub><mo stretchy="false">)</mo><mi>i</mi></msub><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>v</mi><mi>i</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><msub><mi>v</mi><mi>i</mi></msub><mtext> is in the top </mtext><mi>k</mi><mtext> elements of </mtext><mi>v</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>otherwise</mtext><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\text{KeepTopK}(v, k)_i =
\begin{cases}
v_i &amp; \text{if } v_i \text{ is in the top } k \text{ elements of } v \\
-\infty &amp; \text{otherwise}.
\end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">KeepTopK</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:3em;vertical-align:-1.25em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em"><span style="top:-3.69em"><span class="pstrut" style="height:3.008em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.25em"><span class="pstrut" style="height:3.008em"></span><span class="mord"><span class="mord">−</span><span class="mord">∞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em"><span style="top:-3.69em"><span class="pstrut" style="height:3.008em"></span><span class="mord"><span class="mord text"><span class="mord">if </span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord text"><span class="mord"> is in the top </span></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mord text"><span class="mord"> elements of </span></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span><span style="top:-2.25em"><span class="pstrut" style="height:3.008em"></span><span class="mord"><span class="mord text"><span class="mord">otherwise</span></span><span class="mord">.</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="32-专家experts">3.2-专家Experts<a href="#32-专家experts" class="hash-link" aria-label="Direct link to 3.2-专家Experts" title="Direct link to 3.2-专家Experts">​</a></h3>
<p>在训练的过程中，输入的数据被门控模型分配到不同的专家模型中进行处理；在推理的过程中，被门控选择的专家会针对输入的数据，产生相应的输出。这些输出最后会和每个专家模型处理该特征的能力分配的权重进行加权组合，形成最终的预测结果。</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Importance</mtext><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></munder><mi>G</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Importance}(X) = \sum_{x \in X} G(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">Importance</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.3717em;vertical-align:-1.3217em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em"><span style="top:-1.8557em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.07847em">X</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3217em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">G</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>L</mi><mtext>importance</mtext></msub><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>w</mi><mtext>importance</mtext></msub><mo>⋅</mo><mi>C</mi><mi>V</mi><mo stretchy="false">(</mo><mtext>Importance</mtext><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L_{\text{importance}}(X) = w_{\text{importance}} \cdot CV(\text{Importance}(X))^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">importance</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7306em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">importance</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.1141em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mopen">(</span><span class="mord text"><span class="mord">Importance</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>
<p>混合专家模型:</p>
<ul>
<li>在训练过程中通过门控模型实现“<strong>因材施教</strong>”</li>
<li>在推理过程中实现专家模型之间的“<strong>博采众长</strong>”</li>
</ul>
<p>MoE的专家模型可以是小型的MLP或者复杂的LLM。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-moe与稀疏性">4-MOE与稀疏性<a href="#4-moe与稀疏性" class="hash-link" aria-label="Direct link to 4-MOE与稀疏性" title="Direct link to 4-MOE与稀疏性">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="41-稀疏性">4.1-稀疏性<a href="#41-稀疏性" class="hash-link" aria-label="Direct link to 4.1-稀疏性" title="Direct link to 4.1-稀疏性">​</a></h3>
<p><font color="red"><b>稀疏性的概念采用了条件计算的思想</b></font>。</p>
<ul>
<li>在传统的稠密模型中，<strong>所有的参数都会对所有输入数据进行处理</strong>。</li>
<li>相比之下，稀疏性允许我们仅针对整个系统的某些特定部分执行计算。这意味着并非所有参数都会在处理每个输入时被激活或使用，而是根据输入的特定特征或需求，<strong>只有部分参数集合被调用和运行</strong>。</li>
</ul>
<p>条件计算概念 的提出(即仅在每个样本的基础上激活网络的不同部分) 使得在不增加额外计算负担的情况下扩展模型规模成为可能。这一策略在每个 MoE 层中实现了数以千计甚至更多的专家的有效利用。</p>
<p>这种稀疏性设置确实带来了一些挑战。例如，在混合专家模型 (MoE) 中，尽管较大的批量大小通常有利于提高性能，但当数据通过激活的专家时，实际的批量大小可能会减少。</p>
<p>比如，假设输入批量包含 10 个token， <strong>可能会有五个token被路由到同一个专家，而剩下的五个token分别被路由到不同的专家。这导致了批量大小的不均匀分配和资源利用效率不高的问题</strong>。在接下来的部分中，将会讨论让 MoE 高效运行的其他挑战以及相应的解决方案。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="42-moe对稀疏性的处理">4.2-MoE对稀疏性的处理<a href="#42-moe对稀疏性的处理" class="hash-link" aria-label="Direct link to 4.2-MoE对稀疏性的处理" title="Direct link to 4.2-MoE对稀疏性的处理">​</a></h3>
<p>利用一个可学习的门控网络 (G) 决定将输入的哪一部分发送给哪些专家 (E):</p>
<p>为了有效控制稀疏性，主要依赖于门控网络的设计和参数调整。门控网络负责决定哪些专家模型参与处理当前的输入数据。然而，在进行参数选择时需要注意一个权衡：如果门控网络在单次选择中激活了较多的专家模型，虽然这可能提升了模型的表现能力，但却会导致稀疏性的降低。因为更多的专家模型参与计算，这会带来额外的计算复杂性和耗时。</p>
<p>MoE模型的稀疏性存在平衡的挑战，需要根据具体的应用需求和计算资源限制来调整门控网络的设计和参数。在实际应用中，可以根据不同的场景，灵活地选择专家模型的数量，以在效率和性能之间找到最佳的平衡点。这种个性化的调整能够确保混合专家模型在各种应用中发挥出最佳的优势，为模型提供更大的灵活性和可塑性。</p>
<p><img decoding="async" loading="lazy" alt="center|500" src="/assets/images/v2-563e7f4723e1ba36831f206df3941aea_1440w-a148de54f1fa45accc28e35c9e4c44a7.jpg" width="1080" height="535" class="img_ev3q"></p>
<p>这里的“门”概念，与LSTM网络 的“门”概念有所不同，MoE的“门”概念主要是用于匹配数据和专家模型之间的连接，就好比不同班级的学生要进不同的教室上课一样，而LSTM的“门”概念主要是一种控制信息流动的装置，它可以保留或通过一定比例的数据，更像是在控制流量，而MoE的“门”概念可以看作是选择要通过的对象。</p>
<p>MoE的稀疏性与dropout的原理类似，MoE是根据任务的具体情况选择激活一定数量的专家模型来完成这个任务，而dropout则是对神经网络中的神经元进行随机性失活，每次训练的时候只保留一定的参数，这不但让网络具备了稀疏性特征，减轻了整个网络的参数压力，还会降低模型发生过拟合的概率，提高模型的泛化能力。</p>
<p>在这种设置下，虽然所有专家都会对所有输入进行运算，但通过门控网络的输出进行加权乘法操作。但是，如果 G (门控网络的输出) 为 0 会发生什么呢？如果是这种情况，就没有必要计算相应的专家操作，因此我们可以节省计算资源。那么一个典型的门控函数是什么呢？一个典型的门控函数通常是一个带有 softmax 函数的简单的网络。这个网络将学习将输入发送给哪个专家。</p>
<p>还有另外一种门控机制，其中包括带噪声的 TopK 门控 (Noisy Top-K Gating)。这种门控方法引入了一些可调整的噪声，然后保留前 k 个值。具体来说:</p>
<ol>
<li>添加一些噪声</li>
<li>选择保留前 K 个值</li>
<li>应用 Softmax 函数</li>
</ol>
<p>这种稀疏性引入了一些有趣的特性。通过使用较低的 k 值 (例如 1 或 2)，我们可以比激活多个专家时更快地进行训练和推理。为什么不仅选择最顶尖的专家呢？最初的假设是，需要将输入路由到不止一个专家，以便门控学会如何进行有效的路由选择，因此至少需要选择两个专家。</p>
<p>为什么要添加噪声呢？这是为了专  家间的负载均衡！</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="43-混合专家模型中令牌的负载均衡">4.3-混合专家模型中令牌的负载均衡<a href="#43-混合专家模型中令牌的负载均衡" class="hash-link" aria-label="Direct link to 4.3-混合专家模型中令牌的负载均衡" title="Direct link to 4.3-混合专家模型中令牌的负载均衡">​</a></h3>
<p>如果所有的令牌都被发送到只有少数几个受欢迎的专家，那么训练效率将会降低。在通常的混合专家模型 (MoE) 训练中，门控网络往往倾向于主要激活相同的几个专家。这种情况可能会自我加强，因为受欢迎的专家训练得更快，因此它们更容易被选择。为了缓解这个问题，引入了一个 <strong>辅助损失</strong>，旨在鼓励给予所有专家相同的重要性。这个损失确保所有专家接收到大致相等数量的训练样本，从而平衡了专家之间的选择。接下来的部分还将探讨专家容量的概念，它引入了一个关于专家可以处理多少令牌的阈值。在 <code>transformers</code> 库中，可以通过 <code>aux_loss</code> 参数来控制辅助损失。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="5-moe-和-transformer">5-MoE 和 Transformer<a href="#5-moe-和-transformer" class="hash-link" aria-label="Direct link to 5-MoE 和 Transformer" title="Direct link to 5-MoE 和 Transformer">​</a></h2>
<p>Transformer 类模型明确表明，增加参数数量可以提高性能，因此谷歌使用 GShard 尝试将 Transformer 模型的参数量扩展到超过 6000 亿并不令人惊讶。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="51-gshard">5.1-Gshard<a href="#51-gshard" class="hash-link" aria-label="Direct link to 5.1-Gshard" title="Direct link to 5.1-Gshard">​</a></h3>
<p>GShard 将在编码器和解码器中的每个前馈网络 (FFN) 层中的替换为使用 Top-2 门控的混合专家模型 (MoE) 层。下图展示了编码器部分的结构。这种架构 对于大规模计算非常有效: <font color="red"><b>当扩展到多个设备时，MoE 层在不同设备间共享，而其他所有层则在每个设备上复制</b></font>。我们将在 “让 MoE 起飞”部分对这一点进行更详细的讨论。</p>
<p><img decoding="async" loading="lazy" alt="center|600" src="/assets/images/v2-8a6efbe3e61ad82c612d94dcd18e2e0b_1440w-19ddf141465606bdbe98fd4a366002fe.png" width="860" height="677" class="img_ev3q">
<center> <font face="华文宋体" size="4"> GShard 论文中的 MoE Transformer Encoder </font> </center></p>
<p>为了保持负载平衡和训练效率，GShard 的作者除了引入了上一节中讨论的类似辅助损失外，还引入了一些关键变化:</p>
<ul>
<li><strong>随机路由</strong>: 在 Top-2 设置中，始终选择排名最高的专家，但第二个专家是根据其权重比例随机选择的。</li>
<li><strong>专家容量</strong>: 我们可以设定一个阈值，定义一个专家能处理多少令牌。如果两个专家的容量都达到上限，令牌就会溢出，并通过残差连接传递到下一层，或在某些情况下被完全丢弃。<font color="red"><b>专家容量是 MoE 中最重要的概念之一</b></font>。为什么需要专家容量呢？因为所有张量的形状在编译时是静态确定的，我们无法提前知道多少令牌会分配给每个专家，因此需要一个固定的容量因子。</li>
</ul>
<p>GShard 的工作对适用于 MoE 的并行计算模式也做出了重要贡献。</p>
<p><strong>注意</strong>: 在推理过程中，只有部分专家被激活。同时，有些计算过程是共享的，例如自注意力 (self-attention) 机制，它适用于所有令牌。这就解释了为什么我们可以使用相当于 12B 稠密模型的计算资源来运行一个包含 8 个专家的 47B 模型。如果我们采用 Top-2 门控，模型会使用高达 14B 的参数。但是，由于自注意力操作 (专家间共享) 的存在，实际上模型运行时使用的参数数量是 12B 。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="52-switch-transformers">5.2-Switch Transformers<a href="#52-switch-transformers" class="hash-link" aria-label="Direct link to 5.2-Switch Transformers" title="Direct link to 5.2-Switch Transformers">​</a></h3>
<p>尽管混合专家模型 (MoE) 显示出了很大的潜力，但它们在训练和微调过程中存在稳定性问题。Switch Transformers 是一项非常激动人心的工作，它深入研究了这些话题。作者甚至在 Hugging Face 上发布了一个 1.6 万亿参数的 MoE，拥有 2048 个专家，你可以使用 <code>transformers</code> 库来运行它。Switch Transformers 实现了与 T5-XXL 相比 4 倍的预训练速度提升。
<img decoding="async" loading="lazy" alt="center|500" src="/assets/images/v2-a5baa99a155ad83b8cb0d21c2957ce0f_1440w-e204e01e4ceec4a842bc3985948fc6bc.jpg" width="1080" height="551" class="img_ev3q">
<center> <font face="华文宋体" size="4"> Switch Transformer 论文中的 Switch Transformer Layer </font> </center></p>
<p>就像在 GShard 中一样，作者用混合专家模型 (MoE) 层替换了前馈网络 (FFN) 层。Switch Transformers 提出了一个 Switch Transformer 层，它接收两个输入 (两个不同的令牌) 并拥有四个专家。</p>
<p>与早期使用至少两个专家的想法相反，Switch Transformers 采用了简化的单专家策略。这种方法的效果包括:</p>
<ul>
<li>减少门控网络 (路由) 计算负担</li>
<li>每个专家的批量大小至少可以减半</li>
<li>降低通信成本</li>
<li>保持模型质量</li>
</ul>
<p>Switch Transformers 也对 <strong>专家容量</strong> 这个概念进行了研究。</p>
<p>上述建议的容量是将批次中的令牌数量均匀分配到各个专家。如果我们使用大于 1 的容量因子，我们为令牌分配不完全平衡时提供了一个缓冲。增加容量因子会导致更高的设备间通信成本，因此这是一个需要考虑的权衡。特别值得注意的是，Switch Transformers 在低容  量因子 (例如 1 至 1.25) 下表现出色。</p>
<p>Switch Transformer 重新审视并简化了前面章节中提到的负载均衡损失。在训练期间，对于每个 Switch 层的辅助损失被添加到总模型损失中。这种损失鼓励均匀路由，并可以使用超参数进行加权。</p>
<p><font color="red"><b>Switch Transformer使用了混合精度的方法</b></font>，例如用 <code>BFloat16</code> 精度训练专家，同时对其余计算使用全精度进行。较低的精度可以减少处理器间的通信成本、计算成本以及存储张量的内存。然而，注意，当专家和门控网络都使用 <code>bfloat16</code> 精度训练时，出现了不稳定的训练现象。<strong>这种不稳定性特别是由路由计算引起的，因为路由涉及指数函数等操作，这些操作对精度要求较高</strong>。因此，为了保持计算的稳定性和精确性，保持更高的精度是重要的。为了减轻不稳定性，路由过程也使用了全精度。</p>
<p>📒：利用BFloat16精度训练专家，同时对其余计算使用全精度进行。</p>
<p><img decoding="async" loading="lazy" alt="center|500" src="/assets/images/v2-9024da18472e7f67fc9cee91f570657f_1440w-02416ddadba2c789f8de7fff7a68b335.jpg" width="670" height="151" class="img_ev3q">
<center> <font face="华文宋体" size="4"> 使用混合精度不会降低模型质量并可实现更快的训练   </font> </center></p>
<p>Switch Transformers 采用了编码器 - 解码器的架构，实现了与 T5 类似的混合专家模型 (MoE) 版本。GLaM 这篇工作探索了如何使用仅为原来 1/3 的计算资源 (因为 MoE 模型在训练时需要的计算量较少，从而能够显著降低碳足迹) 来训练与 GPT-3 质量相匹配的模型来提高这些模型的规模。作者专注于仅解码器 (decoder-only) 的模型以及少样本和单样本评估，而不是微调。他们使用了 Top-2 路由和更大的容量因子。此外，他们探讨了将容量因子作为一个动态度 量，根据训练和评估期间所使用的计算量进行调整。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="53-用-router-z-loss-稳定模型训练">5.3-用 Router z-loss 稳定模型训练<a href="#53-用-router-z-loss-稳定模型训练" class="hash-link" aria-label="Direct link to 5.3-用 Router z-loss 稳定模型训练" title="Direct link to 5.3-用 Router z-loss 稳定模型训练">​</a></h3>
<p>之前讨论的平衡损失可能会导致稳定性问题。我们可以使用许多方法来稳定稀疏模型的训练，但这可能会牺牲模型质量。例如，引入 dropout 可以提高稳定性，但会导致模型质量下降。另一方面，增加更多的乘法分量可以提高质量，但会降低模型稳定性。</p>
<p>ST-MoE 引入的 <code>Router z-loss</code> 在保持了模型性能的同时显著提升了训练的稳定性。这种损失机制通过惩罚门控网络输入的较大 <code>logits</code> 来起作用，目的是促使数值的绝对大小保持较小，这样可以有效减少计算中的舍入误差。这一点对于那些依赖指数函数进行计算的门控网络尤其重要。为了深入了解这一机制，建议参考原始论文以获得更全面的细节。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="54-专家如何学习">5.4-专家如何学习？<a href="#54-专家如何学习" class="hash-link" aria-label="Direct link to 5.4-专家如何学习？" title="Direct link to 5.4-专家如何学习？">​</a></h3>
<p><strong>ST-MoE</strong> 的研究者们发现，<strong>编码器中不同的专家倾向于专注于特定类型的令牌或浅层概念</strong>。例如，某些专家可能专门处理标点符号，而其他专家则专注于专有名词等。与此相反，解码器中的专家通常具有较低的专业化程度。此外，研究者们还对这一模型进行了多语言训练。尽管人们可能会预期每个专家处理一种特定语言，但实际上并非如此。由于令牌路由和负载均衡的机制，没有任何专家被特定配置以专门处理某一特定语言。
<img decoding="async" loading="lazy" alt="center|500" src="/assets/images/v2-7685396214b70df746c68be87f82a94d_1440w-252c8b71992a52493cd40bd49df72e70.jpg" width="742" height="687" class="img_ev3q">ST-MoE 论文中显示了哪些令牌组被发送给了哪个专家的表格</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="55-专家的数量对预训练有何影响">5.5-专家的数量对预训练有何影响?<a href="#55-专家的数量对预训练有何影响" class="hash-link" aria-label="Direct link to 5.5-专家的数量对预训练有何影响?" title="Direct link to 5.5-专家的数量对预训练有何影响?">​</a></h3>
<p>增加更多专家可以提升处理样本的效率和加速模型的运算速度，但这些优势随着专家数量的增加而递减 (尤其是当专家数量达到 256 或 512 之后更为明显)。同时，这也意味着在推理过程中，需要更多的显存来加载整个模型。</p>
<p>值得注意的是，Switch Transformers 的研究表明，其在大规模模型中的特性在小规模模型下也同样适用，即便是每层仅包含 2、4 或 8 个专家。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="56-微调混合专家模型">5.6-微调混合专家模型<a href="#56-微调混合专家模型" class="hash-link" aria-label="Direct link to 5.6-微调混合专家模型" title="Direct link to 5.6-微调混合专家模型">​</a></h3>
<p><code>4.36.0</code> 版本的 <code>transformers</code> 库支持 Mixtral 模型。安装命令: <code>pip install &quot;transformers==4.36.0 --upgrade</code></p>
<p>稠密模型和稀疏模型在过拟合的动态表现上存在显著差异。稀疏模型更易于出现过拟合现象，因此在处理这些模型时，尝试更强的内部正则化措施是有益的，比如使用更高比例的 dropout。例如，我们可以为稠密层设定一个较低的 dropout 率，而为稀疏层设置一个更高的 dropout 率，以此来优化模型性能。</p>
<p>在微调过程中是否使用辅助损失是一个需要决策的问题。ST-MoE 的作者尝试关闭辅助损失，发现即使高达 11% 的令牌被丢弃，模型的质量也没有显著受到影响。令牌丢弃可能是一种正则化形式，有助于防止过拟合。</p>
<p>Switch Transformers 的作者观察到，在相同的预训练困惑度下，稀疏模型在下游任务中的表现不如对应的稠密模型，特别是在重理解任务 (如 SuperGLUE) 上。另一方面，对于知识密集型任务 (如 TriviaQA)，稀疏模型的表现异常出色。作者还观察到，在微调过程中，较少的专家的数量有助于改善性能。另一个关于泛化问题确认的发现是，模型在小型任务上表现较差，但在大型任务上表现良好。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/v2-87b348b932b1698fed0ea160a6df8d01_1440w-adbc3f207995950702d44deaecad900b.jpg" width="955" height="363" class="img_ev3q">在小任务 (左图) 中，我们可以看到明显的过拟合，因为稀疏模型在验证集中的表现要差得多。在较大的任务 (右图) 中，MoE 则表现良好。该图来自 ST-MoE 论文一种可行的微调策略是尝试冻结所有非专家层的权重。实践中，这会导致性能大幅下降，但这符合我们的预期，因为混合专家模型 (MoE) 层占据了网络的主要部分。我们可以尝试相反的方法: 仅冻结 MoE 层的参数。实验结果显示，这种方法几乎与更新所有参数的效果相当。这种做法可以加速微调过程，并降低显存需求。</p>
<p><img decoding="async" loading="lazy" alt="center" src="/assets/images/v2-aaca206249c475049e09007120fdb0fb_1440w-dcee286a01d6848442545dc588cd3da6.jpg" width="400" height="308" class="img_ev3q"></p>
<p>通过仅冻结 MoE 层，我们可以在保持质量的同时加快训练速度。该图来自 ST-MoE 论文在微调稀疏混合专家模型 (MoE) 时需要考虑的最后一个问题是，它们有特别的微调超参数设 置——例如，稀疏模型往往更适合使用较小的批量大小和较高的学习率，这样可以获得更好的训练效果。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/v2-b39bd15590674731f023d1da97415763_1440w-50b40caaf46fd5fd17b50eeaddb78401.jpg" width="889" height="334" class="img_ev3q">降低学习率和调大批量可以提升稀疏模型微调质量。该图来自 ST-MoE 论文最近(2023 年 7 月)的论文 《MoEs Meets Instruction Tuning》 进行了以下实验:</p>
<ul>
<li>单任务微调</li>
<li>多任务指令微调</li>
<li>多任务指令微调后接单任务微调</li>
</ul>
<p>当对 MoE 和对应性能相当的 T5 模型进行微调时，可以发现 T5 的对应模型表现更为出色。然而，当研究者们对 <a href="https://zhida.zhihu.com/search?content_id=239283275&amp;content_type=Article&amp;match_order=1&amp;q=Flan+T5&amp;zhida_source=entity" target="_blank" rel="noopener noreferrer">Flan T5</a> (一种 T5 的指令优化版本) 的 MoE 版本进行微调时，MoE 的性能显著提升。更值得注意的是，Flan-MoE 相比原始 MoE 的性能提升幅度超过了 Flan T5 相对于原始 T5 的提升，这意味着 MoE 模型可能从指令式微调中获益更多，甚至超过了稠密模型。此外，MoE 在多任务学习中表现更佳。与之前关闭 <strong>辅助损失</strong> 函数的做法相反，实际上这种损失函数可以帮助防止过拟合。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/v2-8e883f442563dd910f591a8be7574979_1440w-3326037b488ba9ab4d7b16620659cf24.jpg" width="855" height="381" class="img_ev3q">与稠密模型相比，稀疏模型从指令微调中受益更多。该图来自 MoEs Meets instructions Tuning 论文</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="57-稀疏-vs-稠密如何选择">5.7-稀疏 VS 稠密，如何选择?<a href="#57-稀疏-vs-稠密如何选择" class="hash-link" aria-label="Direct link to 5.7-稀疏 VS 稠密，如何选择?" title="Direct link to 5.7- 稀疏 VS 稠密，如何选择?">​</a></h3>
<p>稀疏混合专家模型 (MoE) 适用于拥有多台机器且要求高吞吐量的场景。在固定的预训练计算资源下，稀疏模型往往能够实现更优的效果。相反，在显存较少且吞吐量要求不高的场景，稠密模型则是更合适的选择。</p>
<p><strong>注意</strong>: 直接比较稀疏模型和稠密模型的参数数量是不恰当的，因为这两类模型基于的概念和参数量的计算方法完全不同。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="6-优化-moe">6-优化 MoE<a href="#6-优化-moe" class="hash-link" aria-label="Direct link to 6-优化 MoE" title="Direct link to 6-优化 MoE">​</a></h2>
<p><strong>最初的混合专家模型 (MoE) 设计采用了分支结构，这导致了计算效率低下</strong>。这种低效主要是<font color="red"><b>因为 GPU 并不是为处理这种结构而设计的，而且由于设备间需要传递数据，网络带宽常常成为性能瓶颈</b></font>。接下来，我们会讨论如何使这些模型在预训练和推理阶段更加高效和实用。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="61-并行计算">6.1-并行计算<a href="#61-并行计算" class="hash-link" aria-label="Direct link to 6.1-并行计算" title="Direct link to 6.1-并行计算">​</a></h3>
<p>让我们简要回顾一下并行计算的几种形式:</p>
<ul>
<li><strong>数据并行</strong>: 相同的权重在所有节点上复制，数据在节点之间分割。</li>
<li><strong>模型并行</strong>: 模型在节点之间分割，相同的数据在所有节点上复制。</li>
<li><strong>模型和数据并行</strong>: 我们可以在节点之间同时分割模型和数据。注意，不同的节点处理不同批次的数据。</li>
<li><strong>专家并行</strong>: 专家被放置在不同的节点上。如果与数据并行结合，每个节点拥有不同的专家，数据在所有节点之间分割。</li>
</ul>
<p>在专家并行中，专家被放置在 不同的节点上，每个节点处理不同批次的训练样本。对于非 MoE 层，专家并行的行为与数据并行相同。对于 MoE 层，序列中的令牌被发送到拥有所需专家的节点。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/v2-559ecbf9d28630d977c32051bca026d5_1440w-27f1785c9e634e3eb76da0348b413107.jpg" width="824" height="487" class="img_ev3q">Switch Transformers 论文中展示如何使用不同的并行技术在节点上分割数据和模型的插图</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="62-容量因子和通信开销">6.2-容量因子和通信开销<a href="#62-容量因子和通信开销" class="hash-link" aria-label="Direct link to 6.2-容量因子和通信开销" title="Direct link to 6.2-容量因子和通信开销">​</a></h3>
<p>提高容量因子 (Capacity Factor, CF) 可以增强模型的性能，但这也意味着更高的通信成本和对保存激活值的显存的需求。在设备通信带宽有限的情况下，选择较小的容量因子可能是更佳的策略。一个合理的初始设置是采用 Top-2 路由、1.25 的容量因子，同时每个节点配置一个专家。在评估性能时，应根据需要调整容量因子，以在设备间的通信成本和计算成本之间找到一个平衡点。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="63-部署技术">6.3-部署技术<a href="#63-部署技术" class="hash-link" aria-label="Direct link to 6.3-部署技术" title="Direct link to 6.3-部署技术">​</a></h3>
<p>部署混合专家模型 (MoE) 的一个关键挑战是其庞大的参数规模。对于本地使用情况，我们可能希望使用更小的模型。为了使模型更适合部署，下面是几种有用的技术:</p>
<ul>
<li>预先蒸馏实验: Switch Transformers 的研究者们进行了预先蒸馏的实验。他们通过将 MoE 模型蒸馏回其对应的稠密模型，成功保留了 30-40%的由稀疏性带来的性能提升。预先蒸馏不仅加快了预训练速度，还使得在推理中使用更小型的模型成为可能。</li>
<li>任务级别路由: 最新的方法中，路由器被修改为将整个句子或任务直接路由到一个专家。这样做可以提取出一个用于服务的子网络，有助于简化模型的结构。</li>
<li>专家网络聚合: 这项技术通过合并各个专家的权重，在推理时减少了所需的参数数量。这样可以在不显著牺牲性能的情况下降低模型的复杂度。</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="64-高效训练">6.4-高效训练<a href="#64-高效训练" class="hash-link" aria-label="Direct link to 6.4-高效训练" title="Direct link to 6.4-高效训练">​</a></h3>
<p>FasterMoE 深入分析了 MoE 在不同并行策略下的理论性能极限，并且探索了一系列创新技术，包括用于专家权重调整的方法、减少延迟的细粒度通信调度技术，以及一个基于最低延迟进行专家选择的拓扑感知门控机制。这些技术的结合使得 MoE 运行速度提升高达 17 倍。</p>
<p>Megablocks 则专注于通过开发新的 GPU kernel 来处理 MoE 模型中的动态性，以实现更高效的稀疏预训练。其核心优势在于，它不会丢弃任何令牌，并能高效地适应现代硬件架构 (支持块稀疏矩阵乘)，从而达到显著的加速效果。Megablocks 的创新之处在于，它不像传统 MoE 那样使用批量矩阵乘法 (这通常假设所有专家形状相同且处理相同数量的令牌)，而是<strong>将 MoE 层表示为块稀疏操作，可以灵活适应不均衡的令牌分配</strong>。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/v2-a6b366fd6c12b8ee88695616be9410da_1440w-f1c46b6f0626410920f532f0d21104a3.jpg" width="1080" height="308" class="img_ev3q">针对不同规模的专家和令牌数量的块稀疏矩阵乘法。该图来自 MegaBlocks 论文</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="7-开源混合专家模型">7-开源混合专家模型<a href="#7-开源混合专家模型" class="hash-link" aria-label="Direct link to 7-开源混合专家模型" title="Direct link to 7-开源混合专家模型">​</a></h2>
<p>目前，下面这些开源项目可以用于训练混合专家模型 (MoE):</p>
<ul>
<li>Megablocks: <a href="https://link.zhihu.com/?target=https%3A//github.com/stanford-futuredata/megablocks" target="_blank" rel="noopener noreferrer">https://github.com/stanford-futuredata/megablocks</a></li>
<li>Fairseq: <a href="https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/fairseq/tree/main/examples/moe_lm" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm</a></li>
<li>OpenMoE: <a href="https://link.zhihu.com/?target=https%3A//github.com/XueFuzhao/OpenMoE" target="_blank" rel="noopener noreferrer">https://github.com/XueFuzhao/OpenMoE</a></li>
</ul>
<p>对于开源的混合专家模型 (MoE)，可以:</p>
<ul>
<li>Switch Transformers (Google): 基于 T5 的 MoE 集合，专家数量从 8 名到 2048 名。最大的模型有 1.6 万亿个参数。</li>
<li>NLLB MoE (Meta): NLLB 翻译模型的一个 MoE 变体。</li>
<li>OpenMoE: 社区对基于 Llama 的模型的 MoE 尝试。</li>
<li>Mixtral 8x7B (Mistral): 一个性能超越了 Llama 2 70B 的高质量混合专家模型，并且具有更快的推理速度。此外，还发布了一个经过指令微调的模型。有关更多信息，可以在 Mistral 的 公告博客文章 中了解。</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="8-moe可能面临的问题">8-MoE可能面临的问题<a href="#8-moe可能面临的问题" class="hash-link" aria-label="Direct link to 8-MoE可能面临的问题" title="Direct link to 8-MoE可能面临的问题">​</a></h2>
<ol>
<li>训练复杂性：混合专家模型的训练相对复杂，尤其是涉及到门控网络的参数调整。为了正确地学习专家的权重和整体模型的参数，可能需要更多的训练时间。</li>
<li>超参数调整：选择适当的超参数，特别是与门控网络相关的参数，以达到最佳性能，是一个复杂的任务。这可能需要通过交叉验证等技术进行仔细调整。</li>
<li>专家模型设计：专家模型的设计对模型的性能影响显著。选择适当的专家模型结构，确保其在特定任务上有足够的表现力，是一个挑战。</li>
<li>稀疏性失真：在某些情况下，为了实现稀疏性，门控网络可能会过度地激活或不激活某些专家，导致模型性能下降。需要谨慎设计稀疏性调整策略，以平衡效率和性能。</li>
<li>动态性问题：在处理动态或快速变化的数据分布时，门控网络可能需要更加灵活的调整，以适应输入数据的变化。这需要额外的处理和设计。</li>
<li>对数据噪声的敏感性：混合专家模型对于数据中的噪声相对敏感，可能在一些情况下表现不如其他更简单的模型。</li>
<li>专家冲突：在MoE模型训练中，处理专家冲突主要通过门控机制和稀疏性策略实现。门控机制根据专家的预测准确度分配权重，让表现好的专家获得更多权重，从而减少冲突。同时，稀疏性策略只激活部分专家，降低计算复杂度并进一步减少冲突，使模型更高效地处理大规模数据集和适应新任务。</li>
</ol>
<p>此外，还有重要的一点是混合专家模型在分布式计算环境下可能面临通信宽带瓶颈的问题。这主要涉及到混合专家模型的分布式部署，其中不同的专家模型或门控网络可能分布在不同的计算节点上。在这种情况下，模型参数的传输和同步可能导致通信开销过大，成为性能瓶颈。</p>
<p><img decoding="async" loading="lazy" alt="center|500" src="/assets/images/v2-063e21031e458f2a1e48c0d5e181d273_1440w-39ec3179f66af4ec88ea95c02cec49cf.jpg" width="930" height="770" class="img_ev3q"></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://buaaer-xing.github.io/docs/blogs/6-Transformer学习/2-MOE/1-混合专家模型MoE.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/blogs/Transformer学习/DeepSeek/论文精读/DeepSeek R1"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">DeepSeek R1</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/blogs/Transformer学习/MOE/MoE中的All-to-All算子"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">MoE中的All-to-All算子</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#0-introduction" class="table-of-contents__link toc-highlight">0-Introduction</a></li><li><a href="#1-什么是混合专家模型" class="table-of-contents__link toc-highlight">1-什么是混合专家模型？</a><ul><li><a href="#11-混合专家模型moe" class="table-of-contents__link toc-highlight">1.1-混合专家模型MoE</a></li><li><a href="#12-moe的优点" class="table-of-contents__link toc-highlight">1.2-Moe的优点</a></li><li><a href="#13-混合专家模型简史" class="table-of-contents__link toc-highlight">1.3-混合专家模型简史</a></li></ul></li><li><a href="#2-混合专家模型-moe-的关键研究" class="table-of-contents__link toc-highlight">2-混合专家模型 (MoE) 的关键研究</a></li><li><a href="#3-moe模型结构" class="table-of-contents__link toc-highlight">3-MOE模型结构</a><ul><li><a href="#31-门控网络gatenet" class="table-of-contents__link toc-highlight">3.1-门控网络GateNet</a></li><li><a href="#32-专家experts" class="table-of-contents__link toc-highlight">3.2-专家Experts</a></li></ul></li><li><a href="#4-moe与稀疏性" class="table-of-contents__link toc-highlight">4-MOE与稀疏性</a><ul><li><a href="#41-稀疏性" class="table-of-contents__link toc-highlight">4.1-稀疏性</a></li><li><a href="#42-moe对稀疏性的处理" class="table-of-contents__link toc-highlight">4.2-MoE对稀疏性的处理</a></li><li><a href="#43-混合专家模型中令牌的负载均衡" class="table-of-contents__link toc-highlight">4.3-混合专家模型中令牌的负载均衡</a></li></ul></li><li><a href="#5-moe-和-transformer" class="table-of-contents__link toc-highlight">5-MoE 和 Transformer</a><ul><li><a href="#51-gshard" class="table-of-contents__link toc-highlight">5.1-Gshard</a></li><li><a href="#52-switch-transformers" class="table-of-contents__link toc-highlight">5.2-Switch Transformers</a></li><li><a href="#53-用-router-z-loss-稳定模型训练" class="table-of-contents__link toc-highlight">5.3-用 Router z-loss 稳定模型训练</a></li><li><a href="#54-专家如何学习" class="table-of-contents__link toc-highlight">5.4-专家如何学习？</a></li><li><a href="#55-专家的数量对预训练有何影响" class="table-of-contents__link toc-highlight">5.5-专家的数量对预训练有何影响?</a></li><li><a href="#56-微调混合专家模型" class="table-of-contents__link toc-highlight">5.6-微调混合专家模型</a></li><li><a href="#57-稀疏-vs-稠密如何选择" class="table-of-contents__link toc-highlight">5.7-稀疏 VS 稠密，如何选择?</a></li></ul></li><li><a href="#6-优化-moe" class="table-of-contents__link toc-highlight">6-优化 MoE</a><ul><li><a href="#61-并行计算" class="table-of-contents__link toc-highlight">6.1-并行计算</a></li><li><a href="#62-容量因子和通信开销" class="table-of-contents__link toc-highlight">6.2-容量因子和通信开销</a></li><li><a href="#63-部署技术" class="table-of-contents__link toc-highlight">6.3-部署技术</a></li><li><a href="#64-高效训练" class="table-of-contents__link toc-highlight">6.4-高效训练</a></li></ul></li><li><a href="#7-开源混合专家模型" class="table-of-contents__link toc-highlight">7-开源混合专家模型</a></li><li><a href="#8-moe可能面临的问题" class="table-of-contents__link toc-highlight">8-MoE可能面临的问题</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/paper_notes_intro">论文笔记</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/blogs_intro">个人博客</a></li><li class="footer__item"><a class="footer__link-item" href="/blog">相关内容</a></li><li class="footer__item"><a class="footer__link-item" href="/resume">个人简历</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://t.me/cx_cst" target="_blank" rel="noopener noreferrer" class="footer__link-item">Telegram<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://blog.csdn.net/qq_45575167" target="_blank" rel="noopener noreferrer" class="footer__link-item">CSDN</a></li><li class="footer__item"><a href="https://github.com/BUAAer-xing" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 BUAAer-xing, 此网站使用 Docusaurus 进行构建✨</div></div></div></footer></div>
</body>
</html>