## 8.1-简介

了解多CPU和多GPU，可以更加有效地调度和分配任务。

## 8.2-局部性

局部性原理用在GPU和CPU上是相当不错的。接近设备的内存(GPU上的共享内存或 CPU上的高速缓存)会更快地访问。一个卡槽内（比如核之间）的通信远远快于和不同卡槽上的另一核的通信。访问另一个节点核的通信方式至少比该节点内的访问慢一个数量级。 

## 8.3-多CPU系统

略

## 8.4-多GPU系统-选择GPU

在编码过程中，通过cuda函数`cudaSetDevice(int device_num)`来选择需要运行程序的GPU设备。参数device_num是一个从零（默认设备）到系统中所含有设备数量的一个数字。

```cpp
cudaError_t cudaSetDevice(int device_num);
```

查询系统中所含有设备的数量：

```cpp
int num_devices;
CUDA CALL(cudaGetDeviceCount(&num_devices));
```

查询某个设备的详细信息：

```cpp
cudaError_t cudaGetDeviceProperties(struct cudaDeviceProp* properties, int device);
// example
struct cudaDeviceProp device_0_prop; 
CUDA_CALL(cudaGetDeviceProperties(&device_0_prop,0));
```

properties的结构特性：

![](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/202409062118822.png)

比较重要的几个属性是：

| 结构体成员                             | 意义             | 单位  |     |
| --------------------------------- | -------------- | --- | --- |
| `char name[256];`                 | 设备名称，如GTX460   | 字符串 |     |
| `size_t totalGlobalMem;`          | 设备全局内存最大值      | 字节  |     |
| `size_t sharedMemPerBlock;`       | 每个块支持的共享内存最大值  | 字节  |     |
| `int regsPerBlock;`               | 每个块支持的寄存器数量最大值 | 寄存器 |     |
| `int warpSize;`                   | 设备的线程束大小       | 线程  |     |
| `int maxThreadsPerBlock;`         | 每个块支持的线程数量最大值  | 线程  |     |
| `int multiProcessorCount`         | 设备上SM的数量       | int |     |
| `int memoryBusWidth`              | 内存总线宽度         | 位   |     |
| `int maxThreadsPerMultiProcessor` | 一个SM支持线程的最大数目  | 线程  |     |
|                                   |                |     |     |

## 8.7 单节点系统

略

## 8.8 流

### 8.8.1 流机制

流是GPU上的虚拟工作队列。它们用于异步操作。每个流可以被看作是一条任务执行的“流水线”，通过流的概念，可以实现任务的并行化和异步化执行。流主要用于控制内核函数（kernel）、数据传输和同步操作的调度。

 GPU中的流的特点与作用：
1. **并行执行**：
   在同一个GPU设备上，不同的流可以并行执行，从而提高资源利用率。比如，在一个流中执行内核计算时，另一个流可以进行数据传输，避免计算与传输的相互阻塞。
2. **异步执行**：
   GPU中的大多数操作可以通过流进行异步执行。这意味着启动了某个操作后，主机程序不需要等待该操作完成，可以立即继续执行其他任务。这样的异步执行能显著提升整体的计算效率。
3. **任务调度与依赖**：
   在同一个流中，操作是按照顺序依次执行的，即使这些操作是异步的。但不同的流之间则可以独立进行，不存在顺序依赖关系。通过合理的流调度，程序可以在计算与数据传输之间进行优化，实现更高的并行度。
4. **默认流与自定义流**：
   每个GPU设备都有一个默认流（Stream 0），如果不显式指定流，所有的操作都会在默认流中执行。而通过创建自定义流，可以使得不同的操作在不同的流中独立执行，从而实现并行化。
   
流的典型使用场景：
1. **计算与数据传输并行**：
   在CUDA编程中，通常需要将数据从主机（CPU）传输到设备（GPU），然后进行计算，再将结果传回主机。如果所有这些操作都在一个流中执行，数据传输和计算会顺序进行。而通过使用不同的流，数据传输和计算可以并行进行，从而提高效率。
2. **多个内核函数的并行执行**：
   如果程序中需要执行多个内核函数（kernel），可以将它们分别分配到不同的流中，从而利用GPU的并行计算能力，实现更高的吞吐量。
3. **减少同步开销**：
   当多个任务没有严格的依赖关系时，可以将它们分配到不同的流中，避免不必要的同步操作，提高执行效率。

示例：

在CUDA中，创建流的常见方式如下：
```cpp
cudaStream_t stream1, stream2;
cudaStreamCreate(&stream1);
cudaStreamCreate(&stream2);
// 在stream1中执行内核1
kernel1<<<gridDim, blockDim, 0, stream1>>>(...);
// 在stream2中执行内核2
kernel2<<<gridDim, blockDim, 0, stream2>>>(...);
// 在stream1中执行数据传输
cudaMemcpyAsync(dest, src, size, cudaMemcpyHostToDevice, stream1);
// 等待所有流完成
cudaStreamSynchronize(stream1);
cudaStreamSynchronize(stream2);
cudaStreamDestroy(stream1);
cudaStreamDestroy(stream2);
```
通过上述代码，两个流 `stream1` 和 `stream2` 中的操作可以并行执行，而不会互相阻塞。

对于大多数程序，每个设备至少使用两个流能够有助于提高整体吞吐量。

### 8.8.2 锁页内存

注：异步传输的背景

通常情况下，当使用 cudaMemcpy 进行数据传输时，CPU 会发起从主机内存（Host Memory）到设备内存（Device Memory）的拷贝请求。然而，**同步数据传输**意味着在数据从主机传输到设备的过程中，CPU必须等待传输完成才能继续执行后续的任务。这样，CPU 资源在数据传输期间可能会闲置，造成性能浪费。而通过 **异步数据传输**（如 cudaMemcpyAsync），<font color='red'><b>可以让 CPU 发起数据传输后立即返回继续执行其他任务，而无需等待传输完成</b></font>。这种异步操作可以与计算任务并行进行，显著提升系统的整体计算效率。

为了实现上面提到的效果，就需要借助锁页内存，**锁页内存在异步传输中的作用在于它确保主机内存的物理位置是固定的，操作系统不会将这部分内存换出到磁盘**。由于**异步传输要求在传输过程中主机内存的数据地址保持不变**，因此<font color='red'><b>锁页内存是异步数据传输的前提条件</b></font>。

----

锁页内存🆚普通分页内存：
• **普通分页内存**：当**使用分页内存时，操作系统有可能将数据暂时换出到虚拟内存（如硬盘），从而影响数据传输的稳定性和一致性**。因此，在分页内存中无法实现真正的异步数据传输，因为**操作系统可能在数据传输时换出或调度内存页面，导致性能下降，甚至引发错误**。
• **锁页内存**：通过使用锁页内存，操作系统**保证这部分内存始终驻留在物理内存中，数据传输时不必担心地址的变化或数据的换出**。因此，GPU可以直接从主机内存中读取数据，GPU和CPU可以并行工作，而不必互相等待。

---

在GPU编程中，**锁页内存（Pinned Memory 或 Page-locked Memory）** 是指一块系统内存被锁定，确保操作系统不会将它换出到虚拟内存（如硬盘）中。这种内存的主要优势在于<font color='red'><b>加速主机（CPU）与设备（GPU）之间的数据传输</b></font>。

锁页内存的特点与优势：
1. **加速数据传输**：
	- 当使用分页内存（Pageable Memory）时，主机和GPU之间的数据传输通常涉及多个步骤。CUDA会先将数据从分页内存拷贝到锁页内存中，然后再从锁页内存传输到GPU。这会产生额外的开销。而锁页内存能够避免这一步骤，直接进行数据传输，因此速度更快。
2. **异步数据传输**：
	- 锁页内存允许异步的数据传输操作。当使用cudaMemcpyAsync等异步传输函数时，锁页内存可以在数据传输的同时进行其他操作，无需等待传输完成，从而提升计算效率。
3. **提升系统内存与GPU内存的协作**：
	- 锁页内存与GPU直接交互的能力增强了内存传输的效率和响应速度，尤其适合那些需要频繁传输数据的场景，如实时数据处理或高性能计算。

使用锁页内存的代价：
1. **物理内存占用**：
	- 锁页内存占据系统的物理内存资源，不能被操作系统换出到虚拟内存。这会导致系统可用的物理内存减少，从而影响其他进程的内存使用。因此，滥用锁页内存可能会导致系统性能下降，尤其在内存资源紧张的情况下。
2. **分配时间较长**：
	- 分配锁页内存的时间可能比分页内存更长，尤其是在大规模分配时，因此需要权衡锁页内存的性能提升与分配开销。

锁页内存的使用场景：
- **大规模数据计算**：在需要大量数据传输的GPU程序中，比如科学计算或大规模数据处理，锁页内存可以显著提升数据传输的效率。
- **实时数据处理**：对于需要实时处理输入数据并传输到GPU的应用，如视频流处理或信号处理，锁页内存可以减少延迟，提升响应速度。
- <font color='red'><b>异步数据传输</b></font>：当需要主机和设备并行执行任务时，锁页内存能够与异步数据传输一起使用，以充分发挥GPU的并行计算能力。

示例：
```cpp
float *hostPtr;
cudaMallocHost((void**)&hostPtr, size);  // 分配锁页内存
// 异步传输数据到设备内存
cudaMemcpyAsync(devicePtr, hostPtr, size, cudaMemcpyHostToDevice, stream);
// 使用完毕后释放锁页内存
cudaFreeHost(hostPtr);
```

锁页内存为异步数据传输提供了一个稳定的物理内存区域，使得主机和设备之间的传输更加高效，避免了内存分页带来的额外开销。通过异步数据传输，CPU 和 GPU 可以并行执行不同的任务，最大限度地利用系统资源，从而提高整体计算效率。


## 8.9 多节点系统

单台计算机形成网络上的一个节点，将许多机器连接起来就得到了一个机器集群。通常，这种集群将由一套机架式节点组成，这个机架本身可以与一个或多个其他的机架相互连接。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240906221524.png)


为了使用这样的系统上，我们需要一种通信机制以允许将工作调度到给定的CPU/ GPU集合上，而不用关心它们处于网络的什么地方。为了达到这一目的，可以使用 **ZeroMQ**，它是一个非常轻量级的和相当容易使用的通信函数库。






