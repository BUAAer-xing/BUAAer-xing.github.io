## 5.1 简介

英伟达的硬件调度方式可以归为SPMD（单程序多数据），属于SIMD的一种变体。从某些方面来说，这种调度方式的选择是基于英伟达自身底层硬件的实现。

并行编程的核心是线程的概念，<font color='red'><b>一个线程就是程序中一个单一的执行流</b></font>。一个个线程组合在一起就形成了并行程序。

CUDA的编程模型将线程组合在一起形成了线程束、线程块以及线程网格。

## 5.2 线程

### 5.2.1 问题分解

线程是并行程序的基本构建块。在过去的很多年里，大多数程序员编写的程序都是单线程的，因为当时运行程序的CPU也是单核的。但是，在现在来看，面对大量的算力以及数据处理能力的要求，想要提高程序的速度，就必须考虑并行设计。

一个显而易见的思想是：当只有数量较少的强劲设备时，例如在CPU上，最重要的问题是：**解决平均分配工作量的问题**。当然，这个问题很好解决，因为毕竟设备的设备较少。但如果像GPU那样拥有大量较小设备时，尽管也能很好的平均工作量，但是却<font color='red'><b>需要花大量的精力在同步和协调</b></font>上。

### 5.2.2 CPU与GPU的不同

执行的任务性质不同：
- CPU的设计是用来运行少量比较复杂的任务
	- 主要是针对执行大量离散而不相关任务的系统
- GPU的设计是用来运行大量比较简单的任务
	- 主要是针对解决那些可以分解成许多个小块并可独立运行的问题。

CPU与GPU支持线程的方式不同。
- CPU的每个核只有少量的寄存器，每个寄存器都将在执行任何已分配的任务中被用到。
	- 为了能执行不同的任务，CPU将在任务与任务之间进行快速的上下文切换。从时间的角度来看，CPU上下文切换的代价是非常昂贵的，**因为每一次上下文切换都要将寄存器组里的数据保存到RAM中，等到重新执行这个任务时，又从RAM 中恢复**。
- GPU同样用到上下文切换这个概念，但<font color='red'><b>它拥有多个寄存器组而不是单个寄存器组</b></font>。
	- 因此，一次上下文切换只需要设置一个寄存器组调度者，用于将当前寄存器组里的内容换进、换出，**它的速度比将数据保存到RAM中要快好几个数量级**。

CPU与GPU处理失速状态（这种现象通常是由**I/O操作**和**内存获取**引起的）：
- CPU的调度策略是基于时间分片，将时间平均分配给每个线程。一旦线程的数量增加，上下文切换的时间百分比就会增加，那么效率就会急剧的下降。
- GPU就是专门设计用来处理这种失速状态，并且预计这种现象会经常发生。
	- GPU采用的是数据并行的模式，它需要成千上万的线程，从而实现高效的工作。它利用有效的<font color='red'><b>工作池</b></font>来保证一直有事可做，不会出现闲置状态。
	- 当GPU遇到内存获取操作或在等待计算结果时，**流处理器就会切换到另一个指令流**，而在之后再执行之前被阻塞的指令。

CPU和GPU的一个主要差别就是每台设备上处理器数量的巨大差异，也可以说是每一次所有设备上的处理器共同运行，所产生的<font color='red'><b>数据吞吐量的差异</b></font>！
- 假设CPU是双核或四核，而CPU通常运行的是单线程的程序，即它的**每个核的每次迭代仅仅计算一个数据**。
- GPU上的每个SM可以看作是CPU上的一个核心，并且，GPU默认的就是并行的模式，它的**SM每次可同时计算32个数**（这个数量需要根据wrap的大小来进行确定）。也就是说，GPU上的每个SM每次可以计算32个数据。可以看出，GPU的数据吞吐量将远远大于CPU。

而且，值得注意的是，GPU为<font color='red'><b>每个SM提供了唯一并且高速的存储器，即共享内存</b></font>。它为设备中的SM提供了在标准寄存器文件之外的**本地工作区**。因此，可以安心地将数据留在内存中，不必担心由于上下文切换操作需要将数据移出去，另外，共享内存也为SM中线程之间的通讯提供了重要机制。


### 5.2.3 任务执行模式

任务执行的模式主要有两种。
- 一种基于锁步（lock-step）思想，执行N个SP（SM中的更小一级的处理单元）组，**每个SP都执行数据不同的相同程序**。
	- 传统的CPU会将一个单独的指令流分配到每个CPU核心中，而GPU所用的SPMD模式是将同一条指令送到N个逻辑执行单元。
- 另一种则是利用巨大的寄存器文件，使线程的切换高效并且达到零负载。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240829232255.png)

### 5.2.4 GPU线程

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240829232615.png)

如上图CPU代码和GPU的kernel代码。可以看出 i 不再是循环控制变量，而是用来<font color='red'><b>标识当前所运行的线程</b></font>的一个变量。CUDA提供一个特殊的变量，它在每个线程中的值不一样，使得它可以标识每一个线程，这就是线程的索引--线程ID。从而，可以直接将这个线程标号用作数组的下标对数组进行访问。

从上面的代码中可以看到，每个线程执行的代码是相同的，但是数据却是不同的，这就是CUDA的核心--SPMD模型。

### 5.2.5 硬件初探索

假设目前GPU上有一个SM，那如何运行128个线程呢，是一起运行吗？这里需要理清代码执行的逻辑，在对数据进行运算之前，首先要做的就是将数据从内存中取出，但这一阶段是十分耗时的，那有什么办法可以解决的？这就要想到CPU中的流水线机制了，从而实现一定的overlap操作。

在NVIDIA GPU中，线程都是以每32个一组，当所有的32个线程都在等待诸如内存读取这样的操作时，它们就会被挂起。换另一组线程组去到计算单元进行计算。在术语上，这些线程组叫做<font color='red'><b>线程束</b></font>（32个线程）。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240829234151.png)

### 5.2.6 CUDA内核

调用内核时，必须按照下面的语法：

$$\text{kernel\_function}<<<\text{num\_blocks},\text{num\_threads}>>>(\text{param1},\text{param2},...)$$

需要注意的是，参数num_thread表示执行内核函数的线程数量。由于受到硬件的限制，早期的一些设备**在一个线程块（SM）中最多支持512个线程**，而在后期出现的一些设备中则最多可支持1024个线程。（寄存器数量限制等）

一个SM上的共享内存大小以及寄存器的数量是有限制的，故当一个SM上的线程数量太多时，会导致每个线程分配到的资源较少，即使可以使用warp（32个线程）来进行流水线的overlap，但也会导致速率的下降。当一个SM上的线程数量太少时，会导致线程束的数量较少，从而导致在访问内存时，所有的线程束都处于挂起状态，导致SM的计算单元空闲。因此，在实际的编程过程中，需要尽量避免SM闲置状态的出现。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240830000742.png)

在后面可以看到，为了最大限度的利用设备，使程序性能得到提升，应尽量将**每个线程块开启的线程数设置为**<font color='red'><b>192</b></font>**或**<font color='red'><b>255</b></font>。

## 5.3 线程块

线程块是CUDA中<font color='red'><b>编程的基本单元</b></font>，是CUDA中的基本调度单元，它是一个由多个线程组成的三维线程网格。一个CUDA内核函数中的所有线程被组织成若干个线程块。

每个线程块中的线程可以通过SM内的共享内存进行通信和数据共享。同一个线程块中的线程还可以通过同步指令（`__syncthread()`）进行同步。一个线程块中的最大线程数通常是1024（取决于GPU的计算能力），并且线程块的大小是由程序员在启动内核时进行指定的。

	一个SM上可能具有多个线程块在运行，一个线程块可能有多个warp，但只有同一个线程块内的线程可以利用共享内存进行通信和共享数据，即使有可能两个线程块都在同一个SM上。线程块就是逻辑上的独立，是逻辑上的分块，如何调度是硬件设计的范围。


---

举个例子（SM、线程块、线程束的关系）：

**Ampere架构（A100）**：
- 每个SM最多可以执行2048个线程，每个SM可以支持最多64个<font color='red'><b>并发执行</b></font>的Warp（每个Warp32个线程，Warp中的32个线程是<font color='red'><b>并行</b></font>执行的！）
- 并发是指在同一时间段内，多个任务**交替**执行。并发系统可以在不同任务之间切换执行，并不一定同时执行多个任务。

可以并发执行多个Warp的原因：（CPU中的流水线机制）
SM能够并发执行多个Warp的能力，源于其硬件架构设计的初衷：**最大化资源利用率和隐藏延迟**。通过并发调度多个Warp，CUDA架构可以<font color='red'><b>充分利用硬件资源</b></font>，保持高吞吐量，并<font color='red'><b>减少因各种延迟带来的性能瓶颈</b></font>。这种设计使得CUDA程序能够在大规模并行计算中高效执行。

每个SM包含多个计算核心，用于执行各种类型的指令（如整数、浮点数、特殊功能指令等）。这些计算核心通常分为几类，包括标量处理器（SP，Scalar Processor）和特殊功能单元（SFU，Special Function Unit）。

在A100中，每个SM包含64个标量处理器（SP），4个特殊功能单元以及8个Tensor Core。如果时机合适的话，可以通过调度，使得有两个Warp（2\*32个线程）同时在A100的某个SM上同时进行计算。

以后只需要记住，线程块内的线程以及各个线程块之间的线程可以最大程度的并发执行，一个Warp内的线程是并行执行的，就可以。至于如何调度，如何最大效率的利用硬件，这是硬件层面的事情了。（当然，还是要设计更适合并发以及并行执行的数据结构存储以及算法，来和硬件做配合才是最好的！！！）

---

## 5.4 线程网格

1. **线程、线程块、线程网格的关系**
   - **线程（Thread）**：这是CUDA中的最基本执行单元，负责执行代码中的一小部分任务。每个线程可以处理一个数据元素，例如计算一个像素的颜色或矩阵中的一个元素。
   - **线程块（Block）**：线程块是由多个线程组成的组。可以把它看作是表格中的一小部分区域，比如一行或一块区域。每个线程块中的线程可以互相合作、共享数据，并通过共享内存进行通信。
   - **线程网格（Grid）**：线程网格是由多个线程块组成的更大的结构，类似于整个表格或棋盘。线程网格中的所有线程块一起完成整个任务。
2. **线程网格的用途**
	线程网格允许我们将一个大任务分解为多个小任务，并将这些小任务分配给多个线程并行处理。例如，在处理一个大图片时，我们可以将图片分割成许多小块，每个小块由一个线程块处理，而每个线程块中的每个线程处理小块中的一个像素。
 3. **线程网格的维度**
	线程网格可以是一维（1D）、二维（2D）或三维（3D）的，这取决于问题的需要。例如：
	   - **一维网格**：适合处理一列或一行的数据，如数组或列表。
	   - **二维网格**：适合处理二维数据，如图像或矩阵。
	   - **三维网格**：适合处理三维数据，如体积数据或3D网格。
 4. **定义线程网格**
	在编写CUDA代码时，你需要定义线程网格的大小和线程块的大小。CUDA会根据这些定义自动将任务分配给每个线程。
```cpp
dim3 blockSize(16, 16);  // 16x16的线程块
dim3 gridSize(32, 32);   // 32x32的线程网格
myKernel<<<gridSize, blockSize>>>(...);
```
这里的代码定义了一个32x32的线程网格，每个网格中有16x16的线程块。

## 5.5 线程束

<font color='red'><b>线程束是GPU的基本执行单元</b></font>。每一组线程或每个线程束中的线程同时执行。在理想状态下，获取当前指令只需要一次访存，然后将指令广播到这个线程束所占用的所有SP（标量处理器）中。

当前，GPU上的一个线程束的大小为32，可以通过获取变量`wrapSize`，来获得当前硬件所支持线程束的大小。

### 5.5.1 分支

一个线程束是一个单独的执行单元，使用分支（例如：if、else、for、while、do、swith等语句）可以产生不同的执行流。

GPU执行分支的逻辑是：
1. **Warp内所有线程一起执行同一指令**：
	- 在CUDA架构中，Warp是执行的基本单位，每个Warp包含32个线程。在没有分支的情况下，Warp内的所有线程都会同步执行同一条指令。
2. **遇到条件分支时的处理**：
	- 当Warp中的线程遇到条件分支（如if-else或switch语句）时，不同线程可能会对条件表达式做出不同的判断，从而导致<font color='red'><b>分支发散</b></font>。比如，某些线程进入if分支，而另一些线程进入else分支。
3. **分支发散的处理方式**：
	- 如果一个Warp中的线程在分支条件下发生了发散，CUDA硬件会将这些线程分组，分开执行每条分支路径。具体处理步骤如下：
		- **屏蔽执行**：Warp内的线程被分成多个子组，每个子组中的线程执行相同的分支路径。执行过程中，未执行当前路径的线程会被屏蔽（即不执行任何指令）。
		- **逐个执行分支路径**：CUDA硬件首先执行一组线程的指令（如进入if路径的线程），然后执行另一组线程的指令（如进入else路径的线程）。<font color='red'><b>对于每一个路径，屏蔽掉不需要执行该路径的线程</b></font>。
		- **汇合（Re-convergence）**：当所有的分支路径都执行完毕后，Warp中的所有线程会再次汇合，继续同步执行接下来的指令。
4. **性能影响**：
	- 分支发散会导致Warp中的部分线程在执行某些路径时被屏蔽，从而浪费计算资源。Warp的执行效率会因为发散而降低，因为每条分支路径都需要分别执行，这会增加整体的执行时间。
	- 如果没有分支发散（即所有线程走同一条路径），Warp中的所有线程能够同时执行，提高执行效率。

总而言之，就是这组warp要把所有涉及到的分支都走一遍，对于warp中的线程而言，如果该线程处于这个分支中，那么这个线程将会执行这个分支中的代码，如果这个线程不属于这个分支，那么，该线程将会被屏蔽。

### 5.5.2 GPU的利用率

需要关注线程束大小的另一个重要的原因是：防止GPU未被充分的利用。

CUDA的模式是用成千上万的线程来隐藏内存操作的延迟（从发出存储请求到完成访存操作所花的时间），比如，对全局内存的访问延迟一般是400-600个时钟周期，在这段时间里，GPU会忙于其他任务，而不是空闲地等待访存操作的完成。

SM一次能容纳的线程数量主要与底层的计算能力有关，当同时容纳的线程数量足够多时，就更容易填补访存操作的空闲，提升GPU的利用率。

正如前面所提到的，应该尽量将每个线程块开启的线程数设置为192或256个。此外，数据集也要和线程布局尽量一一对应，以达到更高的性能优化！

当然，每个线程块开启的线程数越多，就会增加该线程块中的warp数量，从而潜在的增加等待执行较慢的线程束的可能性。因为，当所有的线程没有到达同步点时，gpu是无法继续向下执行的。

## 5.6 线程块的调度

1. **线程块分配到SM**
- 当一个CUDA内核启动时，CUDA运行时（Runtime）会根据内核的网格（Grid）配置来确定需要启动的线程块总数。
- GPU有多个SM，线程块会被分配给这些SM进行执行。通常，每个SM可以并行执行多个线程块。
- 分配过程中，线程块的分配顺序通常不固定，而是由硬件调度器动态决定。线程块被分配到某个SM后，SM就会负责执行该线程块的所有线程，直到完成任务。
	- **占用（Occupancy）**：GPU的调度器会根据SM的资源情况来分配线程块。每个SM有有限的硬件资源，包括寄存器、共享内存、Warp调度器等。调度器会评估线程块的资源需求（如每个线程块需要的寄存器数量、共享内存大小）来决定能同时容纳多少个线程块。
	- **循环分配**：线程块通常按照循环分配的方式调度到各个SM。即如果有多个SM，线程块会轮流分配给它们，以尽量均匀地分布负载。
	- CUDA调度是动态的，这意味着调度器会根据当前的执行情况和资源使用情况来实时调整线程块的分配。例如，当某个SM的资源被释放时，它可以接收新的线程块进行执行。调度器还会考虑负载均衡和资源争用问题，确保所有SM尽可能地保持高占用率，以达到更好的性能。
2. **线程块的结束**
	- 当一个线程块中的所有线程完成执行后，SM会释放该线程块占用的资源，如寄存器和共享内存。
	- 释放资源后，SM会从队列中取出下一个未执行的线程块，并开始其执行过程。


## 5.7 Demo

结论：<font color='red'><b>原子操作对性能的影响很大</b></font>。

减少原子操作：

让每一个SM都计算出总任务的一小部分，最后再将所有的子任务汇总成一个，从而避免所有线程都直接汇总到总任务上的原子操作。
