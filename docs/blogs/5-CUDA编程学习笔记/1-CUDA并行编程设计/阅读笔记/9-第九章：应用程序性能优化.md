## 9.1 策略1：并行/串行在GPU/CPU上的问题分解

### 9.1.1 分析问题

略

### 9.1.2 时间

界定算法执行时间“可接受的”时间段是很重要的。目前，可接受的并不一定意味着是现实可能的最好时间。

### 9.1.3 问题分解

问题分解需要关注的点是：<font color='red'><b>这个问题是否可以被分解成并行运行的组块</b></font>？也就是说这个问题可以充分在GPU上去运行的关键是能发掘出问题中的并行性。

问题分解应该总是从数据开始，然后考虑执行的任务。看到问题潜在的并行性通常是第一个障碍。

### 9.1.4 依赖性

依赖关系主要有两种主要形式，要么**一个元素是依赖于它附近的若干元素**，要么**在多轮遍历数据集时下一轮依赖于当前轮**。

由于多次读取全局内存会比较耗费时间，因此，可以尝试将这些具有依赖关系的算法写成只涉及单个或少量目标数据点的内核程序，从而可以将数据放入到共享内存或寄存器中，这样，相较于给定内核需要较多次全局内存访问的方式，可以大大提高完成的工作量，从而明显改善多数内核的执行时间。

### 9.1.5 数据集的大小

GPU的主要问题不是缓存，而是能在一张GPU卡上保存多少数据。将<font color='red'><b>数据从主机系统传入或传出会耗费大量的计算时间</b></font>。为了隐藏这一时间，应该<font color='red'><b>把计算与数据传输重叠起来执行</b></font>。在更先进的显卡上，可以做到同时传入与传出数据。然而，为了实现这一点，需要使用主机上的<font color='red'><b>锁页内存</b></font>。由于锁页内存不会被虚拟内存管理系统换出，所以它必须是存在于主机上的真正的DRAM内存。

同时需要注意的是，节点间的通信是非常耗时的，相比任何内部的数据通信至少慢一个数量级，而且还需要掌握另一套通信的API，因此，如果问题可以放在单个节点上，最好避免节点间通信这一步骤。

### 9.1.6 分辨率

略

### 9.1.7 识别瓶颈

#### Amdahl定律

Amdahl定律（Amdahl's Law）是并行计算中的一个重要理论，用来描述系统中提升计算性能的潜在限制。它由计算机科学家Gene Amdahl在1967年提出，主要用于分析并行化程序的加速性能。

Amdahl定律的核心思想是：**在一个计算任务中，能被并行化的部分越大，加速效果就越明显；而不能并行化的部分对整体加速性能有极大的限制**。定律可以通过以下公式表示：

$$ S = \frac{1}{(1 - P) + \frac{P}{N}} $$
其中：
- $S$ 表示加速比（即加速后的执行时间与单线程执行时间的比值）；
- $P$ 是可以并行化的程序部分所占的比例；
- $N$ 是使用的并行处理器（或线程）的数量。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240908142525.png)

从这个公式可以看出，程序中不能并行化的部分（即 $1 - P$）是性能提升的瓶颈。即使增加更多的处理器，程序的总执行时间也会受到串行部分的制约，因此存在一个加速的上限。

Amdahl定律表明，**当并行化比例较小时，即使增加处理器的数量也难以显著提升性能**，所以，无限扩展程序的唯一办法是<font color='red'><b>消除程序执行中的所有串行瓶颈</b></font>。

比如，当考虑如计算直方图这样的算法时，如果把所有的线程都加入同一个桶里，就形成了同样的瓶颈。通常会采用**原子操作**，这样一组并行线程就要串行执行。相反，如果**分配给每个线程属于它自己的一组桶**，后面再将这些桶<font color='red'><b>合并</b></font>起来，就能消除串行瓶颈问题。

#### 分析

分析是确定当前在哪以及应该在什么地方多花点时间的最有用的任务之一。并且，优化应该根据确切的数字和事实，而不是猜测哪“可能”是最应该优化的地方。

获取分析信息，可以通过英伟达提供的两个工具：CUDA Profiler 和 Parallel Nsight，来提供分析信息。（PS：由于这本书写的比较早，所以现在更为推荐的工具是：**Nsight Systems** 和 **Nsight Compute**）

上述两个工具的主要区别在于：
1. **Nsight Systems**：
   - **定位**：用于<font color='red'><b>系统级的性能分析工具</b></font>。
   - **功能**：主要用于提供整个应用程序在 CPU 和 GPU 之间的时间分布情况。它能够帮助开发者找到程序在系统中的瓶颈，了解 CPU 和 GPU 之间的协作以及多线程、多进程应用程序的性能表现。
   - **分析内容**：包括进程间通信、内存带宽使用情况、I/O 活动等。它能生成 CPU 和 GPU 的时间线视图，帮助开发者发现潜在的性能瓶颈。
   - **适用场景**：适用于复杂、多线程、多进程的应用程序调优，尤其是需要从全局角度分析系统中 CPU 与 GPU 的交互时。
2. **Nsight Compute**：
   - **定位**：用于 CUDA 内核级的性能分析工具。
   - **功能**：主要专注于 CUDA 内核的性能剖析。它提供详细的 GPU 性能指标，如内存带宽、指令吞吐量、寄存器使用情况、内存延迟等。用户可以使用它来优化 CUDA 内核的执行效率。
   - **分析内容**：深入分析 CUDA 代码的执行效率，包括内核的每个阶段，帮助用户优化内存访问模式、指令使用和线程调度等。
   - **适用场景**：适用于开发者需要在细粒度上优化 CUDA 内核的执行效率时，尤其是关注特定内核函数的性能调优。
简而言之，**Nsight Systems** 是系统级的分析工具，侧重于整体性能和 CPU-GPU 协作的分析；而 **Nsight Compute** 则是针对 CUDA 内核的深入分析工具，侧重于 GPU 计算的性能优化。

典型的未优化的程序，80%的时间是花在20%的代码上的，优化这20%代码是有效减少使用时间的关键，分析器是确定这20%代码所在的一把钥匙。

### 9.1.8 CPU和GPU的任务分组

最好的应用程序往往可以充分利用CPU和GPU两者的优势，并相应的划分数据。任何基于GPU的优化也必须考虑CPU，因此这对于总的应用程序时间很重要。

所以，基于上述的思想，一种优化策略是，当迭代到一定的阀值，剩余部分的计算就转交给CPU来完成，事实上，如果CPU空闲并且剩余的数据量也不是很大，那么这种策略相比于等待GPU完成整个归约过程，会有显著的收益。

## 9.2 策略2：内存因素

### 9.2.1 内存带宽

<font color='red'><b>内存带宽</b></font>和<font color='red'><b>延迟</b></font>是所有应用程序都要考虑的关键因素，尤其是GPU应用程序。
- 带宽是指与某个给定目标之间传输的数据量。在GPU的情况下，**主要关心的是全局内存带宽**。
- 延迟则是指操作完成所用的时间。 

GPU上的**内存延迟设计为由运行在其他线程束中的线程所隐藏**。当线程束访问的内存位置不可用时，硬件向内存提交一次读或者写请求。**如果同一线程束上其他线程访问的是相邻内存位置并且内存区域的开始位置是对齐的，那么该请求会自动与这些线程的请求组合或者合并**。因此，如果线程没有访问连续的内存地址，会导致内存带宽快速的下降。例如，如果线程0读取地址0、1、2.....、31，线程1读取32、33、34、...、63地址，它们将不会被合并。

在费米架构中，每个内存读取事务获取32字节或128字节的内存，而不支持64字节的内存读取。在默认情况下，<font color='red'><b>每个内存事务就是一个128字节的缓存行</b></font>（cache line）读取，因此，一个关键的区别是，当采用一组128字节而不是单独1个访存请求时，现在将通过访问缓存，而不是另一内存读取。

而且，还需要关注的是，运行过程中的内存事务的数量。每一个内存事务被送入一个队列中然后由内存子系统单独执行，这会引入一定的开销。**一个线程一次提交对4个浮点数或整型数的一个读操作比提交4个单独的读操作花费的代价更小**。基于上述思想，可以采用两种方法，来接近峰值带宽。
- 方法1:使用线程束完全加载处理器。
- 方法2:使用<font color='red'><b>向量类型</b></font>，比如通过float2/int2或float4/int4向量类型使用64/128位读操作。
	- 实际上，通过使用向量类型，可以**提交少量可以由硬件有效处理的较大事务**。
	- 通过每线程处理多个元素，还**引入了一定的指令级并行**。

---
cuda中的向量类型：

在 CUDA 中，向量类型允许开发者在处理 2、3、4 维向量时以更加简洁的方式进行操作。常用的 CUDA 向量类型有 `float2`、`float3`、`float4`、`int2`、`int3`、`int4` 等。这些类型不仅简化了对多维数据的操作，还能利用 CUDA 硬件的向量处理能力进行优化。
1. 向量类型的声明
	- CUDA 向量类型通过类似 `float3`、`int4` 等来表示维度和数据类型。例如：
```cpp
float3 vec3;
int4 vec4;
```
2. 向量的初始化
	- CUDA 为每种向量类型提供了构造函数，可以通过 `make_*` 函数进行初始化：
```cpp
float2 vec2 = make_float2(1.0f, 2.0f);
float3 vec3 = make_float3(1.0f, 2.0f, 3.0f);
int4 vec4 = make_int4(1, 2, 3, 4);
```
3. 访问向量元素
	- 可以使用点操作符访问向量的各个分量。对于二维、三维、四维向量，使用 `.x`, `.y`, `.z`, `.w` 来访问对应的元素：
```cpp
float x = vec3.x;
float y = vec3.y;
float z = vec3.z;
vec3.x = 4.0f;
```

4. 示例代码

```cpp
#include <stdio.h>
// CUDA kernel，简单地对每个向量的元素进行加法操作
__global__ void addVectors(float3* a, float3* b, float3* c, int N) {
    int idx = threadIdx.x + blockDim.x * blockIdx.x;
    if (idx < N) {
        c[idx].x = a[idx].x + b[idx].x;
        c[idx].y = a[idx].y + b[idx].y;
        c[idx].z = a[idx].z + b[idx].z;
    }
}
int main() {
    const int N = 10;
    float3 h_a[N], h_b[N], h_c[N];
    float3 *d_a, *d_b, *d_c;
    // 分配 GPU 内存
    cudaMalloc(&d_a, N * sizeof(float3));
    cudaMalloc(&d_b, N * sizeof(float3));
    cudaMalloc(&d_c, N * sizeof(float3));
    // 初始化向量数据
    for (int i = 0; i < N; i++) {
        h_a[i] = make_float3(i, i + 1, i + 2);
        h_b[i] = make_float3(i * 2, i * 2 + 1, i * 2 + 2);
    }
    // 拷贝数据到 GPU
    cudaMemcpy(d_a, h_a, N * sizeof(float3), cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, h_b, N * sizeof(float3), cudaMemcpyHostToDevice);
    // 执行 CUDA kernel
    addVectors<<<1, N>>>(d_a, d_b, d_c, N);
    // 拷贝结果回主机
    cudaMemcpy(h_c, d_c, N * sizeof(float3), cudaMemcpyDeviceToHost);
    // 输出结果
    for (int i = 0; i < N; i++) {
        printf("Result[%d]: (%f, %f, %f)\n", i, h_c[i].x, h_c[i].y, h_c[i].z);
    }
    // 释放 GPU 内存
    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);
    return 0;
}
```
5. 注意
	- 不同于结构体，CUDA 向量类型的内存布局是连续的，所以可以直接将这些向量类型与内核函数中的指针结合使用。
	- 尽管 CUDA 提供了 `float3`、`int3` 等三维向量类型，但实际在内存中，它们通常被存储为 4 元组，因此要小心处理内存对齐问题。

---

### 9.2.2 限制的来源

内核通常被两个关键因素限制：<font color='red'><b>内存延迟/带宽</b></font>和<font color='red'><b>执行延迟/带宽</b></font>。理解这两类关键限制因素中哪一个正在限制系统的性能，对于合理分配精力是很关键的。

一定要注意，要尽可能充分利用合并访存的特性，即为：线程0必须访问地址0，线程1必须访问地址1，线程2访问地址2，并以此类推。理想情况下，数据模式应该生成一个基于列的线程访问模式，而不是基于行的访问。 如果不能轻易地重新安排数据模式， 需要考虑能否重新安排线程模式（这样就能在访问数据前将线程载入到共享内存中)，如果可以，那么在从共享内存中访问数据时就不用再关心合并读操作了。

---
解释：

1. **合并内存读取（Coalesced Memory Access）**：在 CUDA 中，多个线程同时从全局内存读取数据时，通常需要优化访问模式，以确保不同线程的读取请求可以合并为少量的内存事务。如果访问模式不当，读取性能可能会受到严重影响。这种优化被称为“合并内存读取”，它要求多个线程在读取全局内存时按照一定的规则访问相邻地址。
2. **共享内存（Shared Memory）**：CUDA 设备上的共享内存是一种高速缓存存储器，位于每个线程块的局部范围内，且访问速度比全局内存快得多。共享内存可以由线程块中的所有线程共享，且不需要像全局内存那样严格遵守合并读写规则。
- <font color='red'><b>线程在共享内存中访问数据时，不用特别优化数据的排列和访问模式，也不会因为非合并读写导致性能严重下降</b></font>。

---

一些优化的Tips：
- 数组索引通常被替换为基于指针的代码，可以将速度较慢的乘法替换成更快的加法。
- 使用2幂次的除法和乘法指令分别可以被替换为速度更快的右移和左移位运算。
- 循环体中的所有常量（不变量）应该被移到循环体外部。如果线程包含一个循环，那么展开循环通常会实现加速。

注意，对于编译的选项也可以进行选择：比如使用-O3进行优化，使用-arch指定GPU架构型号等。

### 9.2.3 内存组织

在许多GPU应用程序中，**使用正确的内存模式**往往是关键的考虑因素。因此，需要尝试安排内存模式以使连续线程对内存的访问以列的方式进行。此原则同时适用于全局内存和共享内存。这意味着对于一个给定的线程束(32个线程)，线程0应该访问偏移量为0的地址，线程1访问偏移量为1 的地址，线程2访问偏移量为2的地址，以此类推。

注意：对齐是一个很重要的标准，它将决定内存事务或缓存行需要获取一次还是两次。**针对对齐问题的一个非常简单的解决方案是使用<font color='red'><b>填充值</b></font>**。只要保证该值对于计算结果没有影响即可。

### 9.2.4 内存访问以计算比率

所期望的理想比例是：对于每一个内核，从全局内存执行的读取操作需要执行10条或更多的指令。这些指令可能是数组索引计算、循环计算、分支或条件判断。

基于切换到其他线程束的能力，使用最少个数的常驻线程束无法隐藏内存或指令延迟。如果其他线程块没有其他的工作要做，那么SM将会闲置，等待内存事务的完成。

因此，指令流需要有足够的计算密度来充分利用SM上的CUDA核。这里提升计算密度的方式，可以利用上面提到的向量类型，通过每个线程处理两个、四个、八个元素，从而扩展这些内核使用其包括独立的指令流。因此，可以尽可能的使用<font color='red'><b>向量操作</b></font>。

### 9.2.5 循环融合和内核融合

#### 循环融合

循环融合是指两个明显独立的循环在一段范围内交错地执行。

例如，循环1从0执行到100，循环2从0 执行到200。至少在前100次迭代中，循环2的代码可以被融合到循环1的代码中。这样增加了指令级的并行，同样也减少了总体迭代次数的1/3。

#### 内核融合

内核融合是循环融合的演变。开发内核的时候，将操作分解成几个阶段或几轮是很常见的。所以，内核融合的思想就是，将原本分阶段执行的多个内核，融合成一个单独的内核进行执行，这样很大概率会提高内核的执行效率。它能实现效率提升的部分原因是，它所带来的<font color='red'><b>数据重用</b></font>。一旦有数据存储在共享内存或寄存器集中，那么就应该尽可能的重用它。

### 9.2.6 共享内存和高速缓存的作用

从十分分散的内存区域访问一个数据元素的内核，在任何基于缓存的架构，包括CPU或GPU，表现会十分糟糕。原因在于<font color='red'><b>单个元素的读取会载入128字节的数据</b></font>。 对于大多数程序来说，**存入缓存的数据会在下一次循环迭代中命中，这是由于程序常常访问与之前访问过的数据临近的数据**。因此对于大多数程序，这是一个显著的优点。

但是，**对那些只需要单个数据元素的程序来说，剩余的124字节是多余的**。对于这种内核，**你需要为内存子系统只读取其所需的内存事务而不是缓存行大小的**，只能在编译的时候通过`-Xptxasdlc=cg`标志来完成此工作。这就将所有访问减少到每次事务32字节并且令一级缓存失效。 对于只读数据，考虑使用纹理内存或者常量内存。

## 9.3 策略3:传输

### 9.3.1 锁页内存

前面在流机制中已经介绍过锁页内存以及它的使用场景，在此处不再赘述。（锁页内存允许GPU上的DMA（直接内存访问）控制器请求主机内存传输而不需CPU主机处理器的参与，在异步传输中必须使用锁页内存）

![image.png|center|400](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240908213449.png)

### 9.3.2 零复制内存

零复制内存是一种特殊的内存映射，它允许将主机内存直接映射到GPU内存空间上。也就是说， **零复制内存(Zero-Copy Memory)** 是指主机（Host）内存可以在 GPU 设备（Device）端直接访问，而不需要显式地将数据从主机内存拷贝到设备内存。这样就避免了传统上主机和设备之间的内存拷贝操作，从而减少数据传输的开销，尤其是在某些特定的场景下（如共享内存、频繁小数据传输等），可以提高性能。

#### 零复制内存的原理

在零复制机制下，CUDA 允许 GPU 直接访问由主机系统管理的内存区域。这个机制依赖于现代计算机的统一虚拟内存（Unified Virtual Memory, UVA）和操作系统的内存管理，使用零复制内存的前提是<font color='red'><b>主机和设备共享同一物理内存</b></font>（例如在统一内存模型下的系统中，或在支持 Pinned Memory 的系统中）。

通过零复制内存，主机分配的内存可以通过特定的方式标记，使 GPU 在需要数据时直接读取，而不需要先将内存从主机拷贝到设备，避免了数据冗余和延迟。

#### 使用零复制内存的步骤

1. **主机内存固定化（Pinned Memory）**：
   通过 `cudaHostAlloc()` 函数分配主机内存，并将其标记为可直接由 GPU 访问的内存。
   ```cpp
   float *hostPtr;
   cudaHostAlloc(&hostPtr, size * sizeof(float), cudaHostAllocMapped);
   ```
   其中 `cudaHostAllocMapped` 表示该内存是可映射的（mapped），即设备可以直接访问该内存。
2. **映射内存**：
   调用 `cudaHostGetDevicePointer()` 获取设备端的指针。该设备指针将直接映射到主机内存中，而不需要显式的拷贝。
   ```cpp
   float *devicePtr;
   cudaHostGetDevicePointer(&devicePtr, hostPtr, 0);
   ```
3. **内核访问零复制内存**：
   在 CUDA 内核中，可以使用从主机映射到设备的指针（`devicePtr`）直接访问主机内存。
   ```cpp
   __global__ void kernel(float* ptr) {
       int idx = threadIdx.x;
       ptr[idx] = ptr[idx] * 2.0f;
   }
   kernel<<<1, size>>>(devicePtr);
   ```
   
#### 优缺点
- **优点**：
	1. **避免内存拷贝**：减少了显式的内存拷贝操作，提高了效率，<font color='red'><b>尤其适合小数据频繁传输的场景</b></font>。
	2. **简化编程模型**：开发者不需要手动管理主机和设备之间的数据同步。
	3. **低延迟**：适合<font color='red'><b>频繁小数据交互</b></font>的应用场景，如实时数据处理。
- **缺点**：
	1. **性能不如显式内存拷贝**：虽然减少了拷贝开销，但直接访问主机内存的速度远不如设备本地的显存，特别是如果主机和设备不共享物理内存时（如离散的 GPU），会导致较大的性能损耗。
	2. **需要固定化内存**：零复制需要使用 Pinned Memory，这会减少主机操作系统可用的可分页内存，影响系统整体性能。

### 9.3.3 带宽限制

对于绝大多数的程序而言，最终的带宽限制来源于设备获取输入数据和写回输出数据的I/O速度。比如主机内存速度的限制。

### 9.3.4 GPU计时

GPU上对数据进行计时不是特别简单。由于使用GPU和CPU最好的方式是**异步地进行操作**（即GPU和CPU同时运行），因此<font color='red'><b>使用基于CPU的时钟并不是一个很好的方案</b></font>。当同时在GPU和CPU上进行顺序操作时，CPU时钟不是特别精确。由于这并不是我们实际想要的，所以这是一个糟糕的方案。

由于memcpy操作<font color='red'><b>隐式地进行同步</b></font>，**默认情况下，GPU在同步模式下操作**。程序开发人 员希望将数据复制到设备、运行内核、再将数据从设备取回，然后将CPU内存中的运行结 果存入磁盘或进一步处理。虽然这是一个容易理解的模式，但它同时也是一个缓慢的模式。 

而使用流可以异步进行操作，当没有向CUDA API定义流时，0号流用于默认任务队列。然而，0号流包含很多隐式的与主机同步的操作。实际上在使用0号流时，特定的API调用包含隐含的同步。

在 GPU 上使用 CUDA 的流（Stream）机制计算某个特定操作的执行时间，可以通过使用 CUDA 提供的事件（Event）来测量 GPU 上的操作耗时。

![image.png|center|400](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240908215743.png)
#### 流和事件的概念
- **流（Stream）**：CUDA 流是 CUDA 中一种异步执行的机制，多个 CUDA 操作可以在不同的流中并发执行。默认情况下，CUDA 内核和内存操作是在 `cudaStreamDefault` 流中顺序执行的，但你可以创建自定义的流进行异步操作。
- **事件（Event）**：CUDA 事件用于在 GPU 上记录时间戳，可以通过两个事件来测量它们之间的时间差，从而得到操作的执行时间。

#### 代码示例：
```cpp
#include <stdio.h>
#include <cuda_runtime.h>
// 内核函数
__global__ void simpleKernel(float* data, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        data[idx] *= 2.0f;  // 简单的乘法操作
    }
}
int main() {
    const int N = 1 << 20;
    const int size = N * sizeof(float);
    float *d_data;
    float *h_data = (float*)malloc(size);
    // 初始化主机内存
    for (int i = 0; i < N; i++) {
        h_data[i] = i * 1.0f;
    }
    // 分配设备内存
    cudaMalloc(&d_data, size);
    // 创建流和事件
    cudaStream_t stream;
    cudaEvent_t start, stop;
    cudaStreamCreate(&stream);
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    // 异步拷贝数据到 GPU，指定使用流
    cudaMemcpyAsync(d_data, h_data, size, cudaMemcpyHostToDevice, stream);
    // 记录开始时间
    cudaEventRecord(start, stream);
    // 执行 CUDA 内核，指定使用流
    int threads = 256;
    int blocks = (N + threads - 1) / threads;
    simpleKernel<<<blocks, threads, 0, stream>>>(d_data, N);
    // 记录结束时间
    cudaEventRecord(stop, stream);
    // 同步等待流中的操作完成
    cudaEventSynchronize(stop);
    // 计算内核执行时间
    float milliseconds = 0;
    cudaEventElapsedTime(&milliseconds, start, stop);
    printf("Kernel execution time: %f ms\n", milliseconds);
    // 异步拷贝结果回主机，指定使用流
    cudaMemcpyAsync(h_data, d_data, size, cudaMemcpyDeviceToHost, stream);
    // 等待流完成所有操作
    cudaStreamSynchronize(stream);
    // 清理
    cudaFree(d_data);
    free(h_data);
    cudaEventDestroy(start);
    cudaEventDestroy(stop);
    cudaStreamDestroy(stream);
    return 0;
}
```


### 9.3.5 重叠GPU传输

利用<font color='red'><b>计算时间重叠数据传输时间</b></font>。流在GPU计算中是一项非常有用的功能，**通过建立独立工作队列能够以<font color='red'><b>异步方式驱动GPU设备</b></font>**。也就是说，CPU可以将一系列的工作元素压入队列，然后离开，在再次服务GPU之前做别的事情。

通过为GPU创建一个工作流，取代了GPU和CPU同步工作的模式，取代了CPU不得不一直询问GPU来确认是否完成的模式，我们只是给它大量的工作去处理。只需定期去检查工作是否完成，如果完成，则可以将更多的工作压入流或工作队列中。

需要注意的是：**对于异步操作，从GPU传入/传出需要固定的或者是锁页的内存**。

## 9.4 线程使用、计算和分支

### 9.4.1 线程内存模式

**把应用程序分解成大小合适的网格、线程块和线程**，是保证CUDA内核性能的关键环节之一。包括GPU在内的几乎所有计算机设计，<font color='red'><b>内存都是瓶颈</b></font>。线程布局的一个糟糕的选择通常也会导致一个明显影响性能的内存模式。

### 9.4.2 非活动线程

非活动线程的问题有两个方面：
- 只要线程束中的一个线程是活跃的，那么对应的线程束就保持活跃，可供调度，并<font color='red'><b>占用硬件资源</b></font>。
	- 比如：在每个线程束内进行归约操作，依次从32降至16、8、4、2，直到最后1个活动线程。不论使用32个线程还是1个线程，硬件仍然会分配32个并且简单地屏蔽掉非活动线程。因为线程束仍然活跃，所以即使只有一个线程处于活动状态，它们仍然需要被调度并消耗硬件资源。
- 虽然SM内部关心的是线程束，而不是线程块，但是，在外部调度器在进行调度时，调度还是以线程块来进行调度的。如果每个线程块内的线程束活跃的线程数量少，则可能会引起<font color='red'><b>隐藏内存和指令延迟的能力降低</b></font>。
	- 比如：如果每个块包含只有一个活动的线程束，那么仅有6~8个线程束以供SM从中选择进行调度。通常根据计算能力的版本和 资源使用情况，在一个SM中容纳多达64个活跃的线程束。现在存在明显的一个问题，因为线程级的并行模型(TLP)依赖于大量的线程来隐藏内存和指令延迟。**随着活跃线程束数量的减少，SM通过TLP隐藏延迟的能力也明显下降**。一旦超过某个程度，就会伤害到性能尤其是当线程束仍在访问全局内存的时候。

**尽可能地终止最后的线程束以使整个线程块都闲置出来，并替换为另一个包含一组更活跃线程束的线程块**。

### 9.4.3 算数运算密度

**算术运算密度**（Arithmetic Intensity）通常是指每单位数据传输量所执行的算术运算数量。具体来说，它是**执行的浮点运算次数（FLOPs）与内存访问次数的比值**。这一指标用于评估程序的性能瓶颈是计算能力（算术运算）还是内存带宽（数据传输）。

- **算术运算密度高**：意味着每次内存访问后，CUDA核函数会执行较多的算术运算，这样程序更可能受限于计算能力，而不是数据传输。
- **算术运算密度低**：意味着内存访问频繁，但每次内存访问只执行少量的算术运算，这样的程序更容易受到内存带宽的限制。

提高算术运算密度，可以通过减少内存访问次数或增加算术运算量来优化CUDA程序，从而更好地利用GPU的计算能力。
1. 重用数据（Data Reuse）
2. 合并内存访问（Memory Coalescing）
3. 使用更高效的内存层次结构
4. 减少冗余计算
5. 调整线程块和网格配置
6. 减少分支和控制流依赖

#### 超越函数操作

超越函数（Transcendental Function）是指无法通过有限次的代数运算（加、减、乘、除、开方）表达的函数。这类函数超出了代数函数的范畴，因此被称为超越函数。常见的超越函数包括：
1.	指数函数（如  $e^x$ ）
2.	对数函数（如  $log(x)$ ）
3.	三角函数（如  $sin(x)$, $cos(x)$, $tan(x)$ ）
4.	反三角函数（如  $arcsin(x)$, $arccos(x)$, $arctan(x)$ ）
5.	双曲函数（如  $sinh(x)$, $cosh(x)$ ）

如果目标是更快，即使**精度低些也没关系**，则使用编译开关（`-use fast math`)或显式地使用内置操作。第一步仅需要在编译器中启用编译选项并检查现有应用程序的输出结果。这个问题的答案将会改变，至于会变化多少以及变化的重要程度，都是需要考虑的关键问题。

#### 近似

在一定的搜索空间求解问题时，近似是一种有用的技术。双精度数学运算是特别昂贵的，至少比单精度浮点运算慢两倍。

**在支持本地双精度的所有计算硬件版本上，使用单精度近似是双精度计算速度的至少<font color='red'><b>两倍</b></font>。**


#### 查找表

就是通过空间换时间的操作。

### 9.4.4 一些常见的编译器优化

- 编译选项
	- -O3
	- -arch: 指定计算能力
- 循环展开
	- 使用`#pragma unroll`进行循环展开的操作。NVCC支持这个操作，它会自动展开全部的常量次循环。一种用于**循环展开**（loop unrolling）的编译器指令。它告诉编译器将指定的循环体展开成多次迭代的副本，以减少循环控制的开销，并优化代码的执行效率。
		- `#pragma unroll 4`  指示编译器将后面紧跟的循环展开4次。也就是说，编译器会把循环的每次迭代变成4次连续的迭代操作，从而减少循环体的分支判断和循环控制开销。
- 循环不变式
	- 循环不变式分析查找在循环体内不变的表达式，并将其移动到循环体的外面。
- 循环剥离
	- 循环剥离是循环展开的增强技术，常用在循环次数不是循环展开大小的整数倍时。在这里，最后的数次循环分离出来，单独执行，然后展开循环的主体。
	- 当使用`pragma loop unroll N`指令时，编译器将展开循环，使得迭代次数不超过循环的边界，并**在循环末端自动插入循环剥离代码**。
- 窥孔优化
- 公共子表达式和折叠

### 9.4.5 分支

GPU执行代码以**线程块**或**线程束**为单位。一条指令只被解码一次并被分发到一个线程束调度器中。在那里**它将保存在一个队列中直到线程束调度器把它调度给一个由32个执行单元组成的集合**，这个集合将执行该指令。

尽量减少分支的使用，来提升性能，如果有分支的存在，每个线程束中的部分线程就会出现空闲的情况。

### 9.4.6 理解底层汇编代码

GPU将代码编译到一个叫做<font color='red'><b>PTX</b></font>(并行线程执行指令集架构，Parallel Thread eXecution Instruction Set Architecture)的虚拟汇编系统中。它是一种虚拟汇编语言。它既可以在编译时也可以在运行时**解释成真实的、能够在设备中执行的二进制代码**。 编译时的解释仅插入了一些真实的二进制码到应用程序中，**插入的二进制码依赖于你在命令行中选择的架构(<font color='red'><b>-arch开关</b></font>)**。

为了查看已生成的虚拟汇编代码，只需要在编译命令行中添加`-keep`标识即可。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240910180941.png)

查看和理解底层的汇编函数最简单的方法之一是在Parallel Nsight中通过“View Disassembly”(查看反汇编)选项来查看源代码和汇编码的混合体。（这个好像是过时了吧？？？？）

### 9.4.7 寄存器的使用

寄存器是GPU上最快的存储机制。它们是达到诸如设备峰值性能等指标的唯一途径。 然而，它们数量非常有限。 <font color='red'><b>要在SM上启动一个块，CUDA运行时将会观察块对寄存器和共享内存情况的使用。如果有足够的资源，该块将启动。如果没有，那么块将不启动。</b></font>

驻留在SM中的块的数量会有所不同，但通常在相当复杂的内核上可以启动多达6个块，在简单内核上则可达到8个块 (开普勒则多达16个块)。实际上，块数并不是主要的考虑因素。关键的因素是<font color='red'><b>整体的线程数相对于最大支持数量的百分比</b></font>。


### 9.4.8 小节

- 内核启动时，声明的线程数量只使用32的倍数
- 考虑如何避免线程束的分支。


## 9.5 策略5:算法

虽然一个好的算法在CPU领域为最佳，但是在GPU领域却未必。GPU具有自己独特的特性。为了获得最佳的性能，需要先了解GPU的硬件。因此，如果要考虑算法，需要先考虑如下的问题：
- 如何将问题分解成块或片，如何将这些块分解成线程。
- 线程如何访问数据以及产生什么样的内存模式。
- 如何分析数据的重用性以及如何实现数据重用。
- 算法总共要执行多少工作以及与串行化的实现方法有何不同。

### 9.5.1 排序算法

这里有需要注意的几点如下：
- 合并访问要求每个线程以连续的方式进行访问。
- 当数据明显多次重复利用且需要高度本地化时，缓存或共享内存是一个很好的选择。
- CUDA的设计模式允许线程块以任何顺序执行，**当启动一个内核之后，<font color='red'><b>块与块之间无法进行同步操作</b></font>**。

### 9.5.2 归约

<font color='red'><b>归约</b></font>是并行编程中一种常用的技巧。归约主要有以下几种形式：

#### 基于全局内存的原子加法

基于全局内存的原子加法（atomic addition）是归约操作的一种方式，适合在多线程环境中避免竞争条件。在这种方式下，**每个线程对一个全局变量进行原子操作，确保并发写入的正确性**。

下面的例子是使用全局内存的原子加法来实现数组求和的归约例子：
```cpp
__global__ void reduceAtomicAdd(int *input, int *result, int n) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;

    if (index < n) {
        atomicAdd(result, input[index]);
    }
}
```

这种方式虽然实现简单，但由于<font color='red'><b>每个线程都需要访问全局内存并执行原子操作</b></font>，性能可能受限于全局内存的访问速度和原子操作的开销。在数据量较小或需要保证准确性时，这种方法仍然是有效的。

#### 线程内归约

线程内归约指的是**在同一个线程中对数据进行归约运算，这通常在单线程处理一些本地数据时使用**。在CUDA中，这通常意味着每个线程处理一部分数据，并将该部分数据进行归约。
```cpp
__global__ void reduceInThread(int *input, int *output, int n) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index < n) {
        int sum = 0;
        // 线程内归约：该线程处理数据的某个范围内的归约操作
        for (int i = index; i < n; i += gridDim.x * blockDim.x) {
            sum += input[i];
        }
        // 将每个线程的结果保存到输出数组中
        output[index] = sum;
    }
}
```

当每个线程需要处理独立的数据块并对这些数据进行归约时，线程内归约是一种有效的方式。相较于原子加法，这种方法可以减少全局内存的访问和同步开销，提高性能。

#### 线程束内的归约

线程束内归约（warp-level reduction）是指利用一个CUDA线程束（warp）中的多个线程协作进行归约操作。一个线程束由32个线程组成，线程束内归约的目标是高效地在这32个线程中进行数据归约运算。线程束内归约可以使用CUDA的**warp shuffle**指令来实现线程之间的数据交换，而无需使用共享内存，从而减少同步开销（也就是`__shfl_down_sync`命令，该命令可以让一个线程获取其他线程的值，从而实现在不使用共享内存的情况下进行高效的归约操作）。

通过线程束内的线程归约，进行求和的操作：
```CPP
__inline__ __device__ int warpReduceSum(int val) {
    // 使用warp shuffle指令进行归约操作
    for (int offset = warpSize / 2; offset > 0; offset /= 2) {
        val += __shfl_down_sync(0xFFFFFFFF, val, offset);
    }
    return val;
}
__global__ void reduceWarpLevel(int *input, int *output, int n) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    int lane = threadIdx.x % warpSize; // 当前线程在warp中的位置
    // 每个线程获取一个输入值
    int val = (index < n) ? input[index] : 0;
    // 执行线程束内归约
    val = warpReduceSum(val);
    // 线程束内的第一个线程写入结果
    if (lane == 0) {
        output[blockIdx.x * (blockDim.x / warpSize) + threadIdx.x / warpSize] = val;
    }
}
```

#### 基于块数目的归约（使用共享内存进行归约）

基于块数目的归约（block-level reduction）通常是指在CUDA程序中，多个线程块协同工作对大规模数据进行归约操作。**每个线程块首先在自己的范围内完成归约（通常使用共享内存进行块内归约），然后各个线程块的结果再通过全局内存进行进一步归约，直到最终得到全局的归约结果**。

基于块数目的归约操作步骤：
1. **每个线程块内归约**：每个线程块对其负责的数据部分进行归约，通常使用<font color='red'><b>共享内存</b></font>来保存中间结果。
2. **块间归约**：每个块得到一个局部归约结果，将这个结果写到全局内存中。
3. **最终归约**：通常需要一个单独的内核或进一步的归约操作，来对各个块的局部结果做最终的全局归约。

示例代码：
```cpp
__global__ void reduceBlockLevel(int *input, int *output, int n) {
    extern __shared__ int sharedData[];  // 声明共享内存
    // 计算全局索引和线程索引
    int tid = threadIdx.x;
    int globalIdx = blockIdx.x * blockDim.x + tid;
    // 每个线程将对应的全局内存数据加载到共享内存中
    sharedData[tid] = (globalIdx < n) ? input[globalIdx] : 0;
    __syncthreads();      // 同步线程，确保所有线程都加载完毕
    // 进行块内归约：标准的二分归约法（blockDim 一个块的大小）
    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {
        if (tid < stride) {
            sharedData[tid] += sharedData[tid + stride];
        }
        __syncthreads();  // 每次迭代后同步
    }
    // 线程索引为0的线程将块的归约结果写入全局内存
    if (tid == 0) {
        output[blockIdx.x] = sharedData[0];
    }
}
__global__ void finalReduce(int *input, int *output, int n) {
    extern __shared__ int sharedData[];
    int tid = threadIdx.x;
    // 每个线程加载一个块的结果到共享内存
    sharedData[tid] = (tid < n) ? input[tid] : 0;
    __syncthreads();
    // 块内归约操作
    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {
        if (tid < stride) {
            sharedData[tid] += sharedData[tid + stride];
        }
        __syncthreads();
    }
    // 线程索引为0的线程将最终结果写入output
    if (tid == 0) {
        output[0] = sharedData[0];
    }
}
```
1. **第一步：块内归约 (reduceBlockLevel kernel)**
   - 每个线程块对其负责的输入数据进行归约。
   - `sharedData`是每个线程块中的共享内存，所有线程在这里进行局部归约。为了提高效率，归约过程使用二分法，每个线程将自身的数据与“对称”位置的线程数据相加。每次归约后减少一半参与线程，直到最后只剩下一个线程完成归约。
   - 每个线程块的结果存储在全局内存中的 `output[blockIdx.x]` 中。
2. **第二步：块间归约 (finalReduce kernel)**
   - 在 `reduceBlockLevel` kernel 运行完后，`output` 数组中包含每个块的局部归约结果。我们还需要对这些块的结果进行最终归约。
   - 这可以通过再次调用类似的归约kernel来完成。这里的`finalReduce` kernel就是对这些块的结果进行最后的归约。
   - 由于需要处理的数据（每个块的归约结果）比原始输入少得多，通常只需要一个线程块就可以完成剩余的归约。

## 9.6 策略6:资源竞争

### 9.6.1 识别瓶颈

一些进行性能分析的工具：

**Nsight System** 和 **Nsight compute** 的使用介绍

后面补上。


### 9.6.2 解析瓶颈

#### PCI-E 传输瓶颈

PCI-E传输瓶颈往往是需要考虑的一个关键因素。

改善传输瓶颈的思路：
- 压缩技术是一种可以明显增加PCI-E传输速率硬限制的技术。
- 使用流计算与数据传输进行重叠操作。

每个计算节点加载以及保存数据到本地存储设备或网络存储设备的速度也是一个限制因素。


#### 内存瓶颈

传输问题解决后，另一个需要考虑的问题是：**全局内存的内存带宽**。

考虑高效的内存存取以及数据重用是在选择一个合适算法时的基本标准。

当考虑内存时，也需要考虑线程间的合作问题，而且线程的合作最好限制在单一的线程块内。

为了实现更好的数据局部性，也可以通过将大数据集划分为若干个小块，通过重复多次传输来代替之前一整块的传输方式，来实现更好的数据局部性。尽管这样会需要大量的内存事务，<font color='red'><b>内存合并</b></font>还是实现高内存吞吐量的关键。‘


#### 计算瓶颈

##### 复杂性

控制逻辑复杂的算法及其不适合在GPU上进行实现。最好的解决方案就是为每种特殊情况单独写一个处理内核，或者让CPU来处理这些复杂的条件判断。（通过书写多个内核可以很好地消除控制流复杂性的问题）

由于每个单元的值需要通过周围单元的值计算得到，因此，会多次读取每个单元的值。 对于这类问题一种常见的解决方法是，使用多线程将数据分块读入到共享内存中。无论是在读取数据还是写回数据时，允许对全局内存合并访问，从而达到性能提升的效果。然而，**共享内存在线程块之间是不可见的**，即共享内存只能在同一线程块中的线程之间共享，并且<font color='red'><b>线程块与线程块之间也没有直接传输共享内存数据的机制</b></font>。 每次执行时，所有线程块中只有一部分线程块能够执行，因此，SM中的共享内存会在旧线程块撤出、新线程块调度之后重复利用。

如果要想利用从双精度转换为单精度来进行计算加速，这里需要注意：在C语言中，所有整数都默认为有符号整型数，而所有带小数点的数都将被视为双精度浮点数，**如果要想使用单精度浮点数，必须在数值后面有一个F后缀**。比如：3.14F

##### 同步和原子操作

当线程块执行同步时，大量可供调度的线程束变得不可供调度，直到除最后一个线程束往外的其他线程束到达同步点之后才能再次调度。

解决同步问题的方案就是不使用包含大量线程的线程块，需要做的只是尽可能地完全填充SM。（在计算能力3.X的硬件上每个线程块启动的最佳线程数为：256个）

实在需要同步时：
- 线程块中的线程
	- 如果在同一个warp内，可以通过相关的<font color='red'><b>硬件语言</b></font>进行同步。
	- 如果不在同一个warp内，则需要通过SM的<font color='red'><b>共享内存</b></font>进行同步。
- 在线程块之间的线程
	- 通过<font color='red'><b>全局内存</b></font>进行同步。

原子操作的执行方式与同步操作非常类似，**每个线程块中的线程每32个一组形成线程束，线程束中的所有线程排队依次执行相应的原子操作**，因此整个时间开销比较大。

与同步操作不同的是，当每个线程执行完原子操作之后即可全速执行之后的指令。这样有助于增加可供调度的线程束的数量，但无法改善总的线程块执行时间。<font color='red'><b>当线程块内所有线程没有完成原子操作之前，线程块不能从SM中撤出</b></font>。因此，**单个原子操作实际上是串行展开给 定线程块的每个线程束，其执行时间即线程块中每个线程束执行时间的总和。只有线程块内每个线程都完成原子操作，整个线程块才能结束**。

##### 控制流

尽量减少分支的使用。


## 9.7 策略7:自调优应用程序

对于不同的英伟达GPU，它都会编译到一个特定的计算能力上。因此，需要熟悉一个给定的计算能力可以提供什么，也就是需要了解在什么样的硬件上编写代码，这一直都是好的优化的基础。

### 9.7.1 识别硬件

前面已经提到通过编程，获取GPU的硬件信息了，现在需要强调关注的是：

- 成员变量major和minor一起使用时，可以提供该设备的计算能力。 
- 标志变量`integrated`，尤其是和canMapHostMemory标志一起使用时，允许使用零复制内存（策略3中讲述的）以及避免对于那些GPU内存，事实上已经在主机的设备间进行内存复制。 
- `totalGlobalMem`可以最大程度地利用GPU内存并保证不会试图在GPU上分配过多内存空间。
- `sharedMemPerBlock`可以让你知道每一个SM有多少共享内存可用。
- `multiProcessorBlock`代表设备中目前的SM数量。用该值乘以一个SM上可运行线程块的数量就是整个设备可容纳的线程块数。

这些信息给了我们一些定义问题空间的界限。那么就有两种选择：要么**分析离线的最佳解决方案**，要么尝试找出**运行时的解决方案**。通常，离线的方法会有更好的结果，可以极大地提高对所涉及问题的理解，并可能使你重新设计程序的某些方面。对于获得最佳性能，运行时的做法是必要的，即使是已经进行了重要的分析。

### 9.7.2 设备的利用

为打算支持的每个计算能力建立一个目标。根据内核运行在哪一个GPU上，自动**选择目标代码**。同时确保运行任何性能测试前，选择了**发布模式**作为生成的目标。



## 9.8 本章小结

影响性能的因素以及它们的相对重要性如下：<font color='red'><b>传输、内存/数据模式、SM利用率</b></font>





