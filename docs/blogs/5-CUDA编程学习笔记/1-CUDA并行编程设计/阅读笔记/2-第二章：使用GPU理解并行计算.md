## 2.1-简介
简单介绍并行计算。

## 2.2-传统的串行代码

OpenMP和MPI可以联合使用来实现节点内部的并行处理和集群中的并行处理。
- **OpenMP**的并行指令允许程序员通过定义并行区，从而从一个较高的层次分析算法中的并行性。
	- 通常，适合用一台计算机解决的问题，就采用OpenMP。
- **MPI**需要程序员做大量的工作来显式的定义节点间的通信模型。
	- 需要用到集群来解决的大问题就采用MPI。

通常情况下，CPU程序拥有的活动线程数量，不超过起包含的物理处理核数量的两倍。因为线程的切换也是一项很费时的操作，随着线程数量的增加，终端用户也能够感受到线程的存在。所以，CPU的应用程序的线程数要尽量比GPU的少。

## 2.3-串行和并行问题

线程操作的事一个共享的内存空间，这既可以带来不借助消息就可以完成数据交换的便利，也会引起缺乏对共享数据保护的问题。

一般来所，线程模型比较适合于OpenMP，而进程模型比较适合于MPI，而在GPU环境下，就相当于将两者进行了混合，CUDA使用一个线程块（block）构成网格（grid）。这可以看成是一个进程（就是上面提到的线程块）组成的队列（就是上面提到的网格），而进程间没有通信。每个线程块内部有很多线程，这些线程以批处理的方式进行运行，称为线程束（warp），如下图的2-1所示。

## 2.4-并发性

不易并行的算法：比如一个求解问题的算法，在计算每个点的值的时候，必须知道与其相邻的其他点的值，那么算法的加速比将很难提高。因为处理器需要花费更多的时间来进行通信从而实现数据共享，而不做任何有意义的计算。

易并行的算法：设计出一个公式，把每个输出数据都表示成相互无关的，比如矩阵乘法，就是容易进行并行的算法逻辑。

由于易并行不需要或只需要很少的线程间或线程块间的通信，所以CUDA是很理想的并行求解平台。
- 线程间的通信
	- 它用基于片上资源的、显式的通信原语来支持线程间的通信。
- 线程块间的通信
	- 通过按顺序调用多个内核程序才能实现，并且**内核间通信还需要用到片外的全局内存**。
	- 通过对全局内存的原子操作来进行实现，但会具有一定的限制。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240818225134.png)

注：对于绝大多数内核程序而言，分块的数量应该是GPU中物理SM数量的八倍或者更多倍。

目前，计算性能的提高已经从**受限于处理器的运算吞吐率**的阶段，发展到**迁移数据**成为首要限制因素的阶段。（数据的传输速率限制了计算性能的增长！）
在现代计算机的设计中，这个问题是通过使用多级Cache来进行缓解的。缓存的工作基础是时间局部性和空间局部性。而在GPU的编程中，这种局部性的利用是通过程序员来进行实现的。

## 2.5-并行处理的类型

### 基于任务的并行处理

这种并行处理向着“粗粒度并行处理”发展：引入许多计算能力很强的处理器，让每个处理器完成一个庞大的任务。

在GPU方面，粗粒度并行处理是由GPU卡和GPU内核程序来执行的。GPU有两种方法来支持流水线并行处理模式。
- 若干个内核程序被依次排列成一个执行流，然后不同的执行流并发地执行。
- 多个GPU协同工作，要么通过主机来传递数据，要么直接通过PCI-E总线，以消息的形式在GPU之间直接传递数据。（点对点机制（P2P））

这种并行处理，整体的执行时间取决于最慢的任务所需要的时间。

### 基于数据的并行处理

基于数据的并行处理的思路是：首先关注数据及其所需的变换，而不是待执行的任务。

## 2.6-弗林分类法

- SIMD - 单指令、多数据
- MIMD - 多指令、多数据
- SISD - 单指令、单数据
- MISD - 多指令、单数据

与SIMD稍有不同的是，在GPU中实现的是被英伟达成为“**单指令多线程**”的模型。程序员需要通过一个内核程序，来指定每个线程的工作内容，因此，**内核程序将统一读入数据，程序代码根据需要，执行A、B或C变换。实际上，A、B、C是通过重复指令流而顺序执行的。只不过每次执行时，屏蔽掉无须参与的线程**。
也就是说：SIMT 模型与 SIMD 模型类似，都涉及将相同的指令同时应用于多个数据元素。然而，<font color='red'><b>在 SIMT 模型中，程序员编写的内核程序（kernel）会被用来定义每个线程的具体操作，允许每个线程在执行时根据其索引或条件选择不同的操作</b></font>。具体来说，SIMT 模型通过统一的内核程序来控制所有线程的行为。尽管所有线程会从相同的程序代码执行路径开始，<font color='red'><b>但每个线程可以根据其数据或状态来选择执行不同的操作</b></font>。例如，在一个内核中，线程可能会执行操作 A、B 或 C，这些操作通过重复的指令流顺序执行。执行过程中，如果某些线程不需要参与某个操作（例如，只有满足特定条件的线程执行操作 A），这些线程会被“屏蔽”或忽略掉，只有相关的线程会参与到实际的计算中。这样，通过在指令流中动态选择和屏蔽线程，SIMT 模型能够实现高度的并行处理，同时允许一定的灵活性来处理不同线程的不同计算需求。

## 2.7-常用的并行模式

### 基于循环的模式

基于循环的迭代是实现并行化的模式中最容易的一个。如果循环间的依赖被消除掉了， 那么剩下的问题就是在可用的处理器上如何划分工作。**划分的原则是让处理器间通信量尽可能少，片内资源**(GPU上的寄存器和共享内存，CPU上的一级/二级/三级缓存)**的利用率尽可能高**。糟糕的是，通信开销通常会随着分块数目的增多而迅速增大，成为提高性能的瓶颈、系统设计的败笔。

对问题的宏观分解应该依据可用的逻辑处理单元的数量。
- 对与CPU，就是可用的逻辑硬件线程的数量。
- 对于GPU，就是流处理器簇（SM）的数量乘以每个SM的最大工作负载（依赖于资源利用率、最大工作负荷和GPU模型）。
	- SM的最大工作负载取值的范围1～16块。

如果分配给GPU执行的内循环是很小的，通常用一个**线程块内的线程**来处理。由于循环迭代是成组进行的，所以相邻的线程通常访问相邻的内存地址，这就有助于利用访存的局部性，这一点对CUDA程序设计十分重要。外循环的并行处理都是用**线程块**来实现的。
- 内部小循环，通过线程块内的线程来进行实现。
- 外部循环的并行处理，利用各个线程块来实现。

### 派生/汇集模式

通常，派生/汇聚模型是采用数据的静态划分来实现的，即：串行代码派生出N个线程并把数据集等分到这N个线程上。

![image.png|center|400](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240818233408.png)
对于CPU而言，程序员或者多数多线程库通常是按照处理器的个数来派生相同数目的逻辑处理器线程。由于CPU创建或删除一个线程的开销是很大的，而且线程过多也会降低处理器的利用率。

对于GPU则相反，我们的确需要成千上万个线程。我们还是使用在很多先进的CPU调度程序中使用过的线程池的概念，不过将“线程池”改为“线程块池”更好。**GPU上可以并发执行的“线程块”的数目存在一个上限**。<font color='red'><b>每个线程块内包含若干个线程，每个线程块内包含的线程的数目和并发执行的“线程块”的数目会随着不同系列的GPU而不同</b></font>。


### 分条/分块

在很多方面，GPU与集成在单个芯片上的对称多处理器系统非常类似。<font color='red'><b>每个流处理器簇 (SM)就是一个自主的处理器，能够同时运行多个线程块，每个线程块通常有256或者512 个线程</b></font>。若干个SM集成在一个GPU上，共享一个公共的全局内存空间。它们同时工作时， 一个GPU(GTX680)的峰值性能可达3T1ops。

考虑并发性时，还可以考虑采用<font color='red'><b>指令级并行（ILP）</b></font>，实现指令级并行的基础是**指令流可以在处理器内部以流水线的方式执行**。

### 分而治之

关于递归算法和迭代算法的选择：

在可能的情况下最好还是使用**迭代**的方法，这样可以获得更好的执行性能，并可以在更大范围的GPU硬件上运行。


## 2.8-本章小节

随着并行处理带来的复杂度的提高，需要程序员把<font color='red'><b>并发性</b></font>和<font color='red'><b>局部性</b></font>当作关键问题事先考 虑。在设计面向GPU的任何软件时，都应该时刻把这两个概念牢记于心。