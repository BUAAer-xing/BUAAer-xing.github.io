
在各种设备内存中，全局内存具有最低的访问速度（最高的访问延迟），往往是一个CUDA程序性能的瓶颈。

## 7.1-全局内存的合并与非合并访问

对**全局内存的访问**将触发<font color='red'><b>内存事务</b></font>（memory transaction），也就是<font color='red'><b>数据传输</b></font>（data transfer）。

从费米架构开始，有了SM层次的L1缓存和设备层次的L2缓存，可以用于缓存全局内存的访问。在启用了L1缓存的情况下，对全局内存的读取将首先尝试经过L1缓存；如果未命中，则接着尝试经过L2缓存；如果再次未命中，则直接从DRAM读取。

在DRAM中，一次数据传输处理的数据量默认情况下是<font color='red'><b>32字节</b></font>。

---

🔥🔥在 NVIDIA GPU 中，一次全局内存访问的内存事务传输的数据量通常为 **32 字节**或 **128 字节**，具体取决于 GPU 架构和缓存配置。
- **32 字节事务**：这是 <font color='red'><b>L2 缓存行</b></font>的大小。在访问未缓存或直接从全局内存读取的数据时，GPU 通常以 32 字节为单位进行内存事务。
- **128 字节事务**：这是 <font color='red'><b>L1 缓存行</b></font>的大小。如果 GPU 使用 L1 缓存并且访问模式适合缓存，那么内存事务的大小可能为 128 字节。

注意，**L1缓存行**大小与**L2缓存行**大小辨析：
- ⭐️ <font color='red'><b>L1缓存行</b></font>的大小通常较小，常见的大小为**128字节**。L1缓存主要用于每个Streaming Multiprocessor (SM) 上的局部缓存，存储线程块需要频繁访问的数据。
	- **Fermi架构**及之后的架构，L1缓存行的大小大多数是**128字节**。
	- 在Volta、Turing和Ampere架构中，L1缓存与共享内存（shared memory）在硬件上共享资源，缓存行大小通常仍保持为**128字节**。
- ⭐️ <font color='red'><b>L2缓存行</b></font>的大小较大，通常是**32字节**的倍数。在现代NVIDIA GPU架构中，L2缓存行的大小为**32字节**或**128字节**，而在一些较新的架构中，比如**Volta**、**Turing**、**Ampere**架构，L2缓存行的大小已经增加到**512字节**。
	- **Kepler、Maxwell、Pascal架构**：L2缓存行大小为**32字节**。
	- **Volta、Turing、Ampere架构**：L2缓存行的大小为**512字节**，以提高缓存带宽的利用效率和减少全局内存访问的延迟。

---

关于全局内存的访问模式，有合并与非合并之分。<font color='red'><b>合并访问指的是一个线程束对全局内存的一次访问请求（读或写）导致最少数量的数据传输</b></font>。否则称访问是非合并的。定量地说，可以定义一个**合并度(degree of coalescing)**:
$$
\text{合并度} = \frac{\text{线程束请求的字节数}}{\text{由该请求导致的所有数据传输处理的字节数}}
$$
如果**所有数据传输中处理的数据都是线程束所需要的**，那么合并度就是 100%，即对应合并访问。

所以，也可以将合并度理解为一种资源利用率。利用率越高，核函数中与全局内存访问有关的部分的性能就更好；利用率低则意味着对显存带宽的浪费。

举个例子，前提条件：以<font color='red'><b>全局内存读取</b></font>和<font color='red'><b>仅使用L2缓存</b></font>的情况
- 在此情况下，**一次数据传输**指的就是**将32字节的数据从全局内存（DRAM）通过32字节的L2缓存片段（cache sector）传输到SM**。
- 考虑一个线程束访问单精度浮点数类型的全局内存变量的情形。因为一个单精度浮点数占有4字节，故该线程束将请求128字节的数据。当合并度为100%时，这将仅触发128/32=4次用L2缓存的数据传输。
- <font color='red'><b>数据传输对数据地址</b></font>的要求：
	- 在一次数据传输中，从**全局内存转移到L2缓存的一片内存的首地址一定是一个最小粒度(这里是32字节)的整数倍**。
	- 例如，一次数据传输只能从全局内存读取地址为0~31 字节、32~63字节、64~95字节、96~127字节等片段的数据。如果线程束请求的全局内存数据的地址刚好为0~127字节或者128~255字节等，就能与4次数据传输所处理的数据完全吻合。这种情况下的访问就是合并访问。

❗️❗️：**使用CUDA运行时API函数（比如：cudaMalloc）分配的内存的首地址至少是<font color='red'><b>256字节的整数倍</b></font>。


常见的内存访问模式及其合并度：
1. 顺序的合并访问
```cpp
	void __global__ add(float *x, float *y, float *z)
	{
	    int n = threadIdx.x + blockIdx.x * blockDim.x;
	    z[n] = x[n] + y[n];
	}
	add<<<128, 32>>>(x, y, z);
```
其中，`x`, `y` 和 `z` 是由 `cudaMalloc()` 分配全局内存的指针。很容易看出，核函数中对这几个指针所指内存区域的访问都是合并的。例如，第一个线程块中的线程将访问数组 `x` 中第 0~31 个元素，对应 128 字节的连续内存，而且首地址一定是 256 字节的整数倍。这样的访问只需要 4 次数据传输即可完成，所以是合并访问，合并度为 100%。
2. 乱序的合并访问
```cpp
	void __global__ add_permuted(float *x, float *y, float *z)
	{
	    int tid_permuted = threadIdx.x ^ 0x1;
	    int n = tid_permuted + blockIdx.x * blockDim.x;
	    z[n] = x[n] + y[n];
	}
	add_permuted<<<128, 32>>>(x, y, z);
```
3. 不对齐的非合并访问
```cpp
	void __global__ add_offset(float *x, float *y, float *z)
	{
	    int n = threadIdx.x + blockIdx.x * blockDim.x + 1;
	    z[n] = x[n] + y[n];
	}
	add_offset<<<128, 32>>>(x, y, z);
```
由于每个都要偏移一个单位，因此，需要访问的第一个32B中有用的数据为28B，最后一个32B中有用的数据为4B。故这将触发5次数据传输，故这样的访问属于不对齐的非合并访问，合并度为4/5=0.8。

4. 跨越式的非合并访问
```cpp
	void __global__ add_stride(float *x, float *y, float *z)
	{
	    int n = blockIdx.x + threadIdx.x * gridDim.x;
	    z[n] = x[n] + y[n];
	}
	add_stride<<<128, 32>>>(x, y, z);
```
第一个线程块中的线程束将访问数组x中指标为0、128、256、384等的元素。因为这里的每一对数据都不在一个连续的32字节的内存片段，故该线程束的访问将触发32次数据传输。这样的访问属于跨越式的非合并访问，合并度为4/32×100%= 12.5%，

5. 广播式的非合并访问（⭐️）

```cpp
	void __global__ add_broadcast(float *x, float *y, float *z)
	{
	    int n = threadIdx.x + blockIdx.x * blockDim.x;
	    z[n] = x[0] + y[n];
	}
	add_broadcast<<<128, 32>>>(x, y, z);
```

第一个线程块中的**线程束将一致地访问数组x中的第0个元素**。这只需要一次数据传输（处理32字节的数据），但是，在传输的所有数据中，有用的数据只有4字节，因此，合并度为4/32=12.5%。这样的访问属于**广播式的非合并访问**。

这样的读访问，则适合采用<font color='red'><b>常量内存</b></font>。

```cpp
#include <cuda_runtime.h>

__constant__ float x_const;

__global__ void add_broadcast(float *y, float *z)
{
    int n = threadIdx.x + blockIdx.x * blockDim.x;
    z[n] = x_const + y[n];  // 从常量内存中读取x[0]的值
}

int main()
{
    float h_x = 2.0f; // 假设 x[0] = 2.0
    float *x, *y, *z;
    cudaMalloc(&y, 128 * 32 * sizeof(float));
    cudaMalloc(&z, 128 * 32 * sizeof(float));
    
    cudaMemcpyToSymbol(x_const, &h_x, sizeof(float));
    
    add_broadcast<<<128, 32>>>(y, z);
    
    cudaFree(y);
    cudaFree(z);
    return 0;
}
```

- **优化点在于常量内存的缓存机制和广播模式的适配**：将`x[0]`放入常量内存，CUDA会自动优化这种数据访问模式，避免重复访问全局内存，从而减少带宽和延迟。
- **性能提升来自减少全局内存访问和更高效的缓存机制**，尤其在每个线程访问相同数据时，常量内存能够通过缓存显著提高性能。

## 7.2-全局内存的注意事项

从帕斯卡结构开始，如果编译器能够判断一个全局内存变量在整个核函数的范围都<font color='red'><b>只可读</b></font>，则会自动用函数`__ldg()`读取全局内存，从而对数据的读取进行缓存，缓解非合并访问带来的影响。但是，对于全局内存的写入，则没有类似的函数可用。所以，在不能满足读取和写入都是合并的情况下，<font color='red'><b>一般来说应该尽量做到合并地写入</b></font>。

除了利用只读数据缓存加速非合并的访问外，有时还可以**利用共享内存将非合并的全局内存转换为合并的**。

