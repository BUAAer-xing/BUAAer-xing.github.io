
## 9.1-完全在GPU中归约

```cpp
__global__ void reduce_shared(double *d_x, double *d_y)
{
    const int tid = threadIdx.x;
    const int bid = blockIdx.x;
    const int n = bid * blockDim.x + tid;
    __shared__ double s_y[128];            // 这里需要根据线程块的大小去自定义
    s_y[tid] = (n < N) ? d_x[n] : 0.0;     // 将数据从全局内存拷贝到共享内存中（可能是合并访问的）
    __syncthreads();
    for (int offset = blockDim.x >> 1; offset > 0; offset >>= 1)
    {
        if (tid < offset)
        {
            s_y[tid] += s_y[tid + offset];
        }
        __syncthreads();
    }
    if (tid == 0)
    {
        d_y[bid] = s_y[0];                 // 计算完成后，将共享内存中的数据，及时拷贝到全局内存中
    }
}
```

如果想要一次行获得所有结果，即：GPU传递回去的就是最终结果，只需要修改最后一行代码为：

```cpp
	if (tid == 0)
	{
		atomicAdd(&d_y[0], s_y[0]);        // d_y[0]中将会存放最终归约后的结果
	}
```

这就是原子函数的作用，它虽然不能保证各个线程的执行具有特定的次序，但是能够保证每个线程的操作一气呵成，不被其他线程干扰。

Tips：atomicAdd()函数是具有返回值，返回的是被加之前的旧值。

## 9.2-原子函数

原子函数对它的第一个参数指向的数据进行一次"读-改-写"的原子操作，即一气呵成、不可分割的操作。**第一个参数可以指向全局内存，也可以指向共享内存**。对所有参与的线程来说，**该“读-改-写”的原子操作是一个线程一个线程轮流做的，但没有明确的次序**。另外，原子函数没有同步功能。

---

扩展：基于atomic原子操作，衍生出的两种原子操作：`atomicAdd_block` 和 `atomicAdd_system` 是 CUDA 12.2 中引入的两种新的原子操作，分别适用于<font color='red'><b>线程块内</b></font>和<font color='red'><b>整个系统范围</b></font>的同步。这两种原子操作都针对不同的同步范围，提供了在不同层级上执行加法操作的能力。以下是这两个函数的特点与应用：

1. `atomicAdd_block`
	- **作用范围**：`atomicAdd_block` 只在线程块内部进行同步。这意味着线程块内的多个线程可以对同一个共享变量进行原子加法操作，并确保操作的原子性，而不同线程块之间并不会通过该函数进行同步。
	- **性能优化**：由于 `atomicAdd_block` 的同步范围仅限于线程块内部，不需要进行全局内存的同步，开销相对较小，因此<font color='red'><b>性能高于全局范围的原子操作</b></font>。
	- **适用场景**：适用于在同一个线程块中需要多个线程对同一个变量进行累加或修改的情况，如在共享内存中进行线程块内的计数等。
2. `atomicAdd_system`
	- **作用范围**：`atomicAdd_system` 是系统范围的原子操作，它确保在整个GPU系统内，所有线程块都能够同步对某个变量进行原子加法操作。与 `atomicAdd` 类似，但`atomicAdd_system`会确保在更大范围内进行同步。
	- **全局同步**：它不仅保证同一个GPU内线程块的同步，还可以在支持统一内存访问（UM，Unified Memory Access）的系统中跨GPU甚至跨CPU进行同步。
	- **适用场景**：当多个线程块或多个GPU需要对一个全局变量进行原子性操作时使用。例如，在分布式系统中或者多个GPU协同工作的场景下，它可以确保全局的一致性。

---

原子函数整理：https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html?highlight=atomic#atomic-functions


## 9.3 例子：邻居列表的建立

略
