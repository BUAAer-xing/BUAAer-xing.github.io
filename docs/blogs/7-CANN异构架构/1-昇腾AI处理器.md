
## 1-昇腾AI架构介绍

### 1.1-全栈 AI 架构组成

昇腾 AI 系统基于华为自研 **昇腾系列处理器**（内含达芬奇架构），构建了从硬件到软件的全栈异构计算平台，主要包括：
- **AI 硬件系统**：处理器、模组、板卡、小站、服务器、集群等；
- **AI 基础软件体系**：CANN 异构计算架构、编译器、运行时、加速库、工具链；
- **AI 计算框架**：MindSpore 为主，兼容 TensorFlow、PyTorch、PaddlePaddle；
- **应用使能中间件**：MindX 支持多平台（如 ModelArts、HiAI）及模型部署。

### 1.2-昇腾 AI 硬件体系

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250425150612.png)

#### 产品线
硬件产品覆盖边缘、云端推理与训练，主要系列包括：
- **推理设备**：Atlas 200/300/500 系列、边缘服务器 Atlas 500 Pro、中心服务器 Atlas 800；
- **训练设备**：Atlas 800/900 系列服务器和训练卡，支持 PoD 组网；
- **模式支持**：PCle 通信模式下支持 RC（主机模式）和 EP（从设备模式）。
  
#### 算力性能
- **训练卡算力**：320 TFLOPS（FP16）；
- **集群规模**：最高可达 1024 PFLOPS；
- **推理卡性能**：单卡 88 TOPS（INT8）；
- **能效比**：模组达 2 TOPS/W，集群 PUE < 1.1，支持全液冷散热系统。


### 1.3-昇腾 CANN 计算架构

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250425150649.png)


CANN（Compute Architecture for Neural Networks）是昇腾 AI 的异构计算核心组件，功能包括：
- **设备支持**：支持从 SoC 到集群的多形态设备；
- **模块组成**：
    - 运行时（Runtime）
    - 编译器（TBE/TIK）
    - 加速库（算子库）
    - 调试/调优工具
- **统一编程接口**：Ascend C 封装了 AI 任务、内存管理、图引擎等能力，屏蔽底层硬件差异。


### 1.4-AI 软件栈与框架支持

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250425150704.png)

#### 框架支持
- **原生支持**：MindSpore（适用于端边云全场景，支持自适应建模）；
- **第三方兼容**：支持 TensorFlow、PyTorch、PaddlePaddle 等；
- **模型迁移**：提供模型转换工具，简化模型部署。

#### 应用使能平台
- **MindX DL / Edge**：封装底层资源抽象，简化部署；
- **SDK + ModelZoo**：提供行业知识库和优选模型，降低 AI 开发门槛；
- **无感部署**：对终端开发者屏蔽模型、资源调度等底层细节。

### 1.5-端边云协同
- **统一硬件架构**：达芬奇架构适配端（手机）、边（小站）、云（服务器）；
- **统一软件平台**：一次开发，多端部署；
- **数据协同机制**：边端数据可回传训练，云侧训练优化后再部署回端；
- **端上轻量训练能力**：支持模型的本地个性化微调。

### 1.6-系统架构设计理念
- **模块化部署**：支持端、边、云各层级独立部署；
- **层间协同**：硬件-软件-框架-应用各层解耦协同；
- **独立演进能力**：支持软硬件各层版本独立升级。

### 1.7-昇腾未来技术发展方向
- **异构算力融合**：多处理器内核、多种加速器协同；
- **SoC 高集成形态**：提升边缘和终端部署能力；
- **智能编译系统**：提升编译优化与系统调度智能化程度；
- **AI 开发智能化**：构建“AI 开发 AI”的自动化工具链；
- **生态扩展**：持续投入算法库、开发语言、SDK 等组件的生态构建。


## 2-昇腾AI处理器

### 2.1-昇腾 AI 处理器架构设计

<center> <font  size='5' color='red'> 注：支持的数据类型有只有FP16和int8类型 </font> </center>

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250425151656.png)

昇腾 AI 处理器基于华为自研的<font color='red'><b>达芬奇架构</b></font>，面向云、边、端多场景，提供全栈式 AI 加速解决方案。其设计重心在于提升能效比与算力密度，采用高性能的 3D Cube 矩阵计算单元，每个单元可实现 4096 次 FP16 乘加运算，支持标量、矢量与矩阵等多种计算模式，具备混合精度处理能力，适配训练与推理任务。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250425151809.png)
<center> <font face='华文宋体' size='5'> 在AI Core中主要处理的几类计算示意图 </font> </center>

### 2.2-芯片架构与系统组成

![image.png|center|400](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250425164431.png)

昇腾 AI 处理器构成一个典型的 <font color='red'><b>SoC 系统(system on chip)</b></font>，包含 AI Core、AI CPU、控制 CPU、多层级片上缓存、数字视觉预处理模块（DVPP）以及高速 IO 接口。各模块通过基于 CHI 协议的片上环形总线连接，实现高效、低延迟的数据交互。AI Core 是主算力单元，支持模块化拓展，具备高吞吐、低功耗的矩阵计算能力，并配备大容量 on-chip buffer 以优化神经网络中间态访问效率。专用任务调度器（TS）辅助管理 AI Core 的任务分发。DVPP 实现 JPEG/PNG 编解码、视频预处理等功能，通过硬件电路实现高效图像处理。

📒：**SoC（System on Chip）系统**指的是将计算系统的**多个关键功能模块**集成在**一颗芯片**上的一种集成电路设计方案。通俗来说，就是把传统需要多个芯片完成的功能（如 CPU、GPU、内存控制器、I/O 接口、专用加速单元等）都封装到**单一芯片**里，实现“一个芯片就是一台系统”的设计理念。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250425151936.png)

该处理器大致可以划为：芯片系统控制 CPU（Control CPU），AI 计算引擎（包括 AI Core 和 AI CPU），多层级的片上系统缓存（Cache）或缓冲区（Buffer），数字视觉预处理模块（Digital Vision Pre-Processing，DVPP）等。芯片可以采用 LPDDR4 高速主存控制器接口，价格较低。目前主流 SoC 芯片的主存一般由 DDR（Double Data Rate）或 HBM（High Bandwidth Memory）构成，用来存放大量的数据。HBM 相对于 DDR 存储带宽较高，是行业的发展方向。其它通用的外设接口模块包括 USB、磁盘、网卡、GPIO、I2C 和电源管理接口等。

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250425152916.png)

**当该处理器作为计算服务器的加速卡使用时，会通过 PCIe 总线接口和服务器其它单元实现数据互换**。以上所有这些模块通过基于 CHI 协议的片上环形总线相连，实现模块间的数据连接通路并保证数据的共享和一致性。

昇腾 AI 处理器集成了<font color='red'><b>多个 ARM 架构的 CPU 核心，每个核心都有独立的 L1 和 L2 缓存，所有核心共享一个片上 L3 缓存</b></font>。

集成的 CPU 核心按照功能可以划分为**专用于控制芯片整体运行的主控 CPU** 和**专用于承担非矩阵类复杂计算的 AI CPU**。两类任务占用的 CPU 核数可由软件根据系统实际运行情况动态分配。

除了 CPU 之外，该处理器真正的算力担当是采用了<font color='red'><b>达芬奇架构的 AI Core</b></font>。AI Core 通过特别设计的架构和电路实现了高通量、大算力和低功耗，特别适合处理深度学习中神经网络必须的常用计算如矩阵相乘等。目前该处理器能对整数或浮点数提供强大的乘加计算力。由于采用了模块化的设计，可以很方便的通过叠加模块的方法提高处理器的算力。

针对神经网络参数量大、中间值多的特点，该处理器还特意为 AI 计算引擎配备了一定容量的片上缓冲区（On-Chip Buffer），提供高带宽、低延迟、高效率的数据交换和访问。能够快速访问到所需的数据对于提高神经网络算法的整体性能至关重要，同时将大量需要复用的中间数据缓存在片上对于降低系统整体功耗意义重大。**为了能够实现<font color='red'><b>计算任务</b></font>在 AI Core 上的高效分配和调度，还特意配备了一个专用 CPU 作为任务调度器（Task Scheduler，TS）。该 CPU 专门服务于 AI Core 和 AI CPU，而不承担任何其他的事务和工作。**


### 1.3-典型处理器型号

#### **昇腾 910**

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250425153156.png)

昇腾 910 面向云端训练与推理，采用 chiplet 架构，由计算芯粒、IO 芯粒和四个 HBM 芯粒组成，总带宽达 1.2TB/s。每个计算核心支持 4096 次 FP16 FMA 操作，适配大 Batch Size 场景。集成 100G NIC，支持 ROCE V2 协议，具备万卡集群能力。DVPP 模块最高支持 128 路 1080P 解码，满足高吞吐推理需求。

#### **昇腾 310**

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250425153218.png)

昇腾 310 面向边缘推理，优化外设接口和低 batch size 下的矩阵计算效率，采用 m=4 的矩阵维度设计，提高小规模计算下的硬件利用率。适用于智能城市、机器人、零售等场景。


### 1.4-卷积加速机制

昇腾 AI 处理器在卷积计算中采用 **Img2Col + GEMM** 的标准重构策略，将输入和权重矩阵重组为矩阵形式后，通过矩阵乘法完成卷积计算。整个数据流设计严格遵循局部性优化原则：
1. 指令经由核外内存加载至指令缓存，进入标量指令处理队列进行调度。
2. 数据搬运指令驱动数据从 DDR/HBM 通过数据通路载入输入缓冲区，再经过存储转换单元进行补零与展开（Img2Col），得到矩阵形式输入。
3. 数据在片上通过多个通路（通路5、17、18等）送入矩阵计算单元，进行块状矩阵乘法。
4. 偏置项同步通过通路重组后送入累加器，与中间结果累加，形成最终输出特征矩阵。
5. 输出特征可经过向量计算单元进行激活与池化处理，结果存入输出缓冲区，供后续神经网络层使用。

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250425153246.png)

该设计结合高效的计算资源调度与片上缓存机制，极大降低了外部数据访问开销，并通过灵活数据路径设计支持神经网络计算模式的通用性与多样性。


## 3-AI core

### 3.1-达芬奇架构与 AI Core 总览

昇腾 AI 处理器采用特定域架构（DSA），核心为 AI Core，具备三类计算单元：**矩阵单元（Cube Unit）、向量单元（Vector Unit）、标量单元（Scalar Unit）**，对应张量、向量、标量三类操作。三类单元具有独立流水线，支持 INT8/INT4/FP16 等多种精度类型。

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250425155342.png)

围绕计算单元布置分布式缓冲区与寄存器，提供数据就近存取能力，减少访存延迟。核心特征是**硬件级数据格式转换（MTE）** 模块，可高效完成 Img2Col 等操作，提升卷积类任务的吞吐效率。


### 3.2-计算单元设计

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250425154948.png)

#### **矩阵计算单元（Cube Unit）**

矩阵计算单元和累加器<font color='red'><b>主要完成矩阵相关运算</b></font>。
- 一拍完成一个 FP16 的 16×16 与 16×16 矩阵乘（4096 次运算）；
- 如果是 INT8 输入，则一拍完成 16×32 与 32×16 矩阵乘（8192 次运算）。

![image.png|center|400](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250425155806.png)

支持矩阵**分块计算（Tiling）** 与**数据重排（大Z小z / 大Z小N）** 以优化访存局部性与并行度。内置累加器支持矩阵加偏置与多次中间累加，常用于卷积神经网络中的 C = A·B + C 模式。可实现大规模矩阵乘法，通过将输入矩阵划分成若干 16×16 子块并依次处理，结合硬件缓冲与累加器完成拼接。

#### **向量计算单元**

实现<font color='red'><b>单向量或双向量之间的计算</b></font>，功能覆盖各种基本的计算类型和许多定制的计算类型，主要包括 FP16 / FP32 / INT32 / INT8 等数据类型的计算。
- 一拍可以完成两个 128 长度 FP16 类型的向量相加/乘
- 或者64 个 FP32 / INT32 类型的向量相加/乘。

可作为矩阵计算结果的后处理单元，实现 **ReLU 激活、池化、格式变换等常规神经网络操作**。实现矩阵与输出缓冲区的数据交互及后续向量操作，兼具传输与计算双重功能。

#### **标量计算单元**

负责标量的计算，相当于一个微型 CPU，控制整个 AI Core 的运行，<font color='red'><b>完成整个程序的循环控制、分支判断，可以为 Cube/Vector 提供数据地址和相关参数的计算，以及基本的算术运算</b></font>。
- 负责控制程序流（如循环与分支）、计算数据地址与配置参数等控制逻辑。
- 配置通用寄存器（GPR）与专用寄存器（SPR），如 CoreID、VA（向量地址）等。
- 实现对 AI Core 内部流水线的时序调度与控制指令分发。

### 3.3-存储系统与数据通路

#### 存储系统

AI Core 内部采用分布式片上缓冲区设计，主要包括输入缓冲区、输出缓冲区以及各类寄存器系统，用于高效支撑大规模数据在计算过程中的临时存取需求。核心的数据转换操作由专用的存储转换单元（Memory Transfer Unit, MTE）完成，该单元支持补零、转置、Img2Col、解压缩等格式处理操作，显著优化了核内数据预处理的效率。所有外部存储系统中的数据（如 DDR 或 HBM）在进入 AI Core 前，需通过总线接口单元（Bus Interface Unit, BIU）完成调度，该模块负责 LOAD/STORE 指令的解析与执行，并实现协议转换与数据搬运。

输入缓冲区的设计允许对高复用率的数据进行缓存，从而减少对外部存储的重复访问，有效降低数据通路负载并提升整体能效比。数据通路在架构设计上采用“多进单出”的组织方式，允许多个外部数据流以灵活方式并行注入至矩阵计算单元、输入缓冲区或输出缓冲区等目标模块；而所有计算完成后的数据必须通过输出缓冲区这一统一出口进行集中调度与写回。输出缓冲区不仅是中间结果与最终输出的存储节点，还具备多通道数据并发写入能力，能够协调核内各类计算资源与外部存储之间的数据交互。

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250425160031.png)

在矩阵计算单元还包含有直接的供数寄存器，提供当前正在进行计算的大小为 16\*16 的左、右输入矩阵。在矩阵计算单元之后，累加器也含有结果寄存器，用于缓存当前计算的大小为 16\*16 的结果矩阵。在累加器配合下可以不断的累积前次矩阵计算的结果，这在卷积神经网络的计算过程中极为常见。

#### 数据通路

昇腾 AI Core 的数据通路设计体现为典型的“多进单出”结构，其核心目标是在满足深度学习中高吞吐、低延迟数据访问需求的同时，控制芯片面积与功耗开销。在输入路径上，数据可从核外存储系统（如 DDR 或 HBM）通过 LOAD 指令直接送入矩阵计算单元，或先进入输入缓冲区，再由软件调度分发至各类计算模块。这种输入路径的灵活性允许开发者根据数据重用程度选择直接加载或缓存缓冲的策略，以提升数据访问局部性。
![image.png|center|400](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250425160404.png)

在输出路径上，所有计算结果必须统一通过输出缓冲区进行汇聚与调度，这是 AI Core 中数据流出的唯一通道。矩阵计算单元的中间结果可暂存于输出缓冲区，必要时还可反向传回作为后续计算的输入；向量计算单元和标量计算单元与输出缓冲区之间也具备双向数据通路，用于执行激活、归一化等后处理操作。此外，输出缓冲区连接着系统总线，是唯一能够将核内计算结果写回外部内存的接口，从而保证输出一致性与数据完整性。在整个数据流转过程中，AI Core 内部不存在输入缓冲区直接连接输出缓冲区的路径，这一设计确保了所有数据都必须经由有效计算后才能输出，避免了无效数据传输。通过多个并行输入通道与单一可控输出通道的组合，昇腾 AI Core 实现了对大规模神经网络模型计算过程的高效支撑，特别是在多层卷积操作中体现出优秀的带宽利用率与数据调度能力。


### 3.4-控制单元与指令执行机制

 控制单元包括**系统控制模块、指令缓存、指令发射与多个运算队列**（矩阵/向量/存储转换队列）。支持指令预取与缓存机制，多条指令通过标量队列统一解析后，由发射模块分配至相应执行队列。
 
![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250425160447.png)

**事件同步模块**实现多流水线同步机制，管理指令间的依赖与调度时序，确保多单元协同执行。支持任务块级调度，系统控制模块接收外部调度器初始化信息，配置指令与任务参数，并处理中断和异常反馈。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250425160618.png)


## 4-数据布局

NHWC 的数据排布方式更适合多核 CPU 运算， NCHW 的数据排布方式更适合 GPU 并行运算。




