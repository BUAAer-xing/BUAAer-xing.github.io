# 02_高性能计算平台

<center> <font face='华文宋体' size='5' color='red'> <b>前言：CUDA、ROCm、DTK 傻傻分不清</b> </font> </center>
<center> <font face='华文宋体' size='5'> 整理出各个高性能计算平台的概况 </font> </center>

## 计算平台是什么？

计算平台通常指的是支持进行大规模计算任务的硬件和软件环境。这些平台能够提供必要的工具和接口，让开发者能够更高效地编写、优化和执行计算密集型应用程序。计算平台可以是基于特定类型的处理器的，比如CPU、GPU或DCU，也可以是针对特定计算任务优化的，如深度学习、科学计算等。

计算平台里面会包含很多东西，从硬件层面往上分别是：
- 硬件的**驱动程序**：比如显卡的驱动程序，就是管理GPU的硬件资源，提供GPU计算所需的基本功能。
- 特定硬件语言的**编译器**：比如HIP编译器，用于将相关硬件语言（或者是接口C语言）的代码编译为底层的GPU指令。
- **运行时库**：比如rocm运行时库，用于提供GPU计算所需的核心功能，比如GPU的内存管理、线程调度和并发控制等。
- **工具集**：比如调试器、性能分析工具、代码优化工具等，用于帮助开发人员调试和优化GPU计算应用程序。

比如，在人工智能领域的高性能计算分类平台图：

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240313215735.png)
<center> <font face='华文宋体' size='4'> 图 1 计算平台的大体位置 </font> </center>

简而言之，可以认为，这类屏蔽底层AI芯片细节，通过**构建一套支持端到端AI应用开发，覆盖编译、调试、性能分析**等<font color='red'><b>工具</b></font>，同时**提供各种基础库**（<font color='red'><b>函数库、数学库、深度学习库</b></font>），以及**编程模型和API接口**的<font color='red'><b>软件系统</b></font>称为<font color='green'><b>计算平台</b></font>。

## 英伟达 CUDA

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240313221455.png)
<center> <font face='华文宋体' size='4'> 图2 GPU 计算应用，CUDA 设计用于支持各种语言和应用程序编程接口。 </font> </center>

CUDA(Compute Unified Device Architecture)是Nvidia推出的一种通用并行计算架构，包括CUDA指令集架构（ISA）和GPU内部的并行计算引擎，将GPU的应用领域从图形计算拓展到通用计算.

程序员<font color='red'><b>基于CUDA可以使用更高级的编程语言来编写用GPU运行的程序</b></font>，而不需要去学习特定GPU的指令集系统，支持C、C++、Fortran、Python等多种编程语言。

在CUDA出现之前，程序员要开发基于Nvidia的GPU芯片的应用，需要熟悉芯片的操作逻辑和内部结构，再用汇编语言进行程序开发。CUDA发布之后，程序员只需要基于CUDA对外提供的API选择自己熟悉的语言开发程序，无需熟悉芯片的底层逻辑，大大降低了GPU程序员的开发门槛。

同时随着CUDA版本的不断更新，CUDA现在的开发生态已经非常丰富，<font color='red'><b>不仅提供基础的编译、调试、性能分析与监控工具，还提供基础数学库、线性代数、深度学习等函数库，进一步加速了人工智能应用的开发周期</b></font>。

目前CUDA已经成为人工智能领域底层计算平台的事实标准，各种主流的深度学习框架都默认适配CUDA计算平台，包括TensorFlow、PyTorch、Caffe、MxNet、PaddlePaddle等。

## AMD ROCm

ROCm（Radeon Open Compute Platform）是AMD主导的一个开源计算平台库，Radeon是AMD GPU产品的品牌名，除ROCm之外，还有一系列ROCx的简称，如ROCr（ROC Runtime），ROCk（ROC kernel driver），ROCt（ROC Thunk）等。

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240313223105.png)
<center> <font face='华文宋体' size='4'> 图 3 ROCm的主要组成部分 </font> </center>

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240313222801.png)
<center> <font face='华文宋体' size='4'> 图 4 ROCm的架构 </font> </center>

<font color='red'><b>ROCm之于AMD GPU，基本上相当于CUDA之于NVIDIA GPU</b></font>，ROCm第一个版本是在2016年发布，目前最新的ROCm已经到了6.0版本。虽然从发布的时间来看，ROCm比CUDA晚了将近10年，但ROCm生态发展很快，已经能够提供与CUDA类似的API、工具以及各种函数库。

![image.png|center|400](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240313223429.png)
<center> <font face='华文宋体' size='4'> 图 5 CUDA平台和ROCm平台的对比 </font> </center>

这里需要提一嘴，HIP编程模型，HIP是AMD开发的一种GPU编程模型，意在实现GPU代码的可移植性。HIP提供了一组C++类和函数，允许开发人员在AMD和NVIDIA GPU上编写可移植的代码。<font color='red'><b>HIP的编程模型与CUDA相似，但是它是一个开放的标准，不依赖于任何特定的GPU厂商</b></font>。

![image.png|center|400](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240313224011.png)
<center> <font face='华文宋体' size='4'> 图 6 关系图谱 </font> </center>

也就是说，和CUDA类似，HIP也是一种编程模型，对标CUDA编程模型。HIP 可以说是 CUDA API 的”山寨克隆“版。除了一些不常用的功能（e.g. managed memory）外，几乎全盘拷贝 CUDA API，是 CUDA 的一个子集。HIP is a C++ runtime API 。我们使用C++语言，可以调用HIP的API来进行编程。HIP可以运行在ROCm平台，也可以运行在CUDA平台。所以他可以运行在A卡，但是也可以运行在N卡上（N卡主要还是CUDA） 。HIP 的API和CUDA非常类似，大多数情况下他们代码稍加修改就可以直接转换。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240313224237.png)
<center> <font face='华文宋体' size='4'> 图 7 CUDA和ROCm的对比 </font> </center>

## 国产AI计算平台库 -- DCU（DTK）

目前主流的路线主要有两种：
- 第一种选择从芯片到计算平台库都全自研，比如华为基于自己的Ascend系列ASIC构建的CANN计算平台库以及寒武纪基于自家的MLU系列ASIC构建的Neuware；
- 第二种则是选择自研+开源的路线，比如海光信息<mark> <b>基于开源ROCm定制开发了DTK（DCU Toolkit）计算平台库适配自研的DCU系列GPU。</b></mark>（目前使用的曙光超级计算机，就是DCU加速卡）

### 海光的DCU 

海光的GPU叫做DCU，其借鉴AMD的ROCm而开发出来的计算平台为：<font color='red'><b>DTK（DCU ToolKit）</b></font>，DTK（DCU ToolKit）是海光的开放软件平台，封装了ROCm生态相关组件，同时基于DCU的硬件进行优化并提供完整的软件工具链，对标CUDA的软件栈，为开发者提供运行、编译、调试和性能分析等功能。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240313225024.png)
<center> <font face='华文宋体' size='4'> 图 8 海光的软件栈支持 </font> </center>

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240313224923.png)
<center> <font face='华文宋体' size='4'> 图 9 CUDA与DTK的对比 </font> </center>
同时提供多种深度优化的计算加速库，原生支持TensorFlow/Pytorch/PaddlePaddle等深度学习框架以及Open-mmlab/Vision/FastMoe等三方组件，提供FP32/FP16/INT8等多精度训练和推理支持，覆盖计算机视觉、智能语音、智能文本、推荐系统和强化学习等多个人工智能领域。

### 寒武纪 Neuware

Cambricon Neuware是寒武纪专门针对其云端和终端的智能处理器产品打造的软件开发平台，其中包括了多种深度学习/机器学习编程库，以及编程语言、编译器、程序调试/调优工具、驱动工具和视频编解码工具等，形成了完善的软件栈，为人工智能应用特别是深度学习推理应用的开发和部署提供了便利。

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240313225505.png)
<center> <font face='华文宋体' size='4'> 图 10 寒武纪软件栈 </font> </center>

Neuware软件架构从上往下分别为深度学习应用、主流的深度学习框架、Neuware加速库、Bang编程语言、运行时库CNRT、驱动和虚拟化软件层，此外还包括通用的软件工具。

主流的深度学习框架包括TensorFlow和PyTorch，同时开发者可以结合神经网络加速库和通信库便捷的构造各类深度神经网络模型以及其他机器学习领域的算法，而无须关心寒武纪智能处理器产品的内部硬件资源如何调度。

运行时库CNRT提供了一套针对寒武纪智能处理器产品的上层编程接口，用于与寒武纪智能处理器产品的硬件之间进行交互和调度硬件资源。

<font color='red'><b>Bang语言是专门针对寒武纪智能处理器产品设计的编程语言</b></font>，它支持最常用的 C99和C++11语言的语法特性，并提供了用于编写高性能程序的内置函数接口。

Neuware的通用软件工具包括<font color='red'><b>编译器、调试器、系统性能分析及监控相关软件</b></font>。

### 华为昇腾CANN

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240313225824.png)
<center> <font face='华文宋体' size='4'> 图 11 华为CANN </font> </center>


## 整合各种计算资源 - OrionX异构算力池化

为了<font color='red'><b>解决不同厂商卡的管理运维以及适配开发难题</b></font>，趋动科技的OrionX异构算力池化解决方案帮助客户构建统一的AI算力资源池，通过软件定义AI算力技术，颠覆了原有的 AI 应用直接调用物理算力的架构，增加软件层，将<font color='red'><b>AI应用与物理硬件解耦合</b></font>。OrionX架构实现了AI算力资源池化，让用户高效、智能、灵活地使用AI算力资源，达到了降本增效的目的。

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240313230614.png)
<center> <font face='华文宋体' size='4'> 图 12 OrionX异构算力池化示意图 </font> </center>

异构算力池化解决方案可在实现多厂商AI算力硬件统一管理、统一调度、统一使用的同时，结合软件定义算力技术实现：
- 异构算力池化
	- 既支持底层AI算力基础设施全栈国产化，同时也支持国内厂商算力和国外厂商算力的异构池化管理，从而实现国产化的平稳、逐步替代；
- 按需分配
	- 资源池内各类算力资源按需挂载，用完立即回收，资源高效流转；
- 资源切分
	- 各类算力硬件资源抽象化，上层应用可以算力1%、显存1MB为基本单位进行异构算力资源的申请和使用，异构算力资源使用更加精细；
- 资源聚合
	- 资源池内各类算力资源通过网络远程调用方式实现资源整合，形成算力资源池，一方面可突破单服务器硬件配置闲置，另一方面可减少资源池内硬件资源碎片；
- 远程调用
	- AI 应用可在资源池任意位置进行部署，无需关注底层物理硬件配置细节；
- 弹性伸缩
	- AI 应用可弹性使用资源池内算力资源，无需重启即可改变申请算力资源。

## 异构加速领域概况

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240313231133.png)
<center> <font face='华文宋体' size='4'> 图 13 异构加速领域概况图 </font> </center>
