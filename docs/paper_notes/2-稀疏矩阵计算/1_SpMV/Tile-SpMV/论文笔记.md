
[pdf](zotero://open-pdf/library/items/L2I5AAR4)

## Abstract

随着图形处理器(GPU)在现代超级计算机中的广泛应用，在 GPU 上加速稀疏矩阵向量乘(SpMV)是近几十年来的研究热点。开发了一些技术，如*增加广泛的矢量单元的利用率*，*减少负载不平衡*和*选择最佳格式*。然而，在现有的基于 GPU 的 SpMV 工作中，二维空间稀疏结构还没有得到很好的开发。

文章提出了一个有效的 tile 算法 TileSpMV，通过开发稀疏矩阵的<font color='red'><b>二维空间结构</b></font>来优化图形处理器上的 SpMV。

首先实现了七种warp-level级 SpMV 方法来计算存储在不同格式中的稀疏块，然后设计了一种选择方法来找到每个块的最佳格式和 SpMV 实现。我们还自适应地（条件判断）将非常稀疏的tile中的非零元素提取到一个单独的矩阵中，以最大限度地提高整体性能。

📒：<font color='red'><b>计算方法-找到不同格式的稀疏存储块、选择方法-为不同的块选择不同的稀疏结构、自适应-通过条件来选择，提高整体性能</b></font>

实验结果表明，该方法在完整的 SuiteSparse 矩阵集合中的大多数矩阵中都比最先进的 SpMV 方法如 Merge-SpMV、 CSR5和 BSR 快，分别提高了2.61 x、3.96 x 和426.59 x 的加速速度。

## Introduction

稀疏矩阵向量乘(SpMV)乘以稀疏矩阵 A 和稠密向量 x，得到稠密向量 y。它在稀疏迭代解法(如共轭梯度(CG)方法)和图形处理框架(如 GraphBLAS )中起着关键作用，并且可能是过去几十年中研究最多的二级稀疏 BLAS (稀疏 BLAS)的核心。

SpMV 操作一般都是不规则的，而且存储器带宽有限，因此很难优化。为了在现代处理器上实现高吞吐量的 SpMV，研究人员提出了许多技术，包括：
- 减少稀疏矩阵 
- 增加访问向量 x 的数据局部性 
- 利用现代建筑上的宽向量单元
- 改善大规模并行处理机处理器的负载平衡
- 通过机器学习选择最佳的格式和算法

📒：以上是最近的研究方向。

然而，尽管上述努力，应该注意的是，并行 SpMV 仍然面临许多挑战，以最好地使用现代并行处理器，尤其是 GPU。
存在的问题有：
1. 第一个问题是优化的<font color='red'><b>基本格式</b></font>，如压缩稀疏行(CSR)、 ELLpack (ELL)及其变体，通常会<font color='green'><b>导致内存带宽利用率不足</b></font>。其原因是 CSR 和 ELL 都没有考虑到稀疏矩阵的空间结构，因此<font color='green'><b>对 x 的重用</b></font>往往不能令人满意。
2. 第二个问题是**缺乏针对非常小的稀疏矩阵优化的 SpMV 实现**，这些矩阵可以完全存储在芯片上的暂存器中。
3. 第三，虽然利用机器学习的格式和算法选择技术在 SpMV 中已被证明是有效的，但它们只用于整个矩阵，并且**稀疏矩阵的微结构没有从这些技术中获得好处**。

📒：主要问题是对稀疏矩阵的基本存储格式进行优化，同时又针对自己的tile，编出的两个问题，一个是没有针对小稀疏矩阵的优化技术，另一个是，虽然机器学习的算法选择技术在Spmv中是有效的，但是目前该技术都应用在整个大矩阵上，没有考虑矩阵的微结构（这里的突破点是：可以考虑将系统的运行状态加上去）。

为了应对上述挑战，本文提出了一种称为 TileSpMV 的方法。<font color='red'><b>其目标包括开发稀疏矩阵的二维稀疏块结构，实现和选择各种稀疏块的最佳格式和 SpMV 算法</b></font>。

具体的实现方法为：
- 首先，TileSpMV 将<font color='red'><b>稀疏矩阵存储到相同大小的常规稀疏块</b></font>中(大小总是16乘16) ，以获得更好的缓存位置和更高的带宽利用率。
- 其次，TileSpMV 内核现在<font color='red'><b>将tile视为基本的工作单元</b></font>，而不是现有方法中的行或一组非零元素，
	- 使用七种典型的格式(即 CSR、 COO、 ELL、 HYB、密集、密集行和密集列)优化 SpMV 的实现，这些格式涉及 CUDA 平台的warp 级别。
- 第三，设计了一种自适应选择（条件选择）方法，<font color='red'><b>为每个稀疏块寻找最佳格式和 SpMV 实现</b></font>。
	- 因此，微结构可以从分块格式和算法选择中受益。

📒：分为大小相等的块，根据每个块不同的情况，在块内选择不同的存储结构。在计算时，根据每个计算块的实际情况，来寻找最好SpMV计算方式。

在实验中，将 TileSpMV 内核与三种最先进的 SpMV 方法进行了比较: cuSPARSE v11.1中的块压缩行(BSR)-SpMV (使用大小为4x4的密集块) ，cuSPARSE v11.1中的 CSR-SpMV (即改进的 Merge-SpMV 的实现)和 CSR5-SpMV 。测试数据集包括 SuiteSparse Matrix Collection 中的所有**2757个矩阵** ，实验平台包含最新的 NVIDIA A100(Ampere) GPU 和 NVIDIA Titan RTX (Turing) GPU。实验结果表明，该方法在1813个矩阵上比 Merge-SpMV 快，在2040个矩阵上比 CSR5快，在1638个矩阵上比 BSR 快，分别达到2.61 x，3.96 x 和426.59 x 的加速比。

这项工作做出了以下贡献:
- 提出了一种高效的平铺算法叫做 TileSpMV，用于现代 GPU 上的并行 SpMV。
- 实现了高度优化的warp级 SpMV 内核，用于表示为稀疏方块的小稀疏矩阵。
- 开发了一种自适应选择方法，为每个稀疏tile找到最佳的存储格式和内核。
- 在最新的图形处理器上比最先进的 SpMV 方法有明显的加速。

## Background and Motivation

### 并行稀疏矩阵向量乘法

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240315150333.png)
<center> <font face='华文宋体' size='4'> 图 1 SpMV示意图 </font> </center>

稀疏矩阵-向量相乘(SpMV)运算将一个稀疏矩阵 $A$ 与一个稠密向量 $x$ 相乘，得到一个稠密向量 $y$。在这个过程中，$y_{i}$ 是由 $a_{i*}$ 的点积，即 A 的第 $i$ 行和向量 $x$ 计算出来的。很容易发现，在整个执行过程中，行之间没有依赖关系。因此，SpMV 可以并行执行。

### 研究动机

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240315150601.png)
<center> <font face='华文宋体' size='4'> 图 2 稀疏矩阵结构 </font> </center>

已经有一系列的研究在稀疏矩阵中利用由计算科学与工程问题(如有限元建模)产生的小的致密结构。图2显示了三个矩阵，其中包括明显的小密集块结构。为了在 CPU 上使用加速 SpMV 的结构，Im 等人 开发了可以为小密集块结构提供寄存器级别优化的 SPARSITY 框架，而 Vuduc 等人开发了 OSKI 软件包 ，其中包含了许多用于寄存器阻塞和内存阶层优化的自动调优方法。

然而，这种优化只在 CPU 平台上显示了它们的有效性。在 GPU 方面，尽管最近为 GPU 开发的 SpMV 算法在一定程度上解决了广泛的矢量化和负载均衡问题，但是<font color='red'><b>在 GPU 上，利用小块结构的优点被大大忽视了</b></font>。

实际上，对 GPU 上并行 SpMV 的小块结构进行优化并非易事，原因有几个。
- 第一种是在 GPU 上使用 宽SIMD编程模型，例如 CUDA 中32个线程的wrap，<font color='blue'><b>这些块应该足够大以饱和 GPU 的宽 SIMD 单元</b></font>，也就是说，不应该像用于 CPU 寄存器的块那么小。
- 第二个原因是，<font color='blue'><b>当块较大时，不应该以密集的形式保存它们</b></font>，这样可能会浪费太多空间来填充零，并可能抵消性能/空间方面的好处。
- 第三个原因是，没有一个单一的稀疏格式和算法总是可以提供任何稀疏结构的块的最佳性能，<font color='blue'><b>总是需要一种选择方法</b></font>。因此，如何针对相对较大和较稀疏的块设计高效的 GPU 内核，并为其选择最佳的格式和算法，对于在现代 GPU 体系结构中利用块结构优化 SpMV 具有特别重要的意义。

这促使为 GPU 设计一个高效的平铺 SpMV 算法。

## TileSpMV

### 概述

TileSpMV算法首先将整个输入稀疏矩阵划分为若干相同且足够大的稀疏块（在本文中始终为16x16），以获得更好的数据局部性并饱和GPU SIMD单元。我们还为每个稀疏块提供了七种格式选项（即CSR、COO、ELL、HYB、密集型、密集行和密集列）。第三节B将介绍TileSpMV的存储结构。然后，为了更好地计算用于SpMV的块的稀疏结构，我们针对不同结构开发了七种相应的warp级别SpMV算法。由于稀疏块通常比完整稀疏矩阵小得多，因此需要仔细设计这些算法。第三节C将介绍这七种算法。为了使算法更加高效，我们还设计了一个两级选择方法，自动找到最适合每个稀疏块的稀疏格式和算法，并决定是否值得将非常稀疏的块提取到单独的稀疏矩阵中。第三节D将介绍选择方法。

### 两级存储结构

TileSpMV 首先将输入矩阵 A 划分为大小相同(在本文中为16 × 16)的稀疏块，并<font color='red'><b>使用稀疏块作为基本工作单元</b></font>。分区之后，将生成两个级别的信息，这些信息表示为一组数组，用于存储稀疏的tile。信息的两个级别分别<font color='red'><b>存储矩阵的tile结构</b></font>和<font color='red'><b>每个稀疏瓦片的内部信息</b></font>。图3显示了一个大小为16 × 16的示例矩阵。在这种情况下，为了更加清楚的显示，将矩阵分成10个大小为4乘4的稀疏块，以解释所提出的存储结构。

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240315153614.png)
<center> <font face='华文宋体' size='4'> 图 3 TileSpMV的两级存储结构 </font> </center>

#### 第一层：矩阵的tile结构

第一层是矩阵的tile结构，包含三个数组: 
- (1) 大小为 $tilemA + 1$的 `tilePtr` 数组，其中 $tilemA$ 是矩阵 $A$ 的tile行数，相当于CSR存储格式中的`row_ptr`数组
- (2) 大小为 $numtileA$ 的 `tileColIdx` 数组，其中 $numtileA$ 是矩阵中稀疏tile的数量，用于存储每个具有非零元素tile的列索引，相当于CSR存储格式中的`col_idx`数组; 
- (3) 大小为 $numtileA + 1$的 `tileNnz` 数组，其中存储稀疏tile中非零元数量的偏移量。就是通过该数组，可以计算得到某个非零元tile中的非零元素的个数（为什么不能直接存储每个tile中的非零元个数呢🤔❓）

还有一个format，用于表示每个tile中，所使用的存储格式。（ps：貌似图中的format数组写错了）

注意💡：上图中所涉及到的`某存储格式Ptr`数组，表示的含义各有不同，但大致的思想是，通过该数组，来寻找某个存储格式中的特定tile的存储的指定信息。

#### 第二层：每个tile的内部结构

第二级将非零元素及其索引以不同的格式存储在每个稀疏平铺中。在这项工作中，有七种选择: CSR、 COO、 ELL、 HYB、稠密(Dns)、稠密行(DnsRow)和稠密列(DnsCol)。

##### CSR存储格式

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240315160849.png)
<center> <font face='华文宋体' size='4'> CSR存储格式 </font> </center>
对于 CSR 格式，创建了三个数组来保存tile数据: 
- (1)大小为 $tilennz$ 的 `csrVal` 数组，它按tile的顺序存储所有非零条目的值; 
- (2)大小为 $tilennz$ 的 `csrColIdx` 数组，它存储每个非零条目的列索引。注意，由于稀疏tile的大小(即16乘16) ，瓦片中的列索引只需要4位，并且两个连续条目的列索引被打包成一个8位的无符号字符，以进一步减少所需的空间。
- (3)大小 $16$ 的 `csrRowPtr` 数组存储 tile 中非零的16个内存偏移量。(这里的大小指的是实际实现中的大小)
	- 虽然在经典的 CSR 中，普通的行指针应该包含16 + 1个条目，但是在这里只保存16个条目，以便利用无符号 char 数据类型，因为第二个最后一行指针的值不会超过240。这意味着无符号字符数据类型足以保存行指针中的所有偏移量，除了最后一个可能为256的值。可以从上面提到的1级存储结构的 `tileNnz` 数组中获得非零数目的总数。

##### COO存储格式

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240315161025.png)
<center> <font face='华文宋体' size='4'> COO存储格式 </font> </center>
对于 COO 格式，设置了三个数组 `cooVal`、`cooRowIdx` 和 `cooColIdx` 来分别记录非零条目的值、行索引和列索引。由于瓦片大小是16乘16，可以发现，对于每个行/列索引来说，4位就足够了。因此，我们**将4位行索引和4位列索引打包到一个8位无符号字符**中。

##### ELL存储格式

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240315161345.png)
![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240315163026.png)

<center> <font face='华文宋体' size='4'> ELL格式 </font> </center>

---

<font color='gray' face="宋体-简"><b>用两个和原始矩阵相同行数的矩阵来存：第一个矩阵存的是列号，第二个矩阵存的是数值，行号就不存了，用自身所在的行来表示；这两个矩阵每一行都是从头开始放，如果没有元素了就用个标志比如\*结束。上图中间矩阵有误，第三行应该是 0 2 3。</b></font>

在这个论文中，作者应该使用的是将存储矩阵进行了展开，变成了一维矩阵。

---
对于 ELL 格式，创建两个数组 `ellVal` 和 `ellColIdx` 来存储非零条目的值和列索引。同时将每一行的非零条目的数目设置为相等的数目，称为此 tile 的 tile 宽度。<font color='red'><b>tile 宽度的值记录每行中非零项的最大数目。对于小于 tile 宽度的行，我们用零填充空位置。</b></font>（问：如果用0进行填充，那ellldx怎么办呢？🤔❓）

为了在 ELL 格式中找到每个 tile 的相应 tile 宽度，所以**需要一个额外的数组来存储每个 ELL tile的 tile 宽度信息**（ellPtr实际上还是row_ptr的变体，用于指示某个使用ell存储格式存储的tile中，每行的非零元素的最大数量）。 上方是ELL一个示例，其tile宽度值为1，因为每行中非零条目的数量为1。


##### HYB 存储格式

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240315163552.png)
![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240315164008.png)

<center> <font face='华文宋体' size='4'> HYB存储格式 </font> </center>

--- 

<font color='gray' face="宋体-简"><b>如果某一行很多元素，那么ELL后面两个矩阵就会很胖，为了解决ELL中提到的，如果某一行特别多，造成其他行的浪费，那么把这些多出来的元素（比如第三行的9，其他每一行最大都是2个元素）用COO单独存储。</b></font>

---

对于 HYB 格式，创建了 ELL 和 COO 格式的组合。使用 ELL 格式存储 tile 的常规部分，然后其余的非零条目以 COO 格式存储。

为了确定 ELL 部分的 tile 宽度，**通过将 ELL 部分的宽度从最大值设置为零来逐步计算最小的内存空间，直到找到最小的内存空间为止**。然后得到的宽度是 ELL 部分的 tile 宽度。上图显示了 HYB 格式的一个示例。可以看到，其中的四个非零字节保存在第一列中，并将存储在 ELL 部分中，另外两个保存在 COO 部分中。这里 ELL 的 tile 宽度是1。右侧的数组以 HYB 格式显示六个非零条目数据。


##### Dns 存储格式

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240315164232.png)
<center> <font face='华文宋体' size='4'> Dns存储格式 </font> </center>

对于 Dns 格式，在其中的图3的示例矩阵中存储非常密集的tile。只需要一个数组 dnsVal 就可以按**列-主要顺序**存储所有非零项的值。
（问：为什么要按照列主序来存储非零元素呢？🤔❓）

##### DnsRow和DnsCol格式

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240315164928.png)
<center> <font face='华文宋体' size='4'> DnsRow和DnsCol格式 </font> </center>

对于 DnsRow/DnsCol 格式，本文创建了三个数组: 
- (1) `dnsRowVal/dnsColVal` 数组，以自然顺序存储所有非零条目的值。该数组的大小为 $dnsrownzA/dnscolnzA$
	- 其中 $dnsrownzA/dnscolnzA$ 是以 DnsRow/DnsCol 格式存储的所有 tile 块中非零项的总和。
- (2)大小为 $numrow/numcoll$ 的 `rowidx/colidx` 数组记录存储以 DnsRow/DnsCol 格式存储的tile块中具有非零元行或列的位置信息。
	- 其中，numrow/numcol 表示的是存储的总行数或者总列数。（或者是该tile中的非零元占有的总行数和总列数）
- (3) 大小为$numtileA + 1$ 的 `dnsRowPtr/dnsColPtr` 记录 以DnsRow格式或者DnsCol格式存储稀疏tile的密集行/列数目的偏移量，类似row_ptr。（比如，第一个DnsRow矩阵，有一行。DnsCol矩阵，具有一列）


### Tile-Wise SpMV 算法

#### Warp-Level CSR-SpMV算法

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240315222145.png)
<center> <font face='华文宋体' size='4'> Warp-Level CSR-SpMV算法 </font> </center>

在warp级CSR-SpMV算法中，**32个线程的warp始终处理一个具有16行的tile**（也就是说，<font color='red'><b>一个warp处理一个tile的数据</b></font>），这意味着每两个连续的线程处理一行。在计算之前，将向量x中的16个条目对应段加载到芯片上共享内存中以获得更好和可控的数据局部性。在线程计算后，部分y被相加在一起。如上图的示例，假设有8个线程（t0-t7）来处理CSR格式中的4x4 tile，并且每两个连续线程处理一行。值得注意的是第三行只有一个元素，因此t4可以单独计算它，而t5不执行任何操作（应该是t5单独计算，投t4不执行任何操作）。相反，第四行有三个元素，所以t6需要处理两个元素（应该是t7需要处理两个元素，分别是第3行的第1列和第3列的元素（计数从0开始计数））。然后我们使用sum来存储每个线程的计算结果，并且shuffle将被用于两次添加相邻线程的结果，并传输以适应sum（应该是sumsum）。

📒：这一段作者在进行写作以及画图时，应该是犯了一些错误，通过查看作者开源的代码中，可以看出，确实出错了。
解释：每两个连续的线程处理一行，按照开源的代码中，意思应该是奇数的线程负责处理该行中列号为奇数的非零元，偶数的线程负责处理该行中列号为偶数的非零元。所以，按照该说法，上面黄色的示意图中，在2行元素只要有一个时，因为非零元对应的列号为1，因此，应该是t4闲置，t5进行处理。在3行非零元素有三个时，非零元的序号分别为0，1，3。所以，t6只需要处理一个，而t7需要处理1，3两个非零元素。
还有，在代码的第10行，进行元素的归并时，原来的伪代码肯定是不对的，应该是源代码中提到的全新变量`sumsum`，只有这样才可以实现论文中所阐述的功能。

#### Warp-level COO-SpMV 算法

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240315223954.png)
<center> <font face='华文宋体' size='4'> Warp-level COO-SpMV 算法 </font> </center>
在warp级别的COO-SpMV算法中，计算非常稀疏的tile。一个warp中的32个线程被分配来处理所有的非零元素，并通过使用`atomicAdd`操作将结果部分和相加到共享内存中。图中的COO部分展示了一个例子。在这种情况下，只有两个元素位于COOtile中，并且两个线程（t0和t1）同时处理它们。算法3展示了其伪代码。

#### Warp-level ELL-SpMV算法

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240315232438.png)

<center> <font face='华文宋体' size='4'> Warp-level ELL-SpMV算法 </font> </center>

在warp级别的ELL-SpMV算法中，使用32个线程的warp来处理以**列为主**要存储方式的非零条目。每个线程负责的非零元序号为：$t_i + 32 * j$，当达到ELL宽度时计算完成。上图展示了一个例子。由于ELL数据以列为主要存储方式，四个元素被连续存储，并且内存访问将会对齐。对于这四个元素，我们分别指派四个线程（t0-t3）来处理它们。为了更快地访问向量x中的数据，x相应部分被加载到寄存器中，并通过寄存器`shuffle`指令进行访问。计算后，每个线程的结果都被存储到相应总和中作为该块tile最终结果。

#### Warp-level HYB-SpMV 算法

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240315233416.png)
<center> <font face='华文宋体' size='4'> Warp-level HYB-SpMV算法 </font> </center>

在warp级别的HYB-SpMV算法中，使用了分别计算ELL和COO部分的两个步骤。上图具体解释了这两个步骤。HYB tile 由ELL部分和COO部分组成，因此在第一步中，四个线程（t0-t3）处理存储在ELL数据中的四个元素，在第二步中，两个线程（t0和t1）处理存储在COO数据中的两个元素。与ELL-SpMV类似，向量x和计算处理提前加载到寄存器中。最后，再将两个结果合并到一起即可。

#### Warp-level Dns-SpMV 算法

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240315234511.png)

<center> <font face='华文宋体' size='4'> Warp-level Dns-SpMV算法 </font> </center>

在warp级别的Dns-SpMV算法中，tile 的所有元素都参与计算。一个32线程的warp需要处理一个16x16的密集tile，每个线程处理8个元素，并在八轮后完成工作。计算后，结果存储到每个线程的 sum 中，并使用`shuffle`（偏移量为tile的大小=16）将处理同一行的线程总和值相加。

如图中Dns部分所示，在灰色4x4瓦片上我们假设有八个线程（t0-t7）进行计算。一个线程处理两个元素，并在两轮后完成计算。第一轮中，这八个线程处理第一列和第二列中的元素。第二轮时，它们处理最后两列中的元素。Dns部分中灰色数组显示了每个线程工作细节。

ps：为什么要列向呢？当为横向时，为第一行计算线程为：0，1，2，3，此时需要通信的就是4个，而使用列向，进行通信的就只有两个，因此采取这种方式

#### Warp-level DnsCol-SpMV 算法

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240315234917.png)
<center> <font face='华文宋体' size='4'> Warp-level DnsCol-SpMV 算法 </font> </center>

在warp级别的DnsCol-SpMV算法中，线程的任务分配类似于Dns-SpMV。图中的DnsCol部分显示了一个示例。现在寄存器中有向量x的可用元素。由于第三列有四个元素，线程（t0-t3）独立处理它们，但在寄存器中重复使用x相同条目，如粉色数组所示。



#### Warp-level DnsRow-SpMV算法

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240315235018.png)
<center> <font face='华文宋体' size='4'> Warp-level DnsRow-SpMV算法 </font> </center>

在warp级别的DnsRow-SpMV算法中，需要进行reduction-sum操作，并且相应的x值应该加载到寄存器中。在图4中的DnsRow部分，元素分布在第三行。四个线程（t0-t3）将处理每个元素，并通过使用`shuffle`实现的reduction-sum将四个线程的结果相加到正确结果中，如DnsRow部分所示。

### Tile 中存储格式的选择方法

除了存储结构和基本的warp级别SpMV内核之外，本文还实现了三种TileSpMV算法来验证利用各种格式的有效性：
- （1）始终使用CSR格式存储所有稀疏块的TileSpMV_CSR方法。
- （2）首先检查每个块的稀疏结构并自适应地从七种中选择一种格式来存储和计算该块的TileSpMV_ADPT方法；
- （3）推迟计算非零值应该以COO形式存储（即以COO格式或HYB格式中的COO部分），通过将它们提取到单独的矩阵中并计算其自身的SpMV来进行。这个操作类似于HYBSpMV，它计算ELL-SpMV和CSR/COO-SpMV。

由于**将所有块都存储在CSR格式中并计算CSR-SpMV很简单，我们不详细介绍TileSpMV_CSR方法**。

至于TileSpMv_ADPT方法，在图中展示，并构建以下步骤：

对于非常稀疏的块，例如非零条目数量少于12且非零条目在行之间分布不均匀的情况下，COO格式无疑占用最少内存空间，因此被选定。

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240316160119.png)
<center> <font face='华文宋体' size='4'> 图 5 TileSpMV_ADPT 格式选择方法的流程图。 </font> </center>


在本文的方法中，通过实验将te和th分别设置为0.2和1.0，因为在实验过程中发现这两个阈值通常能给出最佳性能。<font color='red'><b>除了tile格式的SpMV实现外，改善负载平衡也应该被考虑进来。</b></font>

尽管使用固定大小的稀疏瓦片作为基本工作单元已经可以在一定程度上自然地避免负载不平衡，但我们还需要将非常长的瓦片行分割成小块以获得更加均匀的工作量。在我们的实现中，我们添加了一个名为`tbalance`（在我们的代码中始终设置为8）的参数，并让一个warp处理不超过tbalance个瓦片。如果一个瓦片行中稀疏瓦片数量大于tbalance，则会划分该 tile行 并使用多个warps一起处理它。

最后，由warps生成的部分y属于同一条tile row 的数据会通过原子加法相加。通过这种方式，我们可以确保每个warp具有类似任务以改善负载平衡。

📒：这是实现的负载均衡，只是简单的进行了实现。

此外，尽管上述选择和负载平衡方法可以为大多数矩阵实现良好的性能，但来自图问题的非常稀疏矩阵的SpMV性能可能仍然不理想。它们**最明显的结构是COO块占主导地位**。于是，开发了第三种选择方法，称为TileSpMV_deferredCOO。在这种方法中，具有COO数据（包括所有以COO格式存储的块和HYB格式中COO部分）的块被提取出来形成一个单独存储在普通CSR格式中，并通过CSR5-SpMV方法计算的、矩阵。也就是说，在SpMV计算中，将计算两个矩阵以共同生成最终结果向量y。


## Experimental results

### 实验设置

实验平台包括两个 NVIDIA 图形处理器: 一个 Geforce Titan RTX (图灵架构)和一个 A100(安培架构)。GPU 驱动程序版本是455.23.05，CUDA 版本是11.1。实验将TileSpMV工作与
- （1）最新的使用BSR格式的cuSPARSE v11.1内核cusparse?bsrmv()，
- （2）以及由Merrill和Garland提出的Merge-SpMV算法，
- （3）以及由Liu和Vinter提出的CSR5-SpMV算法。
以上三个进行比较。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240316163803.png)


在表I中列出了被测试的GPU和算法的规格。此外，实验并没有测试其他几种开源SpMV算法，如yaSpMV、HolaSpMV和CSR-Adaptive，因为尽管已经尽力构建它们，但仍无法让它们在CUDA v11.1环境和最新GPU上运行。

测试数据集包括 SuiteSparse Matrix Collection 中的所有2757个稀疏矩阵。



### 自适应格式选择的有效性

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240316164753.png)
<center> <font face='华文宋体' size='4'> 图 7  两个图分别显示了不同格式的tile数和非零数与总tile数和总非零数的比率 </font> </center>

在 TileSpMV 算法中，格式的选择和相应的方法给出了显著的性能增益。为了显示其有效性，通过对2757个矩阵进行基准测试，我们分别绘制了不同格式的tile数量与总tile数量的比率，以及图7(a)和(b)中不同格式的非零数量与总非零数量的比率。在图中，不同的颜色条对应不同的格式。可以看到，绿色条(代表 COO 格式)占据了tile格式的最大面积。另外，在图7(b)中，尽管 COO 格式中有许多tile，但由于 COO tile 的密度较低，COO 格式的非零比率与格式比率相比并不是很高。

实验在两个GPU上测试了TileSpMVCSR，其中每个tile最初以CSR格式存储，`TileSpMV_ADPT`采用自适应格式选择每个tile，并且`TileSpMV_Deferredcoo`可以选择是否将COO tile 拆分为新的矩阵。

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240316165152.png)

性能和加速比如图6所示。可以看到，在格式选择之后，`TileSpMV_ADPT`的性能可能比`TilespMV_CSR`快高达6.75倍，并且随着矩阵大小的增加优势变得更加明显。此外，当矩阵大小相对较小时，我们仍然选择`TilespMV_ADPT`。但是当尺寸大于某一特定尺寸（在我们的工作中为1.8M）时，`TilespMv_ Deferredcoo`的优势开始显著提升。它进一步实现了对`TileSpMV_ADPT`高达7.02倍的加速比，这表明优化效果非常有效。

### 现有 SpMV 工作的性能比较

![image.png|center|400](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240316165329.png)
<center> <font face='华文宋体' size='4'> 实验选取的矩阵 </font> </center>

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240316165318.png)
<center> <font face='华文宋体' size='4'> 测试结果 </font> </center>



### 空间成本比较

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240316165455.png)
<center> <font face='华文宋体' size='4'> 空间成本对比分析 </font> </center>

### 预处理开销分析

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240316165640.png)
<center> <font face='华文宋体' size='4'> 预处理对比分析 </font> </center>
## Related work

- 从不同角度研究利用小块结构加速 SpMV。
- 设计新的格式和算法是并行 SpMV 研究中应用最广泛的方法。
- 因为没有一种单一的格式可以为所有的稀疏矩阵提供最佳的 SpMV 性能，所以机器学习技术被用来选择给定矩阵的最佳格式和 SpMV 方法。

## Conclusion

在这项工作中，提出了一种称为Tile-SpMV的分块算法，通过利用稀疏矩阵的二维空间结构来加速GPU上的SpMV。该算法优化了warp级别的以tile为单位的SpMV，并自适应地选择每个tile的最佳格式和SpMV算法。对SuiteSparse Matrix Collection中2757个矩阵进行测试得到的实验结果表明，我们的方法比Merge-SpMV快1813个矩阵，比CSR5快2040个矩阵，比BSR快1638个矩阵，并分别实现了它们高达2.61倍、3.96倍和426.59倍的加速。
