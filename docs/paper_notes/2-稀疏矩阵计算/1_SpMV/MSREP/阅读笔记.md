## Abstract
**稀疏线性代数核**在众多应用中扮演着关键角色，这些应用范围从超大规模科学模拟到大规模数据分析。*将线性代数核心迁移到一个GPU上在这些应用中将不再可行*，<font color='red'><b>原因是迅速增长的数据量可能超过单个GPU的内存容量和计算能力。</b></font>当今的多GPU系统在超级计算机和数据中心无处不在，它们在扩大大型稀疏线性代数核心方面展现了巨大的潜力。

在这项工作中，我们为多GPU系统设计了一个名为MSREP的<font color='red' face='华文楷体'><b>新型稀疏矩阵表示框架</b></font>，以便基于增强的稀疏矩阵格式来扩展稀疏线性代数操作（平衡的格式）。与密集操作不同，稀疏性显著增加了在多个GPU之间平衡分配计算工作负载的难度。

本文优化了三种主流稀疏数据格式 - CSR、CSC和COO，以实现细粒度数据分布。我们以<b>稀疏矩阵-向量乘法（SpMV）</b>为例，展示了我们的MSREP框架的效率。此外，MSREP可以轻松扩展以支持基于这三种基本格式（即CSR、CSC和COO）的其他稀疏线性代数核心。

## Introduction

图形处理单元(GPU)在过去十年中已经成为主流和强大的加速器，并支持广泛的应用，如基于直接和迭代求解器的应用，机器学习算法等。由于互连技术的进步，多图形处理器系统如今已被广泛采用，从世界上最快的超级计算机 Sum mit [4]到 NVIDIA 的 Super-AI DGX 系统[2] ，甚至台式工作站。**通过压缩配置，最多可以将16个 GPU 安装在一个具有快速互连的计算节点上，如 NVLink 或 NVSwitch**。与传统的 PCI-e 解决方案相比，这些高速互连提供了更高的 GPU 间数据交换速率，因为传统的 PCI-e 解决方案中数据总是由 CPU 路由。这一根本性的变化为设计**用于大规模数据处理的可扩展多 GPU 算法提供了独特的机会**。

尽管如此，除了深度学习任务，很少有人研究如何利用这些新的互连优化多 GPU 系统的性能。一些针对多 GPU 的工作 ，但由于 PCI-e 的带宽有限，他们倾向于尽可能避免通信，破坏了快速互连的能力; 一个能够填补这一空白的通用框架可以对社区非常有益。

为了促进多 GPU 编程和 GPU 之间的数据共享，像 NVIDIA 这样的供应商引入了统一存储器和 NVSHMEM 技术。尽管如此，在实现工作负载平衡的同时，可延展的多 GPU 数据结构的设计仍然由程序员自行决定。

在这项工作中，我们的专注于稀疏矩阵运算的领域。

特别地，<font color='red'><b>我们介绍了一种新的数据结构，用于在多 GPU 系统上存储稀疏数据</b></font>。此外，我们还提出了一种针对多 GPU 稀疏内核的工作负载平衡技术。与密集线性代数核不同，稀疏特性使得实现平衡的工作负载分配变得非常困难。

为此，我们考虑了稀疏矩阵向量乘(SpMV)核。SpMV 是大数据分析和科学计算中应用最广泛的稀疏矩阵运算之一。在数据结构设计、性能优化、编译器实现、硬件体系结构支持以及共享内存、分布式系统和 GPU 等方面，这个内核已经被许多研究工作广泛研究。然而，<font color='skyblue' face='宋体-简'>据我们所知，利用 GPU-GPU 快速互连的优势，设计多 GPU 系统的 SpMV 内核的研究还很少。</font>现有的关于 SpMV 的工作要么涉及 CPU 来协调数据传输，要么将整个工作负载分解成小的、独立的任务，分布在多个 GPU 上，而没有仔细考虑工作负载不平衡问题。<font color='skyblue' face='宋体-简'>此外，大型线性系统的应用程序使得在单个 GPU 上装载 SpMV 内核变得不可能，因为它的内存容量有限</font>。尽管存在部署外核或分布式 SpMV 的可能解决方案，但这些**解决方案存在 CPU-GPU 传输速率慢或网络延迟**等问题。在这些情况下，保证高性能是一个挑战，特别是对于内存绑定的 SpMV 内核。因此，我们**以 SPMV 作为驱动核心来展示和说明我们的稀疏矩阵表示框架在多 GPU 系统上的有效性**。

在这项工作中，我们提出了一个通用的框架，即 MSREP，包括**增广稀疏矩阵格式**和**平衡分布**。我们证明，在我们的框架下，稀疏线性代数核，特别是 SpMV，可以在多 GPU 系统上实现可伸缩性能。此外，**我们的框架可以支持基于这些格式的现有 SpMV 内核**。具体来说，我们的贡献如下:

- 我们**通过存储稀疏矩阵的一部分，其中包含任意起始和结束位置**，扩展了三种流行的稀疏数据格式：压缩稀疏行（CSR）、压缩稀疏列（CSC）和坐标（COO）。新的格式pCSR、pCSC和pCOO需要**额外的小内存**来维护元数据，并且可以快速从众所周知的格式进行转换。
- 为了展示我们框架在多GPU系统上的效率，我们**开发了一个名为mSPMV的多GPU系统SpMV内核**。为了实现更好的可扩展性，mSPMV利用我们在多个GPU上对稀疏矩阵表示进行高效工作负载分配。
- 我们在两个密集型GPU系统上评估了我们的MSREP：奥克岭国家实验室Summit超级计算机和NVIDIA V100-DGX-1系统。使用Suit-Sparse Matrix Collection [14]中提供的矩阵进行实验表明，在Summit上使用六个GPU可以实现5.5倍加速，在NVIDIA V100-DGX-1系统上使用八个GPU可以实现6.2倍加速。

本文的其余部分按照以下方式组织。第2节中，我们提供了关于三种流行的稀疏数据格式以及使用SpMV作为示例来说明由于数据分布而导致的工作负载不平衡的背景讨论。在第3节中，我们详细讨论了我们稀疏矩阵表示框架的设计以及多GPU SpMV的设计。在第4节中，我们针对多GPU系统上的SpMV提出了几种实现优化方法。在第5节中报告了我们的实验结果。在第6节总结了我们的观察结果。在第7节概述相关工作，并在第8节得出结论。

## Background

### 主要的稀疏矩阵存储格式
![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20231218231346.png)

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20231218231408.png)

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20231218231425.png)

### 稀疏矩阵向量乘

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20231218231602.png)


### 在多GPU系统上的不平衡问题

为了激励我们的工作，我们展示了工作负载分配策略如何影响稀疏矩阵操作的性能。以 SpMV 为例，它通常是主流体系结构上的内存绑定计算，因为它的每秒浮点运算次数转换为字节数大约为 O (1)。因此，<font color='red' face='华文宋体'><b>加载输入数据的成本是影响其性能的主要因素</b></font>，因为与两个向量相比，它的内存占用相对较大，而且由于缓存和内存阶层，数据可能被重用。对于输入矩阵，每个元素在整个计算过程中只使用一次，因此 **SpMV 的主要开销来自于访问非零元素**。对于其他稀疏矩阵运算，稀疏矩阵-稀疏向量和稀疏矩阵乘以多重密集向量与 SpMV 具有相似的性质。即使对于稀疏矩阵密集/稀疏矩阵运算，由于右矩阵具有数据重用的潜力，**稀疏矩阵的存储访问仍然起着重要作用**。

对于**密集矩阵乘法**，一个常用的策略来分布元素的输入矩阵只是划分矩阵成行块，然后分配给每个不同的图形处理器进行计算。在 GPU 之间均匀地分布行将导致良好的工作负载平衡和性能。

然而，当应用到稀疏矩阵时，如果不考虑矩阵的稀疏性，这种负载分配方法将不能很好地工作。<b><font color='green' face='宋体-简'>由于对零的计算是不必要的，通常会省略它们，这导致了工作负载不平衡，因为行块之间的零的数量可能会有所变化</font></b>，如图5所示。对于SpMV来说，每个GPU的工作负载与非零元素（𝑛𝑛𝑧）的数量成比例，而不是行数（𝑚）。**SpMV中的工作负载不平衡可能严重影响整体性能**。图6显示了在SpMV中使用直接分布策略进行基准测试的结果。我们生成具有不同类型非零元素分布导致各个GPU上 𝑛𝑛𝑧 不平衡 的输入矩阵。为简化起见，该分布导致了两种类型之间 GPU 之间存在着工作负载差异。一种类型具有较高数量 𝑛𝑛𝑧 ，而其他类型则较低。低到高之间 𝑛𝑛𝑐 比率显示在x轴上。此测试在NVIDIA V100-DGX-1系统上进行。

## MSREP FRAMEWORK

在本节中，我们将介绍我们的 MSREP 框架以及我们提出的三种增强格式: pCSR、 pCSC 和 pCOO 以及工作负载管理。

### 挑战

根据 nnz 分布输入矩阵(如图7所示)是将整个工作负载划分为<font color='red'><b>更细粒度</b></font>以实现更好的工作负载平衡的最直接和有效的方法。

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20231219000310.png)

由于这项工作的重点是利用 GPU 之间的并行性，我们选择利用现有的优化工作来处理每个 GPU 上的工作负载。流行的最先进的作品和库至少支持三种主流稀疏矩阵存储格式中的一种: CSR、 CSC 和 COO。因此，**与这三种格式兼容不仅可以让我们利用现有的最先进的内核，而且还可以在将来从中受益**。然而，**使工作负载分配高效和兼容是不容忽视的**。

### 细粒度工作负载分配

#### pCSR

我们首先提出一种称为 partalCSR (pCSR)的数据格式。PCSR 可以很容易地在稀疏矩阵中表示非零元素的子集，同时保留所有必要的元素分布信息。它可以有效地从 CSR 格式转换。<font color='red'><b>一旦将 CSR 划分为 pCSR，基于 CSR 的 SpMV 和其他内核就可以使用它，而不需要开销</b></font>。

图8显示了 pCSR 的数据结构。要实现高效的转换，最直接的方法是**避免数据复制**。因此，为了在稀疏矩阵中表示非零元素的子集，我们<font color='red'><b>使用两个索引值(即 start _ idx 和 end _ idx)来标记 CSR 的非零数组中的起始和结束位置</b></font>。存储成本为 O (1)。但是，<font color='red'><b>维护本地行指针数组</b></font>对于 SpMV 内核的正确执行是必要的。存储成本与分区中的行数成正比，这取决于非零元素的分布情况。总成本不超过 O (m)。这个本地行指针数组可以高效地计算，稍后将讨论它。另外，我们<font color='red'><b>使用两个索引值(即 start _ idx 和 end _ idx)来标记列索引数组中的起始和结束位置</b></font>，因此不会引入额外的开销。由于同一行中的元素可以分布到多个 pCSR 中，因此我们还维护了一个<font color='red'><b>标志(即 start _ Flag)来标记当前 pCSR 维护的第一行是否是部分的</b></font>。不需要标记最后一行，因为可以从下一个 pCSR 的 start _ 标志推断出它。最后，为了将多个 pCSR 合并到一个 CSR 中，有必要维护两个索引，这两个索引在全局视图中存储起始行索引和结束行索引。

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20231219113339.png)
<center> <font face='华文宋体' size='5'> pCSR格式说明 </font> </center>
![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20231219110725.png)
<center> <font face='华文宋体' size='5'> 将CSR转换为pCSR </font> </center>
算法2展示了如何将CSR格式转换为pCSR格式。主要的开销来自使用二分搜索查找起始和结束索引（即𝑂(𝑙𝑜𝑔(𝑚)))以及计算本地行指针（即$𝑂(𝑝𝐴[𝑖].end\_row - 𝑝𝐴[𝑖].start\_row)$）。总成本为 $𝑂(𝑛p * 𝑙𝑜𝑔(m) + m)$。<font color='red' face='宋体-简'><b>前者可以在CPU上高效完成，而本地行指针可以使用GPU计算</b></font>。我们将展示使用GPU与不使用GPU进行分区的时间成本比较。最后，每个单独的分区都可以独立生成，因此分区过程可以高效并行化处理。

算法3展示了如何使用pCSR格式启动与CSR兼容的SpMV内核。我们可以看到，在调用SpMV内核时，pCSR可以无额外开销地转换为CSR。这确保了所有现有和未来的单个GPU上与CSR兼容的SpMV内核都可以使用我们的框架。第9至17行展示了如何将一系列部分结果正确合并为最终结果。我们将在第4节进一步讨论如何高效完成此操作。

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20231219114125.png)

#### pCSC
与 pCSR 类似，我们还提出了局部 CSC (pCSC)来划分以 CSC 格式存储的稀疏矩阵，如图9所示。算法4演示了如何有效地将 CSC 转换为 pCSC 格式。很容易看出，转换的总体成本与 pCSR 格式类似。总成本是 O (np = log (n) + n)。该算法还可以有效地并行化。算法5演示了如何在基于 CSC 的 SpMV 内核中使用 pCSC。同样，我们将在第4节中讨论如何进行有效的结果合并。
![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20231219114230.png)
![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20231219114247.png)

#### pCOO

最终，我们提出了一种用于分割基于 COO 格式的稀疏矩阵的 partial COO（pCOO）格式。为了避免元素重排序的成本，我们选择通过划分连续的非零元素来生成分区。**分割 COO 与 CSR 和 CSC 略有不同，因为元素可以是已排序或未排序的**。COO 是否排序对我们能够了解分区的信息量有影响。例如，<font color='red' face='宋体-简'><b>如果元素按行排序，则可以有效地找到与分区对应的起始和结束行索引，以便我们能够定位它将计算的结果向量的相应行，并促进部分合并过程</b></font>。然而，如果元素未排序，我们只能假设特定分区中的元素可能分布在整个矩阵中，而不知道行或列范围。这可能会带来额外的存储部分结果的内存成本和合并部分结果的时间成本。为简单起见，我们在本文中假设元素按行排序。算法 6 展示了如何有效地将 COO 转换为 pCOO 格式。可以看出，假设非零元素已按行索引排序，整体成本为 𝑂(𝑛𝑝 ∗ 𝑙𝑜𝑔(𝑚))。如果按列索引排序，则成本为 𝑂(𝑛𝑝 ∗ 𝑙𝑜𝑔(𝑛))。该算法可以有效地并行化为 𝑛𝑝 个独立任务。算法 7 展示了如何在基于 COO 的 SpMV 内核中使用 pCOO。同样，我们将在第 4 节讨论如何进行高效的结果合并。

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20231219115435.png)
![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20231219115607.png)
![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20231219115631.png)
### 在多 GPU 上管理 SpMV 工作负载

为了同时有效地管理多个 GPU，我们<font color='red'><b>使用一个专用的 CPU 线程来管理一个 GPU</b></font>。**每个线程负责为相应的 GPU 生成工作负载分区**。如果启用 NUMA 优化(将在4.2节中讨论) ，则每个 NUMA 节点上的线程还将选择一个代表性线程来处理 NUMA 节点之间的工作负载分区。

pCSR、pCSC的部分结果将在 CPU 上收集，pCOO的部分结果将在GPU上收集。有关部分结果收集的详细信息将在第4.3节中讨论。

图11-12显示了在 Summit 计算节点的6个 GPU 和 NVIDIA V100-DGX-1系统的8个 GPU 之间如何管理 SpMV 工作负载。

![image.png|center|1200](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20231219150410.png)

##  Implementation Optimizations

在这一部分中，我们讨论了与实现相关的几个问题，这些问题对于实现良好的性能和可扩展性至关重要。

### 划分工作负载

在评估中可以看到，工作负载划分可以在测试矩阵上引入高达85%的开销。由于每个分区可以像3.2节中讨论的那样独立生成，我们**通过多线程来并行化分区过程- -每个线程都是为GPU专用的**。此外，我们将最昂贵的工作负载卸载到GPU上作为专门设计的内核，例如，计算CSR的局部行指针、CSC的列指针和COO的行索引数组。由于数据从CPU到GPU的移动是不可避免的，因此不会产生额外的开销。

### 优化NUMA效率
---
NUMA节点：非一致存储访问，这是一种计算机内存设计，用于多处理器系统。在NUMA架构中，**每个处理器都连接到自己的内存**，我们称之为一个NUMA节点。这种设计有利于减少处理器访问内存时的延迟。

1. **基本架构**：在NUMA架构中，整个系统由多个处理器组成，**每个处理器或处理器组连接到自己的本地内存**。每一组处理器加上它直接连接的内存构成一个“NUMA节点”。这种设计允许处理器快速访问自己节点的内存，但访问其他节点的内存则相对较慢。
2. **内存访问特性**：
    - **本地访问**：当处理器访问其直接连接的内存时，即发生在同一NUMA节点内的访问，这被称为本地访问。本地访问速度较快，因为它避免了网络延迟。
    - **远程访问**：当处理器尝试访问连接到另一个处理器的内存时，这称为远程访问。由于需要通过一个或多个互连网络，因此访问速度会较慢。
3. **优化策略**：为了最大化性能，软件和操作系统需要考虑NUMA架构。这包括内存分配策略（如首选本地节点内存）和任务调度（尽量减少跨节点的数据移动）。
---
由于SpMV是一个受内存限制的计算，数据移动的成本通常主导着整个操作的成本。因此，将由pCSR、pCSC或pCOO表示的分区工作负载有效地从CPU内存复制到每个GPU内存中非常重要。密集型GPU节点通常将多个GPU分区在几个NUMA节点之间。例如，在Summit上，六个GPU被分配在每个计算节点上的两个NUMA节点之间，如图11所示。如果工作负载分区被天真地放置（即在一个NUMA节点上），那么很难将SpMV扩展到三个以上的GPU。这主要受限于单一NUMA节点内CPU内存吞吐量和不同NUMA节点之间互连速度（例如，在Summit上使用X总线）两者共同影响。在这项工作中，我们设计了mSpMV来考虑NUMA效应，并确保pCSR、pCSC或pCOO分区位于不同的NUMA节点之间。

放置策略是**根据每个NUMA节点上的GPU数量来确定工作负载分区数目比例适当地进行放置**。我们使用了两级划分策略来高效完成此过程。如图13所示，**第一级将工作负载划分为不同的NUMA节点**，**第二级则对各自包含多少块GPU进行划分**。这种两级划分策略使得划分工作本身可以并行化进行。
![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20231219173129.png)

### 合并部分结果

影响多GPU系统上SpMV性能的另一个问题是如何将部分结果合并到最终结果中。所使用的划分格式会影响部分结果的合并方式。

基本上可以将其分类为两类：**基于行的划分**和**基于列的划分**。

- **基于行的划分**，例如pCSR和pCOO（按行排序），将连续的行指派给单个划分。因此，每个划分的结果是最终结果向量的一段，除了每端可能是部分结果（如果当前划分与其他划分共享相同行）。为了优化合并过程，我们使用GPU-CPU复制直接将非重叠结果复制到CPU内存中最终位置，并让CPU处理重叠元素。这带来相对较低的性能影响，因为（1）内存复制可以同时进行；（2）由于只需要处理$np$次重叠问题。
- **基于列的划分**，例如pCSC和pCOO（按列排序），将连续列指派给单个划分。因此，每个划分的结果是一个向量， 其维度与最终结果相同 ，但向量中的每个元素都是部分结果 。为了优化合并过程，我们首先让所有GPU收集它们各自的部分结果到一个GPU上，然后将结果复制回CPU。

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20231219182105.png)

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20231219182118.png)

## Experimental evaluation

在这一部分，我们报告我们的实验结果，并详细讨论我们的观察。

### 评估平台

我们在两个多图形处理器平台上评估我们的框架: Sum mit 超级计算机橡树岭国家实验室和 NVIDIA v100-dgx-1系统。在 Summit 上，使用一个计算节点进行计算。每个节点配备了6个 Nvidia Tesla V100图形处理器，每个图形处理器上有16 GB 内存，两个 IBM POWER9 CPU 有512 GB 内存。GPU 和 CPU-GPU 通过 NVLinks 相互连接。CPU-CPU 通过 X 总线相互连接。在 DGX-1系统上，采用整个系统进行计算。DGX-1系统配备了8个 Nvidia Tesla V100图形处理器和两个20核英特尔至强 E5-2698 v4，内存为512 GB。GPU 通过 NVLinks 相互连接。CPU-GPU 通过 PCIe 相互连接。CPU 通过 QPI 相互连接。我们使用 GCC 7.4.0和 CUDA 10.1.168编译了我们的框架。我们使用 OpenMP (版本4.5)在 GPU 上生成任务对于单 GPU 内核执行，我们使用 cusSparse 中基于 CSR 的 SpMV 内核。如果输入为 CSC 格式，则调用内核并打开转置，以避免格式转换。对于基于 COO 的输入，首先在 SpMV 内核之前调用基于 GPU 的 COO-to-CSR 转换内核。我们在报告的时间中包含了转换步骤和计算步骤的执行时间。

### 选择矩阵

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20231219182827.png)
表2列出了我们在评估中使用的稀疏矩阵集合，这些矩阵是从SuitSparse Matrix Collection 中收集而来的。这些矩阵按非零元素的数量进行排序。在选择这些矩阵时，我们选择具有强力法特性（偏斜度分布）的矩阵。这种模式通常在社交网络和Web图中观察到。正如Yang等人所报道的那样，这些矩阵列中非零元素的数量遵循幂律分布。因此，我们应用此规则来定位这些矩阵。幂律形式上表示为：𝑃 (𝑘) ∼ 𝑘 −𝑅 被认为是符合幂律分布的。其中，𝑅被称为幂率指数，并且可以根据非零元素分布计算得出，𝑘表示每列非零元素的数量。通常，在区间内选择一个值作为𝑅与强力法现象相关联。

### 评估方法

### 负载划分

### 部分结果合并开销

### NUMA感知的影响

### 整体表现

## Discussion

在本节中，我们将进一步讨论我们的设计以及这项工作的潜在影响。

与单个GPU工作相比：我们的工作介于单个GPU工作和分布式GPU工作之间。专注于稀疏数据的单个GPU工作需要考虑将数据加载到寄存器的效率以及线程级别的细粒度并行性。因此，通常会开发一种新的存储格式来促进性能优化。我们的多GPU工作旨在利用现有的单个GPU工作，并使其可扩展到超过一个GPU。因此，这项工作主要关注与现有工作兼容性，而稀疏数据格式是兼容性的主要决定因素。我们不提出新格式，而是旨在使现有稀疏格式跨多个GPU可扩展。必须特别注意数据分区、结果合并。此外，还需要明确处理CPU和GPU之间内部连接拓扑中的内存复制问题。

分布式GPU系统的影响：分布式GPU系统与密集型多GPU系统类似。因此，我们的框架也可以支持分布式GPU系统。然而，由于GPU计算节点之间的互连带宽通常有限，在选择工作负载类型和数据格式时必须特别小心。例如，使用CSR或COO输入的SpMV会带来较少的通信成本，因此在分布式GPU系统上更有可能实现相对良好的可扩展性。对应用程序的益处：在多GPU系统上实现可扩展稀疏操作可能使许多依赖这些操作的应用程序受益，如压缩卷积神经网络（CNN）、图卷积网络、基于图形的算法以及许多其他超级计算科学应用程序。


## Related works

在本节中，我们总结了关于多GPU SpMV、单GPU SpMV以及采用我们的稀疏数据格式进行多GPU执行的图算法的现有工作。

多GPU上的SpMV：杨等人[39]在多GPU集群上实现了SpMV，每个CPU节点连接一个GPU。每个节点保留矩阵的一个本地分区，最后所有节点将其本地结果广播给其他节点。广播的通信成本是限制可扩展性的关键因素。我们的工作在三个方面与这项工作有所区别：1) 只有部分结果以较低开销合并；2) 提出了先进的工作负载分配策略以处理工作负载不平衡；3) 我们针对NUMA效应优化了设计，而不是假设均匀的节点配置（使用MPI）。我们还想提到，我们的节点内扩展设计与他们的扩展外设计是独立的，因此可以集成以处理极大的矩阵。克鲁策等人[24]提出了一种基于新稀疏矩阵存储格式pJDS的SpMV设计。同样，这项工作也针对分布式GPU集群。舒伯特等人[5]提出了一种多GPU SpMV设计——KSPARSE SpMV，基于Blocked-Sparse-Row格式。他们的实现将独立任务分配给各个GPU，难以保持工作负载平衡。郭等人[18]在多GPU系统上实现了SpMV，使用CPU多线程和并发GPU流，但无法利用高速GPU互连如NVLink。

单GPU上的SpMV：已经有很多研究工作致力于提高单GPU上的SpMV性能。其中一些提出了新的数据格式以便于SpMV处理，如CSR5[26]、BCCOO[37]、SELL-C[6]、HYB[8]，而其他则应用自动调优方法来提高SpMV性能[12、16、22、28、33]。还有许多工作提出了针对GPU硬件的架构专用设计，利用GPU硬件的独特机会[7、8、17、21、27、32]。我们的工作可以利用这些单GPU上的快速实现，并将它们应用于MSREP。图算法：图算法是可以直接从提出的pCSR和pCSC数据结构中受益的一类重要应用，用于多GPU执行。例如，现有的多GPU图分析框架，如Gunrock[30]和Groute[9]，本质上是在多个GPU中将图数据按CSR格式分区。此外，GraphBLAS[10]规范将图算法表达为在扩展的半环代数上的稀疏矩阵和向量运算，这可以通过GPU加速[38]。我们的稀疏矩阵表示以及SpMV设计可以为GraphBLAS带来新的优化机会。

## Conclusion

稀疏线性代数核在许多应用中起着至关重要的作用。随着数据量的指数增长，由于存储容量和计算性能的限制，在单个 GPU 中进行稀疏线性代数运算将很快变得不切实际。在这项工作中，我们提出了一个新的多 GPU 系统稀疏矩阵表示框架-MSREP，旨在帮助规模稀疏线性代数操作在多个 GPU 上进行计算。我们使用 SpMV 来展示我们的框架的效率。对 NVIDIA V100-DGX-1系统和 Summit 超级计算机的评估结果表明，基于 MSREP 的 SPMV 设计可以实现线性加速比: 在 Summit 上使用6个图形处理器5.5倍，在 NVIDIA V100-DGX-1系统上使用8个图形处理器6.2倍。

