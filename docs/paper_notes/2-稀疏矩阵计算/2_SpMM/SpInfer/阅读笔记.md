
[pdf](zotero://open-pdf/library/items/HUSISW52)

## 概述

---

**研究背景与问题：**
LLM模型的参数量极大，需采用非结构化剪枝（unstructured pruning）减少计算和存储开销。但在GPU上低稀疏度（30%-60%）下，非结构化稀疏SpMM的性能和内存占用仍然不如稠密版本，如cuBLAS，主要受限于稀疏格式的索引开销和kernel执行效率。

**核心方法：**
SpInfer针对低稀疏度非结构化剪枝的SpMM问题，提出两个关键优化组件：
- **Tensor-Core-Aware Bitmap Encoding (TCA-BME)：** 使用位图编码表示稀疏矩阵的非零结构，显著降低索引冗余；
- **Shared Memory Bitmap Decoding (SMBD)：** 在Tensor Core中采用共享内存对位图进行快速解码，结合异步流水机制降低延迟、提升吞吐。
SpInfer兼容当前主流SpTC架构，尤其针对稀疏分布均匀且剪枝比例不高（~50%）的LLM推理场景进行优化。

**实验结果：**
在30%-70%稀疏率下，SpInfer在SpMM kernel上相比Flash-LLM和SparTA分别提升**2.14×和2.27×**，在整体模型推理上加速比达**1.58×**，首次在低稀疏度下超过cuBLAS的稠密实现；内存占用也较传统CSR、COO等稀疏格式显著降低。

---

## 0-Abstract

大型语言模型（LLMs）展示了显著的能力，但其巨大的规模在内存和计算成本方面带来了重大挑战。虽然无结构剪枝通过引入稀疏性来减少资源需求，提供了有希望的解决方案，但在LLM推理中实现其好处仍然难以捉摸。这主要是由于**索引非零元素的存储开销和在低稀疏水平（约50%）下稀疏矩阵乘法（SpMM）内核的效率低下**。

本文提出了SpInfer，这是一个针对GPU上稀疏LLM推理量身定制的高性能框架。SpInfer引入了张量核心感知位图编码（TCA-BME），一种<font color='red'><b>新颖的稀疏格式</b></font>，通过利用高效的基于位图的索引来最小化索引开销，优化了GPU张量核心架构。此外，SpInfer集成了一个优化的SpMM内核，采用共享内存位图解码（SMBD）和异步流水线设计，以提高计算效率【<font color='red'><b>内核计算优化</b></font>】。

实验结果表明，SpInfer在一系列稀疏性水平（30%到70%）中显著超越了最先进的SpMM实现（分别比Flash-LLM和SparTA提高2.14倍和2.27倍），在内存效率和端到端推理速度方面也有显著提升（最高提高1.58倍）。在低至30%的稀疏水平上，SpInfer的性能超过了高度优化的cuBLAS，标志着无结构剪枝的理论优势首次有效转化为LLM推理的实际性能提升。

> [! error]  稀疏矩阵乘法SpMM内核在低稀疏的矩阵上效率低下并且在存储低稀疏的矩阵时索引非零元素开销较大

## 1-Introduction

大型语言模型（LLMs）已彻底改变了人工智能应用，展现出在摘要、遵循指令和回答问题等多个领域的卓越能力。然而，这些模型的巨大规模，通常由数十亿个参数构成，带来了显著的挑战。LLMs 的广泛内存需求和相关计算成本使得它们的部署和推理非常消耗资源，严重阻碍了它们在当代硬件平台上的广泛实施。

为了应对这些挑战，模型压缩技术得到了广泛关注，其中权重剪枝（或稀疏化）作为一种有前景的方法，能够减少内存消耗和计算负担。权重剪枝通过消除神经网络中不那么重要的连接，引入了模型的稀疏性。剪枝方法分为结构化、半结构化和非结构化。
- **结构化剪枝**移除整个组件，但通常需要昂贵的后训练。
- **半结构化剪枝**，如 N:M 剪枝，通过控制稀疏性来实现灵活性与效率的平衡。
- **非结构化剪枝**自由地移除单个权重，提供了最大的灵活性，通常能产生更好的后训练性能，通常在准确性上超过结构化方法。

> [! info]- 剪枝
权重剪枝是一种**模型压缩技术**，通过删除神经网络中不重要的连接（即权重），使得模型变稀疏，从而减少内存占用和计算负担。常见的剪枝方法如下：
**结构化剪枝**：删除整个神经网络结构的单元，如通道、滤波器、头（head）、甚至是整个层（layer）。优点是易于加速（可以映射到高效的稠密计算内核），但缺点是通常会显著影响模型性能，因此在剪枝后**必须通过代价昂贵的后训练**（如重新微调）来恢复精度。                          
**半结构化剪枝（如N:M剪枝）**：在每N个权重中保留M个非零元素，常用于GPU上已有硬件支持的稀疏模式。相比结构化剪枝更灵活，硬件加速也有可能支持，后训练开销中等。                                                                                                                                                                               
**非结构化剪枝**：完全不限制稀疏模式，可以任意地剪掉网络中某些单个权重。它提供最大灵活性，通常能在相同稀疏度下保持更好的准确性，但在硬件上很难实现高效加速（比如 SpMM 在低稀疏度下的效率差）。

然而，在 LLM 推理中利用非结构化稀疏性以获得性能提升和内存节省仍然特别具有挑战性。与较小模型不同，较高的稀疏率在 LLM 中的容忍度要低得多。例如，像 Vision Transformer 和 ResNet-50 这样的模型可以在不显著降低准确度的情况下达到 70%到 95%的稀疏性，这得益于广泛的后训练的可行性。然而，对于极大规模的语言模型来说，由于重新训练相关的高计算和时间成本，这样的后训练通常是难以承受的。从算法的角度来看，在 LLM 中实现同样高的稀疏水平而不严重降低性能是不切实际的。<font color='blue'><b>目前最先进的剪枝技术，如 SparseGPT、Wanda、GBLM-Pruner 和 PrunerZero，通常在模型准确性不可接受地受到影响之前仅能达到约 50%的稀疏性</b></font>。

📒：由于 LLM 模型庞大、再训练代价极高，**非结构化剪枝的优势就更为突出**，因为它可以**以较低的稀疏度（如50%）实现剪枝而不需要昂贵的后训练**，而结构化方法就难以应用于这类大模型。

![image.png|center|500](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250408145223.png)
<center> <font face='华文宋体' size='4'> 图1：Nvidia RTX4090 上非结构化 SpMM 实现与 cuBLAS 的执行时间比较 (M/K/N = 28K/8K/16，典型的 LLM 推断)。 </font> </center>

这一低稀疏水平给在 LLM 推理中实现非结构化剪枝的好处带来了两个主要挑战。
- 首先，在这种稀疏水平下，**减少权重存储变得困难**，因为需要考虑索引开销（即存储非零元素的索引）。像传统 CSR 和 Flash-LLM 中的 Tiled-CSL 这样的稀疏格式，在约 50% 的稀疏性下导致内存使用量增加，因为存储非零值及其索引的需求可能会抵消剪枝带来的内存节省。
- 其次，实现**实际加速**仍然是一个重大障碍，特别是在主导 LLM 部署的 GPU 上。尽管基于 CPU 的稀疏加速解决方案，如 Neural Magic 的 DeepSparse 已显示出一定的潜力，但由于 GPU 的 SIMT 执行模型和复杂的内存层次结构，使得 GPU 加速面临独特挑战。**当前最先进的稀疏矩阵乘法（SpMM）内核在提供剪枝的基础支持时，往往难以超越其密集型对手（cuBLAS）**。尽管专门为 LLM 剪枝设计，Flash-LLM 在 50% 或更低稀疏性时也难以实现加速。因此，LLM 的稀疏化在实际系统中尚未完全实现其理论潜力，理论加速与 LLM 推理中的实际加速之间仍存在显著差距。

为了弥补这些差距，我们提出了SpInfer，这是一个高性能框架，专门设计用于通过利用GPU上的低级非结构稀疏性来加速大规模语言模型的推理。SpInfer的核心是<font color='red'><b>张量核心感知位图编码</b></font>（TCA-BME），这是一种新颖的稀疏矩阵存储格式，通过采用高效的位图索引来最小化索引开销。TCA-BME经过精心设计，以与GPU张量核心架构对齐，确保SpMM操作能够充分利用这些核心的计算能力，即使在存在非结构稀疏性的情况下。通过减少稀疏矩阵的内存占用和优化数据访问模式，TCA-BME使SpInfer在内存效率和计算吞吐量方面都实现了显著的提升。在TCA-BME格式的基础上，SpInfer集成了一个高度优化的SpMM内核，进一步提升了性能。该内核实现了一个优化良好的数据移动路径，并引入了<font color='red'><b>共享内存位图解码</b></font>（SMBD），使稀疏矩阵可以直接在共享内存中解码，显著减少了解码开销。此外，该内核具有<font color='red'><b>异步流水线设计</b></font>，可将内存传输与计算重叠，提高GPU资源的利用率。

我们从内核级和端到端框架级评估SpInfer的性能。
- 在内核级，SpInfer与最先进的SpMM实现进行比较，包括基于Tensor-Core的FlashLLM、SparTA、SMaT和基于CUDA-core的Sputnik和cuSPARSE。SpInfer在不同的稀疏度水平上实现了显著的加速，范围从低（30%）到中等（70%）。
- 在框架级，SpInfer与Flash-LLM、FasterTransformer和DeepSpeed进行比较，在生成延迟和推理期间的内存使用方面取得了显著的改善，证明了其在资源受限环境中部署的有效性。

我们论文的主要贡献包括：
- 我们进行详细分析，并确定**索引开销是实现无结构剪枝收益的关键瓶颈**，强调需要解决它以提高内存效率和计算加速。
- 我们提出了SpInfer，一个高性能的稀疏大模型推理框架。其核心是引入了TensorCore-Aware Bitmap Encoding格式，减少了索引开销并高效压缩稀疏矩阵。
- 我们还设计了一种专门的SpMM内核，进行了定制优化，使SpInfer能够显著加速稀疏矩阵计算。
- 我们展示了SpInfer在内核和框架层面上在推理速度和内存效率上都有显著提升，超越了之前的最先进解决方案，适用于从低稀疏度（30%）到中等稀疏度（70%）的广泛范围。

据我们所知，SpInfer是第一个成功将稀疏大模型理论加速转化为实际性能收益的框架。

## 2-Background and Related Work

### 2.1-LLM 体系结构与推理过程

LLMs（大型语言模型）构建于 Transformer 架构之上，该架构采用多层堆叠的自注意力机制（self-attention）与前馈神经网络（FFNs）。自注意力机制使得 LLM 能够建模序列中所有 token 之间的关系。输入 token 会通过线性投影被转换为查询（Query, Q）、键（Key, K）和值（Value, V）向量。注意力机制的核心是 Q 与 K 矩阵的乘法运算，计算得到的注意力权重随后被用于加权 V 矩阵。此外，每一个 Transformer 层中还包含一个前馈神经网络（FFN），它通过两个线性变换与一个非线性激活函数进一步精炼 token 的表示。LLM 的推理过程由两个阶段组成：**预填充阶段（prefill phase）与解码阶段（decode phase）**。在预填充阶段，整个输入提示（prompt）可以被并行处理；而在解码阶段，模型以自回归（autoregressive）方式逐 token 地生成结果，每次处理一个 token。

LLM 推理的效率主要依赖于矩阵乘法运算，尤其是在自注意力与前馈网络中。我们将权重矩阵表示为 $W \in \mathbb{R}^{M \times K}$，将 token 嵌入表示为 $X \in \mathbb{R}^{K \times N}$，其中：
- M 是输出维度，
- K 是隐藏层维度，
- N 是 token 的数量。
矩阵乘法 $W \times X$会生成变换后的 token 表示。在预填充阶段，N 为序列长度与 batch size 的乘积（即 $\text{seq\_len} \times BS$）；而在解码阶段，由于模型每次只处理一个 token，$N = BS \times 1$，即 batch size 个样本，每个处理一个 token。

### 2.2-NVIDIA 图形处理器和张量核心

NVIDIA GPU 拥有多个流式多处理器（SM），每个 SM 内部包含 CUDA 核心、Tensor Core（简称 TC）以及层级化的内存结构。线程块被调度到 SM 上运行，每个 warp 包含 32 个线程，这些线程在 SIMT（Single Instruction, Multiple Threads）模式下同步执行指令。GPU 的内存结构包括延迟较高的全局内存（global memory，可被所有线程访问）、更快但作用范围仅限于 SM 的共享内存（shared memory）、以及速度最快但容量有限的寄存器（registers，每个线程私有）。缓存系统包括每个 SM 一级缓存（L1 cache，可配置为与共享内存共享），以及一个统一的二级缓存（L2 cache），它优化了计算核心与全局内存之间的带宽和延迟。Tensor Core 是用于加速稠密矩阵乘法的专用硬件单元。TC 执行的计算形式为：
$$

D_{\text{frag}} = A_{\text{frag}} \times B_{\text{frag}} + C_{\text{frag}},

$$
其中 $A_{\text{frag}} \in \mathbb{R}^{m \times k}$ 与 $B_{\text{frag}} \in \mathbb{R}^{k \times n}$ 为输入矩阵，$C_{\text{frag}}$ 是累加器，$D_{\text{frag}}$ 是输出矩阵。我们将矩阵的形状记作 $m \times k \times n$。在我们的实现中，使用了 PTX 层级的低层次指令 mma，这提供了更大的寄存器控制灵活性。在使用 FP16 精度时，这些 mma 指令要求矩阵的形状必须为：$16 \times 16 \times 8 \quad \text{或} \quad 16 \times 8 \times 8$。文献中 Listing 1 提供了一个 FP16 类型的 mma 指令示例。尽管 Tensor Core 在加速稠密矩阵乘法方面表现优异，但要用其高效加速**非结构化稀疏矩阵乘法（unstructured SpMM）** 仍面临极大挑战。

![image.png|center|500](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250408152151.png)

### 2.3-相关工作

**量化与稀疏化**。量化与稀疏化是减少大规模语言模型（LLMs）计算和内存需求的关键模型压缩技术。量化利用低精度表示，许多研究改进了算法，如后训练量化（PTQ）和量化感知训练（QAT），以及系统级支持，如MARLIN、LADDER和Qserve，使其在实际应用中广泛适用。另一方面，稀疏化通过各种剪枝策略减少非零权重的数量，包括结构化剪枝和非结构化剪枝，通常目标是达到约50%的稀疏性，同时保持准确性。尽管非结构化剪枝能实现更好的精度，但其对稀疏矩阵内核的依赖限制了其在当前硬件上的效率。我们的SpInfer为低级稀疏剪枝提供了实用的系统级支持，同时补充了这些量化技术。最近的研究还探讨了动态激活稀疏性以增强效率，如Deja Vu、PIT和PowerInfer。这些方法利用由ReLU激活函数引起的稀疏性，而非权重稀疏性。然而，它们要求模型使用稀疏激活函数，如ReLU，或进行重新训练。我们的方法针对权重稀疏性，消除了重新训练的需求，并且在不同的范围内运作。

**稀疏矩阵-矩阵乘法**。SpMM计算$O_{M×N} = W_{s_{M×K}} × X_{K×N}$，其中$W_s$、$X$和$O$分别是稀疏权重矩阵、输入嵌入和输出矩阵。我们用$NNZ$表示$W_s$中非零元素的数量。许多研究旨在加速针对高度稀疏的科学和GNN工作负载的SpMM。尽管在低稀疏度的LLM推理中效果较差，但它们的设计提供了有价值的见解。对于深度学习工作负载，研究集中于各种粒度的结构稀疏性，包括块稀疏性、向量稀疏性和N:M半结构化剪枝。然而，这些方法对结构化剪枝的依赖限制了它们在非结构化稀疏中的适用性。最近的研究关注在低稀疏水平下更具挑战性的非结构化剪枝问题。**Sputnik应用一维Tile和反向偏移内存对齐，来有效利用CUDA核心**。**SparTA将矩阵划分为2:4结构稀疏和非结构稀疏成分，利用稀疏Tensor Cores和CUDA核心**。**Flash-LLM采用Load-as-Sparse-Compute-as-Dense方法来减少内存占用**。尽管在较高稀疏水平（70%-90%）时有效，但我们的分析显示上述方法忽视了索引开销，这是低稀疏场景中的关键瓶颈。因此， 在低于50%的稀疏水平下，它们难以减少存储或提高性能。

**系统级优化**。系统级优化涉及增强推理引擎和在线服务系统。推理引擎主要旨在通过图和内核优化以及卸载技术来加速前向传播。这些优化集中在改进注意力机制（例如，FlashAttention）、重构计算图（例如，ByteTransformer和DeepSpeed）以及优化线性操作（例如，TensorRT-LLM、MegaBlocks和FlashDecoding++）。卸载方法，例如FlexGen和llama.cpp所实现的，通过将模型组件分配到各种硬件资源来优化内存使用。我们的SpInfer，针对权重剪枝，可以与这些方法结合，以进一步提升性能。此外，许多工作集中于优化在线服务系统，以高效处理来自多个用户的请求。关键改进领域包括内存管理、连续批处理、调度策略和分布式服务。我们的工作与这些服务系统是正交的，可以补充并提升它们的性能。

## 3-Gaps and Opportunities

### 3.1-LLM 推理的瓶颈

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409102808.png)

LLM推理面临显著的计算和存储挑战。图2显示了在2个RTX4090 GPU上使用FasterTransformer的OPT-13B的运行时间和内存分解，批量大小为16，输出长度为256。模型权重存储占据了87.6%的内存，**相关的矩阵乘法操作（GEMM）消耗了61.6%的执行时间**，构成了主要瓶颈。尽管权重剪枝可以通过去除不太重要的权重来减少内存和计算，但LLM剪枝中的低稀疏性限制了当前剪枝方法在现代GPU上的实际有效性。这个挑战在第3.2节中进一步讨论。

### 3.2-被忽视的索引开销

<font color='blue'><b>现有的稀疏LLM推理技术利用稀疏计算，但由于需要存储非零元素的索引信息，在低稀疏度水平下引入了显著的存储开销</b></font>。之前的一些工作如Flash-LLM、SparTA和Sputnik普遍忽视了这些成本。具体而言，索引开销不仅阻碍了存储效率，还损害了计算性能。索引所需的空间可能抵消修剪带来的存储收益，而在矩阵乘法期间访问索引可能降低计算效率，尤其是在内存带宽成为瓶颈的GPU上。

#### 3.2.1 存储复杂性差距

为了量化索引开销对存储复杂度的影响，我们定义了一个压缩比（Compression Ratio, CR）指标，用于衡量稀疏矩阵格式的存储效率：
$$

CR = \frac{2B \times M \times K}{Stor_{Format}},

$$
其中 $2B \times M \times K$ 表示原始稠密矩阵的大小，$Stor_{Format}$ 表示稀疏格式的压缩存储大小。我们对几种广泛使用的稀疏矩阵格式进行了比较分析：Tiled-CSL、CSR（压缩行存储格式）和 SparTA。

**Tiled-CSL** 使用两个数组以 tile 为单位存储非零元素：NonZeros，该数组包含 32 位（16 位 × 2）值（表示权重和位置）；TileOffsets，该数组记录每个 tile 的起始偏移量。Tiled-CSL 的存储开销可以表示为：
$$

Stor_{Tiled-CSL} = 4B \times NT + 4B \times NNZ,

$$
其中 $NT$ 是 tile 的数量。每个非零元素需要一个 16 位索引，因此索引开销与数据本身大小是可比的。


**CSR** 是一种传统的稀疏表示格式，存储非零元素及其列索引。CSR 的存储开销为：
$$

Stor_{CSR} = (2B + 4B) \times NNZ + 4B \times (M + 1).

$$
在 CSR 中，使用 32 位索引来存储列索引，这会带来显著的索引开销。


**SparTA**使用了一种可组合格式，将矩阵分为两部分：一部分遵循2:4稀疏模式，另一部分使用类似CSR的格式。它通过在2:4块内以2位索引存储非零元素来减少开销。对于包含超过两个非零元素的块，SparTA利用类似CSR的存储格式来存储剩余的非零元素。因此，SparTA的实际存储开销依赖于矩阵中非零元素的分布。假设非零元素呈均匀分布，所需CSR存储的块中非零元素的预期数量可以表示为：
$$

E_{CSR_nnz} = \left(\frac{M \times K}{4}\right) \times \left(4 \times (1 - s)^3 \times s + 2 \times (1 - s)^4\right).

$$
因此，SparTA 的存储开销可以写为：
$$

Stor_{SparTA} = \left(2B + \frac{B}{4}\right) \times \frac{M \times K}{2} + Stor_{CSR}(E_{CSR_nnz}).

$$
图 3 展示了不同稀疏度下的 CR 趋势，使用了 $M = K = 4096$ 的代表性规模，适用于大型语言模型的权重。
  ![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409104210.png)
在稀疏率低于 50% 时，CSR 和 Tiled-CSL 的压缩比（CR）均低于 1，意味着它们的索引开销超过了剪枝带来的存储节省。SparTA 表现稍好，在 50% 稀疏率时 CR 略高于 1，但仍未达到理论最优（虚线所示），这主要是由于其依赖于类似 CSR 的索引方式。相比之下，我们提出的 TCA-BME 格式（蓝线）在较低稀疏率下也能稳定实现 CR > 1。这归功于其先进的基于位图的索引技术，**显著降低了记录非零元素位置的开销**。TCA-BME 的详细内容将在第 4.2 节中讨论。

#### 3.2.2 计算效率的差距

为了分析索引开销对计算效率的影响，我们采用 Roofline 模型，并重点研究稠密矩阵乘法（GEMM）与稀疏矩阵乘法（SpMM）的计算强度（Compute Intensity, CI）。

**计算强度（CI）** 是理解计算操作与内存访问操作之间平衡的关键指标。其定义为浮点运算次数（FLOPs）与内存访问次数的比值。对于 GEMM，CI 定义为：
$$

CI_{GEMM} = \frac{M \times N}{M + N}.

$$
对于 SpMM，CI 受到压缩比（Compression Ratio, CR）的影响，CR 反映了由于稀疏性导致的存储减少。此外，索引的开销也会进一步降低有效 CI。因此，SpMM 的 CI 被定义为：
$$

CI_{SpMM} = \frac{M \times N}{\frac{M}{CR} + N}.

$$
为了衡量由索引开销引起的性能差距，我们将这些稀疏格式的实际计算强度与最优 CI 进行比较，最优 CI 假设索引开销可以忽略。最优的 SpMM CI 可定义为：
$$

CI_{Optimal} = \frac{M \times N}{M \times (1 - s) + N},

$$
其中 $s$ 表示稀疏率。该最优 CI 表示性能的理论上限，反映了在非零元素索引开销可忽略的情况下可达到的最大计算强度。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409105612.png)
<center> <font face='华文宋体' size='4'> 图 4 在不同稀疏度和批量大小下，各种 SpMM 实现与 GEMM 的底线比较。 </font> </center>

**Roofline 模型分析。** 图 4 显示，在不同稀疏度和矩阵规模下，GEMM 和 SpMM 操作大多数处于 Roofline 模型中的内存受限区域。在该区域内，性能与 CI 呈线性关系。理论上，稀疏性带来的全局内存访问减少以及 CI 的提升使得 SpMM 在性能上可线性逼近 GEMM，这在图中用星号（∗）表示。然而，SpMM 的实际 CI 受到 CR 的影响，CR 反映了由于稀疏性带来的内存访问成本的降低。<font color='red'><b>随着 CR 的增加，全局内存访问成本下降，从而提高 CI，进而提升性能</b></font>。因此，具有更高 CR 的格式在理论上能比低 CR 格式获得更好的性能。这一关系在 Roofline 模型中表现得尤为明显。我们提出的 TCA-BME 格式由于其高效的基于位图的索引方法，显著提高了 CR，从而使其 CI 也得以提高，并使其更接近计算受限区域。相比之下，像 CSR 和 Tiled-CSL 这样的传统格式，由于索引方式传统，CR 较低，导致内存访问成本更高，从而降低了有效 CI。索引开销显著拉大了它们与最优 CI 之间的性能差距，如图中星号所示。我们分析表明，索引开销是限制剪枝在实际应用中存储与性能收益的主要因素。通过解决该问题，可显著降低存储需求，使性能更接近理论收益。这正是 SpInfer 设计背后的关键动因。

## 4-Design of SpInfer

### 4.1-设计概述

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250408161541.png)
<center> <font face='华文宋体' size='4'> 图5：SpInfer 的系统概述 </font> </center>

SpInfer是一个高性能框架，旨在通过利用稀疏矩阵乘法加速GPU上的LLM推理。图5展示了SpInfer的系统概览。通过先进的无结构剪枝算法，SpInfer在不影响准确性的情况下减少了模型的大小。该框架的基础是其Tensor-Core-Aware Bitmap Encoding (TCA-BME)方案，该方案高效地<font color='red'><b>压缩稀疏权重矩阵，最小化索引开销</b></font>。SpInfer的核心特点是一个<font color='red'><b>高度优化的SpMM内核</b></font>，通过高效的数据移动、共享内存位图解码(SMDB)和细粒度的异步流水线相结合，提高了效率。这些设计使SpInfer显著降低了大规模LLM推理中的延迟和内存消耗，同时保持模型准确性，无需额外的微调。

### 4.2- Tensor-Core-Aware Bitmap Encoding

作为SpInfer的基础，我们开发了一种新的TensorCore-Aware Bitmap Encoding （TCA-BME）方案来高效存储低稀疏度的稀疏权矩阵。这种格式旨在最小化内存占用（增加压缩比），同时保持计算效率，为随后在张量核上的高性能稀疏矩阵乘法奠定基础。

#### 4.2.1-Tile 设计

TCA-BME采用多级Tile设计，将权重矩阵划分为不同粒度的Tile，以适应不同级别的GPU硬件。如图6所示，该设计包括三个关键抽象层次：BitmapTile (BT)、TCTile (TT)和GroupTile (GT)，每个层次对应GPU硬件中的不同计算单元。

![image.png|center|500](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250408162426.png)

最内层的抽象是**BitmapTile**，它作为TCA-BME格式中最小的粒度单元。其尺寸设定为$BT_H × BT_W$为8 × 8，这一设计针对<font color='red'><b>Tensor Cores的最小计算单元</b></font>，即8 × 8的矩阵块。将BitmapTile的尺寸对齐到这个单元的一个额外优势是能够利用CUDA原生支持的uint64_t数据类型作为64位位图，表示BitmapTile中非零元素的位置。位图中的每一位对应BitmapTile中的特定元素，设置的位表示非零值。

中间层由**TCTiles**组成，其尺寸为$TT_H × TT_W$，包括2 × 2 BitmapTiles，总大小为16×16。这个TCTiles抽象对应于PTX层的**Tensor Core** mma指令的矩阵形状。对于FP16精度，有两个相关的PTX级别指令可用：mma.m16n8k8和mma.m16n8k16。微基准测试结果表明，形状较大的mma指令提供更高的吞吐量，使我们选择了mma.m16n8k16指令（2个8\*8的BitmapTile），并将TCTiles的尺寸完全与其m×k对齐。在TCTiles中，2×2 BitmapTile以**列优先格式排列**，确保与<font color='red'><b>mma指令中四个Ra寄存器的顺序一致</b></font>。具体来说，**左上角的BitmapTile对应于Ra0，左下角对应于Ra1，右上角对应于Ra2，右下角对应于Ra3**。这种列优先存储方法方便后续的解码过程，避免复杂的坐标变换，降低在线开销。

最外层是**GroupTile**，尺寸为$GT_H × GT_W$，包含多个TCTiles并对应于**线程块级别**。GroupTiles中的TCTiles也以列优先顺序存储。线程块负责加载和处理GroupTiles，而线程块中的warps处理GroupTile内TCTiles的计算。**GroupTiles本身则以行优先顺序存储**。

📒：
- GroupTile：线程块级别，使用行优先顺序进行存储
- TCTile：TCU计算单元级别，使用列优先顺序存储
- BitmapTile：TCU中最小计算单元


#### 4.2.2-存储格式

TCA-BME格式使用三个数组高效地表示稀疏权重矩阵。
- `GTileOffset`数组记录稀疏矩阵中每个GroupTile的起始偏移位置，便于快速定位和并行处理不同的GroupTiles。📒：每个GroupTile中的非零元的数量
- `Values`数组存储所有非零元素，按GroupTile、TCTiles和BitmapTile的嵌套顺序排列。
- `Bitmap`数组包含所有BitmapTiles的位图值，每个BitmapTile由一个64位整数表示（📒：对应BitmapTile的大小为8\*8），每一位指示相应元素是否为非零。

具体地，我们定义$NGT = (M/GT_H) × (K/GT_W)$为GroupTiles的数量，$NBT = (M/BT_H) × (K/BT_W)$为BitmapTiles的数量，$NNZ = M × K × (1 − s)$为非零元素的数量，其中$s$表示矩阵稀疏度。

`GTileOffset`数组利用32位整数（4B）表示偏移量，大小为$4B × (NGT + 1)$，包括一个额外的元素来标记最后一个GroupTile的结束。`Values`数组使用半精度浮点数（`FP16`）存储非零元素，每个元素占用`2B`，总大小为`2B × NNZ`。在`Bitmap`数组中，每个BitmapTile对应一个64位整数（8B），总大小为$8B × NBT$。因此，TCA-BME格式的总存储开销可以计算为：$$Stor_{TCA−BME} = 4B × (NGT + 1) + 8B × NBT + 2B × NNZ$$。TCA-BME即使在低稀疏度下也保持有效的压缩比率（CR > 1），并且随着稀疏度增加，CR迅速增长。这种优越性能源于其高效的<font color='red'><b>基于位图的索引方案</b></font>，显著减少了索引开销，尤其是在低至中等稀疏度（30%-70%）时。



### 4.3-高性能SpInfer-SpMM内核设计

#### 4.3.1-工作流

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250408165615.png)
<center> <font face='华文宋体' size='4'> 图7. 数据移动和指令流水线。 </font> </center>

SpInfer-SpMM内核的工作流程如图7所示。算法1中提供了详细的伪代码表示。我们的内核采用类似于<font color='red'><b>CUTLASS GEMM的基于Tile的策略与splitK并行性</b></font>，来高效地在线程块之间分配计算，每个块独立处理K维的一部分。在每次迭代中，线程块执行五个关键操作。
- ❶ GTile加载。块内的线程协同将GTile（GroupTile）从全局内存加载到共享内存中的WTile。
- ❷ WTile解码。WTile通过一种名为**共享内存位图解码**（SBMD）的关键技术从共享内存解码到寄存器中。此步骤<font color='red'><b>将稀疏矩阵的紧凑位图表示转化为准备进行Tensor Core计算的寄存器文件中的正确分布</b></font>，全部在高速寄存器文件内完成。
- ❸ XTile加载。相应的来自密集输入矩阵X T的XTile从全局内存加载到共享内存中。
- ❹ XTile寄存器传输。XTile数据随后从共享内存传输到寄存器，并为TC计算进行排列。
- ❺ Tensor Core计算（TCC）。Tensor Cores随后在寄存器中执行解码后的稀疏WTile与密集XTile之间的矩阵乘法。

> [! warning] 💡这里转换后，进入到TC中运算的单元仍然是稀疏的呀？？？但DTC-SpMM引入聚合了呀？？？

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250408171542.png)

#### 4.3.2-高效的数据移动

![image.png|center|500](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250408165615.png)

在步骤❶和❸中，我们使用`LDGSTS.128`异步向量化内存访问指令来提高全局内存带宽利用率。引入自Ampere架构，`LDGSTS`消除了通过L1缓存和寄存器文件的中间数据暂存需求，从而减少了寄存器文件带宽消耗。**128表示每个线程从全局内存读取128位的数据（例如，8个半精度操作数）**。为了在步骤❶中启用128位向量化，每个GTile内的Value数组经过填充预处理，确保**每个GTile的起始地址对齐到8字节边界**。在步骤❹中，我们利用`LDSM.M88`指令（对应于PTX级别的`ldmatrix.x4`）从共享内存加载XTile。该指令允许一个warp从共享内存加载一个16x16的矩阵tile，并根据❺ TC计算所需的布局自动安排寄存器中的数据。对于步骤❷，我们使用`通用LDS指令`将WTile从共享内存解码到寄存器中。图7展示了在cuBLAS-GEMM、FlashLLM和SpInfer-SpMM中获取权重矩阵（W）时的数据移动路径。cuBLAS代表理想情况，其中**使用LDGSTS允许数据绕过L1缓存和寄存器文件，直接存储到共享内存中**。相比之下，Flash-LLM首先使用`LDG.128`将Tiled-CSL格式的NonZeros Array加载到寄存器文件中，随后将其解包到共享内存中。SpInfer-SpMM通过`LDGSTS.128`直接将GTile加载到共享内存中，实现了一个与理想的cuBLAS情况非常接近的数据移动路径。该方法还通过避免通过寄存器文件的来回传输，节省了SM内部带宽，避免了Flash-LLM中所带来的额外开销。

#### 4.3.3 共享内存位图译码 (SMBD)

SMBD机制是SpInferSpMM内核的重要优化，旨在<font color='red'><b>高效地将位图压缩的WTile解压缩到寄存器文件中</b></font>，确保后续Tensor Core计算的正确布局。该技术利用位图表示矩阵的稀疏模式，非零值以压缩格式存储，从而实现高效的内存使用和高性能的矩阵运算。

**寄存器分配**。在warp级的Tensor Core操作中，一个warp（32个线程）共同处理操作数矩阵的片段。warp中的每个线程持有部分操作数矩阵，这些片段在线程之间的分配必须谨慎进行，以确保mma指令的正确执行。对于半精度计算，我们使用`mma.m16n8k16`指令，该指令对16 × 16的矩阵片段进行操作。图8(a)展示了矩阵片段的分配，其中**每个线程在32位寄存器中持有两个半精度值**（.f16x2）。每个线程需要四个这样的寄存器（Ra0, Ra1, Ra2和Ra3）来存储整个片段。这些寄存器通过位图解码填充，从中提取非零值。

**两阶段解码过程**。如第4.2节所述，TCTile由四个BitmapTiles组成，每个对应一个寄存器（Ra0, Ra1, Ra2和Ra3）。BitmapTile是一个64位值，编码了8 × 8矩阵片段的稀疏模式，每个位表示对应位置是否存在非零值。一个挑战来自于非零值的压缩存储，这意味着并<font color='blue'><b>没有显式存储每个线程加载其值的确切偏移量</b></font>。为了计算正确的偏移量，我们依赖两个关键操作。
- ❶ PopCount，由Nvidia GPU的整数内置函数`__popcll`实现，计算64位位图中1位的数量。这个计数表示对应BitmapTile中的非零值数量。通过累计BitmapTile中的PopCount结果，可以动态确定压缩Values数组中每个tile的正确起始偏移量。这允许warp高效加载非零值，而无需在全局内存中存储显式偏移量。
- ❷ MaskedPopCount。除了计算整个BitmapTile的偏移量外，每个线程还需要确定在位图中其`lane`之前有多少个非零值。MaskedPopCount操作计算当前线程的`lane ID`之前的1位数量，如图8(b)所示。该操作对于计算每个线程在压缩Values数组中加载其非零值的正确偏移量至关重要。详细实现见算法2，展示了这一高效的位计数过程。
![image.png|center|500](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250408181948.png)
位图解码过程分为两个阶段，如图8(c)所示。
- ❶ 第一阶段（解码a0）。在第一阶段，每个线程解码其32位寄存器中的第一个半精度值（a0）。线程ID为i的线程检查位图的第($2i$)位。如果该位为1，该线程使用MaskedPopCount计算其位置之前存在多少个非零值，并从压缩的Values数组中加载相应的值。如果该位为0，该线程将零值加载到其寄存器中。
- ❷ 第二阶段（解码a1）。在第二阶段，每个线程从同一个32位寄存器中解码第二个半精度值（a1）。线程ID为i的线程检查位图的第($2i+1$)位，以确定该位置是否存在非零值。然而，在第二阶段不需要额外的MaskedPopCount。**第一阶段的结果被重复使用**。具体而言，**如果第一个值（a0）是非零的，则偏移量增加1以加载第二个值（a1）**。这种重复利用第一阶段的MaskedPopCount结果减少了计数操作的次数，提升了性能。通过使用内置的PopCount和MaskedPopCount操作，我们在所有线程之间高效地并行解码压缩的矩阵片段，确保每个线程访问正确的非零值，而无需显式存储偏移量。

#### 4.3.4 异步流水线设计

我们开发了一种精细粒度的异步流水线，以进一步优化SpInfer-SpMM内核的性能。如图9所示，该流水线通过**最大化内存传输和TC计算之间的重叠**，提高了TC的利用率。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250408182400.png)
<center> <font face='华文宋体' size='4'> 图9  异步流水线设计的示意图，流水线深度为2 </font> </center>

**双缓冲机制**。双缓冲构成了流水线设计的基石。我们为GTiles和XTiles实现了两个独立的共享内存缓冲区。该架构使得在使用当前迭代数据进行计算的同时，可以将下一迭代的数据预取到共享内存中，从而隐藏内存加载延迟并提高整体吞吐量。具体来说，在每次迭代中，当前的GTile和XTile数据存储在一个共享内存缓冲区中，而下一组数据则通过异步预取（使用cp.async）加载到备用缓冲区中。如第4.3.1节所述，我们的工作流程设计允许对W和X矩阵使用LDGSTS异步指令，使我们能够实现类似于cuBLAS的双缓冲机制。

**精细粒度的异步组管理**。为进一步提高效率，我们采用了两个独立的cp.async组来管理GTiles和XTiles的加载。这种精细控制能够在不同的流水线阶段实现更大的并发性。我们的设计包含两种关键的重叠策略。**一旦GTile加载完成，共享内存位图解码（SMBD）过程立即开始**，并与正在进行的XTile加载并行运行。由于XTile加载和SMBD是独立操作，它们的并行执行可以有效隐藏SMBD的延迟，防止其成为性能瓶颈。此外，在为当前Tile发出Tensor Core计算指令后，下一Tile的SMBD过程立即开始。<font color='blue'><b>在SMBD中进行的位操作和计数操作在CUDA核心上执行，与Tensor Core指令无关</b></font>。<font color='red'><b>将SMBD与TensorCore计算交替进行增加了指令级并行性（ILP），通过保持CUDA核心和Tensor Cores的活跃状态，优化硬件资源利用，减少流水线停顿，提高吞吐量</b></font>。

📒：只对GTile进行格式的创新

> [! warning] 也没说XTile采用什么格式存储的呀？？针对SpMM整个计算过程没有提完全？？XTile矩阵如何处理呢？后面又是怎么累加的呢？

## 5-Performance Evaluation

我们在两个层面上评估SpInfer的性能：SpMM内核层面和端到端框架层面。实验在两个平台上进行。❶ Intel Xeon Platinum 8352V CPU (2.10GHz) 搭配4个NVIDIA RTX4090 GPUs (Ada Lovelace, Compute Capability 8.9, 每个GPU 24 GB内存)，通过PCIe连接，带宽为30.5 GB/s。❷ Intel Xeon Gold 6133 CPU (2.50GHz) 搭配4个NVIDIA A6000 GPUs (Ampere, Compute Capability 8.6, 每个GPU 48 GB内存)，通过成对的NVLink连接。代码使用GCC 9.4.5和NVCC 12.1编译。对于内核级评估，使用Nsight Compute测量精确的执行时间。对于端到端评估，推理过程运行100次，并记录平均时间。

### 5.1-内核性能比较

**数据集**。我们使用来自LLM模型的多样的权重矩阵尺寸评估SpInfer-SpMM。这些包括OPT系列（13B、30B、66B和175B）、LLaMA2系列（7B、13B和70B）、LLaMA3系列（8B和70B）、Qwen2（7B和72B）以及Mixtral-8×7B MoE模型。

**基准测试**。SpInfer-SpMM与几个关键基准进行比较，包括：
- ❶ cuSPARSE v12.1，这是一款广泛使用的供应商提供的SpMM库；
- ❷ Sputnik，一种针对深度学习稀疏性优化的最先进的基于CUDA核心的SpMM；
- ❸ SparTA，这是首次利用稀疏Tensor Cores进行无结构SpMM的方法；
- ❹ FlashLLM，一种针对稀疏LLM推理设计的最先进的基于Tensor-Core的SpMM；
- ❺ 基于Tensor-Core的cuBLAS，这是用于稠密LLM推理的相应库。
- 此外，我们还将SpInfer与用于科学工作负载的先进基于Tensor-Core的SpMM进行比较，包括❻ SMaT。
评估在稀疏度水平为40%到70%之间进行，这代表了前沿LLM剪枝技术所针对的最佳稀疏性范围。

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250408184358.png)

**结果**：图10展示了在RTX4090和A6000上的测量性能加速值相对于基于Tensor-Core的cuBLAS（cuBLAS_TC）进行归一化，用红色虚线表示。SpInfer在稠密和稀疏实现中始终提供优于的加速。在RTX4090上，SpInfer平均比cuBLAS快1.79倍，相较于cuSPARSE、Sputnik、SparTA和FlashLLM平均加速分别为18.14倍、2.55倍、1.67倍和1.56倍。在A6000上也观察到类似趋势，SpInfer相较于cuBLAS平均加速1.51倍，并且在cuSPARSE中最高达到了24.80倍。在低稀疏度水平（40%）下，SpInfer是唯一能够始终优于cuBLAS的方法，平均加速为1.46倍，并在94.44%的矩阵中超过cuBLAS。在关键的50%稀疏度水平上，SpInfer以1.66倍的平均加速保持领先，在96.30%的测试用例中优于所有其他内核。竞争方法如SparTA和Flash-LLM对cuBLAS的改进非常有限，加速分别为1.01倍和1.00倍。随着稀疏度增加到70%，SpMM通常变得更具优势，SpInfer的性能更加出色，超过cuBLAS实现1.90倍的加速，并在100%的测试用例中优于cuBLAS。相比之下，SparTA和Flash-LLM的增益更加温和（分别为1.16倍和1.22倍）。这些结果反映了**SpInfer处理低至中等非结构稀疏性的能力，而这通常对传统稀疏内核构成挑战**。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250408184933.png)

图11展示了SpInfer和SMaT之间的性能比较。在50%稀疏度下，SpInfer以2.12倍的加速优于SMaT。SMaT仅在极端稀疏度超过99.7%时超过SpInfer，其设计通过跳过高度稀疏科学矩阵中的零块来优化性能。然而，在低至中等稀疏度范围内，通常在LLM推理中，能够被跳过的块很少，限制了SMaT的优势。

**微观分析**。为了进一步解释SpInfer的性能提升，我们对SpMM内核进行详细的微观层面分析。关键指标包括**寄存器分配**、**读入的DRAM字节**、**带宽利用率**、**bank冲突**和**Tensor Core利用率**。我们通过Nsight Compute收集这些指标。结果如图12所示。
![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250408185443.png)
与其他方法相比，SpInfer消耗的寄存器最少。这种效率是通过直接解码共享内存中的稀疏数据来实现的，避免了额外寄存器用于存储稀疏数据。较低的寄存器使用率允许更高的GPU占用率，使更多线程能够并发运行，提高整体计算效率。此外，SpInfer显著减少了耗时的DRAM访问，最小化了全局内存和计算单元之间的数据传输量。这个减少主要得益于TCA-BME格式的高效性，它优化了数据存储和访问模式。此外，SpInfer在最小化共享内存bank冲突方面表现出色。相比之下，Flash-LLM需要连续线程将稀疏数据写入共享内存中的特定位置。由于稀疏数据的固有随机性，这通常会导致写操作期间不可避免的共享内存bank冲突。SpInfer的设计避免了此类冲突。最后，由于高效的SMBD和异步流水型设计，SpInfer的Tensor Core流水线利用率高于Flash-LLM。这种优化的流水线确保数据传输和计算有效重叠，从而更好地利用Tensor Cores。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250408185531.png)

**消融研究**。为了量化SpInfer中关键优化的影响，我们通过选择性地去除SMBD和异步流水线（AsyncPipe）优化来进行消融研究，并分析它们对性能的影响。结果如表1所示。在没有SMBD的情况下，内核执行时间增加了10.03%，带宽利用率下降了68.78%，发射槽活动减少了75.77%。此外，Tensor Core的利用率下降了78.41%，表明<font color='red'><b>SMBD对优化内存访问和确保高效硬件使用至关重要</b></font>。当去除AsyncPipe时，执行时间增加了1.98%，Tensor Core的利用率下降了2.00%，这表明该优化在重叠内存传输与计算方面发挥了关键作用，从而提高了整体效率。


### 端到端LLM推理

**基准和设置**。我们将SpInfer与最先进的框架进行比较，包括Flash-LLM、DeepSpeed和FasterTransformer，用于稀疏和密集的LLM推理。使用的模型包括OPT-13B、OPT-30B和OPT-66B，提供了广泛的规模范围。在先进的Wanda算法的支持下，模型稀疏性设定为60%，使OPT-13B在WikiText数据集上保持15.9的困惑度。SpInfer的精确性依赖于当前的LLM剪枝算法并得到保证。实验在1、2和4个GPU配置下以批量大小为8、16和32进行，以评估在不同并行处理场景下的可扩展性和效率。输出长度设定为64、128、256、512和1024个标记，以便在推理过程中分析在不同计算负载下的性能。

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250408190038.png)

**结果**。RTX4090 和 A6000 图形处理器上 OPT 模型的端到端推断结果分别如图 13 和图 14 所示。SpInfer 始终优于基线框架，显示出在延迟和内存效率方面的显著改进。

**关于延迟**，SpInfer 在基线框架上有显著的加速。在 RTX4090 上，SpInfer 的平均加速比 FlashLLM、 FT 和 DS 分别提高了 1.35 × 、1.42 × 和 1.49 × 。A6000 的加速比分别为 1.29 × 、1.36 × 和 1.55 × 。RTX4090 上 Flash-LLM 的最大加速比为 1.58 × ，发生在批量大小为 32 的 1-GPU 配置中，与 Flash-LLM 的 1183.58 令牌 / 秒相比，SpInfer 处理超过 1817.02 令牌 / 秒。在使用 OPT-13B 的 2-GPU 配置中，SpInfer 在 Flash-LLM 上的平均加速比为 1.34 × ，而在 4GPU 的 OPT-30B 配置中，加速比略微降低到 1.28 × 。尽管随着 GPU 数量和模型大小的增加 (主要是由于与模型并行性相关的通信开销增加) ，相对加速速度趋于减小，但 SpInfer 仍然是最有效的解决方案。在内存效率方面，SpInfer 优于其他框架，特别是在遇到内存不足 (OOM) 问题的场景中。利用 TCABME 格式，SpInfer 实现了模型权重的稀疏对齐内存减少，从而从根本上提高了存储效率。例如，当执行 OPT-13B 推理时，批量大小为 16，序列长度为 256，SpInfer 的 60% 稀疏模型仅消耗 14.4 GB 内存，与密集基线的 27.4 GB 要求相比减少了 47.5% 。这种内存压缩对于较大的批处理大小和较长的输出序列尤其重要，因为竞争框架存在局限性。在一个 RTX4090 图形处理器上使用 OPT-13B，批量大小为 8，SpInfer 可以支持最多 1024 个输出令牌，而 Flash-LLM 最多只能支持 256 个令牌。类似地，在 2 个 RTX4090 GPU 上使用 OPT-30B 时，Flash-LLM 在所有批量大小和输出长度上都会遇到 OOM 错误，而 SpInfer 可以处理批量大小为 16 的多达 512 个令牌，批量大小为 8 的多达 1024 个令牌。当在 2 个 A6000 GPU 上推断 OPT-66B 模型时，这种趋势也是显而易见的，其中 SpInfer 展示了比其他框架更好的内存管理。这些优势背后的原因在于 SpInfer 卓越的 SpMM 性能和它的 TCA-BME 格式的高压缩比，它几乎以稀疏性线性地有效地降低了内存需求。这种组合使得 SpInfer 在实际 LLM 推理场景中更加通用和可伸缩。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250408190218.png)

**性能分析**。为了进一步解释SpInfer的性能提升，我们使用Nsight Systems对执行时间进行细分，如图15所示。SpInfer和Flash-LLM的主要时间消耗都在SpMM操作上，而FasterTransformer则是GEMM。在相同配置下，**SpInfer的SpMM相比Flash-LLM的SpMM和FasterTransformer的GEMM所需时间明显更少**。此外，由于SpInfer优越的内存效率，它在相同配置下通常只需较少的GPU，通常是Flash-LLM和FasterTransformer所需数量的一半。这不仅减少了硬件需求，还带来了额外的性能优势。例如，使用OPT-13B模型时，SpInfer只需要1个RTX4090 GPU，消除了使用2个GPU时FasterTransformer和Flash-LLM所需的GPU之间通信时间。这个优势在A6000 GPU集群中也很明显。然而，在RTX4090 GPU集群中这一优势尤其突出，因为仅提供带宽较低的PCIe，而不能使用NVLink。

## 6-Limitation and Discussion

尽管 SpInfer 在性能和内存效率上显示出了显著的改进，但在批处理大小和序列长度（$N = BS × Seq_{len}$）较大时，它在预填充阶段面临限制。在这些情况下，SpInfer 的速度可能比 cuBLAS_TC 慢多达 11.8%，因为操作变得更依赖计算，从而减少了我们内存访问优化的好处。位图解码开销也导致了这一性能差距，尤其是在密集矩阵操作中，cuBLAS 更有效地利用了 Tensor Cores。然而，这一影响受多个因素的缓解。即使在预填充阶段，由于其高压缩的 TCA-BME 格式，SpInfer 实现了可观的内存节省，这对于在有限硬件上管理长序列和大型模型至关重要。此外，随着推理系统越来越多地采用解耦的预填充和解码阶段架构，SpInfer 针对解码阶段的优化使其适合可扩展部署。解决这一限制需要硬件级别的支持，比如稀疏 Tensor Cores 或专用的稀疏 GEMM 加速器，这些都是未来优化的有希望的方向。

除了权重稀疏性外，SpInfer 目前不支持动态激活稀疏性，在这种情况下，稀疏模式会根据输入依赖激活在运行时变化。扩展 SpInfer 以适应这种运行时稀疏性需要自适应稀疏编码技术，以保持计算效率。此外，在极端稀疏水平（>90%）时，位图索引的效率下降，因为使用过多的位表示零，导致压缩比低于 CSR 格式。在这种情况下，像 DTC-SpMM 和 SMaT 的替代方法更为有效。

尽管 SpInfer 针对 NVIDIA Tensor Cores 进行了优化，但其核心技术具有普适性，可以迁移到其他硬件架构上。TCA-BME Tile策略可以根据不同的矩阵乘法单元（如 Google TPU、AMD Matrix Cores 和 Intel AMX）进行调整，通过将瓷砖配置与其各自的规格对齐。同样，SMBD 依赖于基本的位操作，这在现代架构中均可用。未来的研究包括开发编译器优化，以自动化 SpInfer 对多种硬件架构的适配，提高其跨平台效率。

## 7-Conclusion

在本文中，我们提出了SpInfer，这是一个高效的框架，旨在通过利用无结构剪枝和稀疏矩阵乘法来加速GPU上的LLM推理。SpInfer的核心是一种新颖的Tensor-Core-Aware Bitmap Encoding（TCA-BME）格式，它解决了索引开销这一关键问题，使得在低稀疏度水平下也能显著提高内存效率和计算性能。我们还提出了一个高度优化的SpInfer-SpMM内核，其中结合了共享内存位图解码（SMBD）和异步管道等技术，以最大化GPU资源的利用。我们的评估显示，SpInfer在各种稀疏度水平下始终超越当前最先进的SpMM内核和推理框架，实现了显著的加速和减少内存使用。SpInfer被证明是第一个能够有效加速低稀疏度（低于50%）下的LLM推理的框架，同时保持计算效率和内存节省，填补了当前稀疏推理技术中的一个关键空白。












