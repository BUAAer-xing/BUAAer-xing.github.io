
[pdf](zotero://open-pdf/library/items/8W97E8NQ)

## 概述

### **(1) 研究背景**
图神经网络（GNN）在处理图结构数据（如社交网络、电商推荐、金融风控等）方面展现了极大的潜力和广泛的应用前景。然而，GNN的核心计算（特别是邻居聚合操作）涉及高度稀疏、结构不规则的图计算，这与深度学习硬件（尤其是GPU上的Tensor Core Unit，TCU）设计目标严重不符。现有GNN框架（如DGL、PyG）主要依赖CUDA核心与稀疏线性代数库（如cuSPARSE），导致其无法充分利用TCU所提供的高吞吐性能。

### **(2)研究问题**
尽管TCU在稠密计算（如GEMM）中表现出色，但其固定的tile结构、对稀疏性支持差，使得直接将其应用于GNN稀疏操作不仅无效，甚至可能比传统CUDA实现更慢。作者指出，当前GNN加速的瓶颈主要集中在稀疏邻居聚合操作（SpMM-like）上，占总计算时间的80%以上。主要问题包括：
1. 稀疏矩阵存取效率低，缓存命中率差；
2. 稀疏计算并行度有限，Streaming Multiprocessor (SM) 占用率低；
3. 当前<font color='red'><b>TCU对不规则稀疏模式支持不足</b></font>，padding浪费严重。

### **(3)方法**
作者提出了 **TC-GNN** 框架，首次在GPU TCU上高效支持稀疏GNN计算。其核心方法包括：
1. **Sparse Graph Translation (SGT)**：通过识别图中邻居共享结构，重新索引稀疏邻接矩阵，将零散的稀疏块压缩为密集块（tile），以便适配TCU对tile输入的要求；
2. **CUDA Core 与 TCU 协同执行**：由CUDA核心负责稀疏数据加载与tile初始化，TCU专注于tile内GEMM计算，提升总体计算吞吐；
3. **TCU定制化内核设计**：通过共享内存重排稀疏图数据布局，结合Fragment机制优化内存访问模式；
4. **PyTorch 接口集成**：提供高层API（如TCGNN.GCNConv）与低层API（如spmm、sddmm），方便研究人员无缝迁移现有PyTorch代码。

### **(4)实验结果**
作者在多种GNN模型（GCN, AGNN）和多类数据集（小图、高维图、大图）上进行了详细评估。主要结论如下：
- **与DGL对比**：平均提升 **1.70×**，在小图数据集上提升可达 **2.23×**；
- **与PyG对比**：在AGNN模型上可达 **2.82×**；
- **与cuSPARSE bSpMM对比**：平均 **1.76×** 提升，有效计算提升 **75.8%**；
- **与tSparse、Triton对比**：分别实现 **3.6×** 和 **5.42×** 的加速；
- SGT使得 **TCU块数平均减少67.47%**，且仅占总训练时间 **约4.43%** 的开销。

### **(5)结论**
TC-GNN通过结构感知的稀疏图转换与TCU定制化执行机制，有效解决了GNN稀疏计算与GPU稠密硬件之间的性能鸿沟。该工作不仅展示了稀疏计算在AI定制硬件上的新方向，也为未来TCU支持动态tile形状与结构稀疏性的设计提供了启示。TC-GNN可作为后续GNN硬件加速和系统优化研究的重要基础。

## Abstract

最近，图神经网络（GNN）作为基于图的机器学习的核心，在各个领域（例如电子商务）取得了巨大成功。然而，由于**高度稀疏**和不规则的基于图的操作，GNN的性能通常令人不满意。

为此，我们提出了TC-GNN，**首个基于GPU张量核心单元（TCU）的GNN加速框架**。其核心思想是将“稀疏”的GNN计算与高性能的“密集”TCU进行结合。具体来说，我们对主流GNN计算框架中的稀疏操作进行了深入分析。我们<font color='red'><b>引入了一种新颖的稀疏图转换技术，以促进TCU对稀疏GNN工作负载的处</b></font>理。我们实现了一种有效的<font color='red'><b>CUDA核心与TCU协同设计</b></font>，以充分利用GPU资源。我们将TC-GNN与PyTorch框架集成，以实现高可编程性。

严谨的实验表明，在各种模型和数据集上，TC-GNN相比于最先进的DGL框架平均获得了1.70倍的加速。

## Introduction

近年来，随着基于图学习的日益普及，图神经网络（GNN）在电子商务、金融服务等广泛领域的基本任务计算中逐渐占据主导地位。与随机游走和图拉普拉斯等标准图分析方法相比，GNN在准确性和泛化能力上表现出显著优势。从计算的角度来看，**GNN在聚合阶段进行图操作（散播与聚集）和在更新阶段进行神经网络操作（矩阵乘法）时，特征呈现交错执行的阶段**。我们的实验研究进一步表明，聚合阶段涉及对不规则输入图进行高稀疏计算，通常占据GNN训练和推理运行时间的80%以上。现有的GNN框架，如深度图书馆和PyTorch Geometric，主要建立在最初针对稠密操作优化的热门神经网络框架上，如通用矩阵乘法。为了支持GNN中的稀疏计算，它们的共同策略是在后端实现中融入稀疏基础操作。然而，cuSPARSE利用的是稀疏线性代数算法，该算法涉及大量高成本的间接内存访问，主要针对稀疏矩阵中的非零元素。因此，cuSPARSE无法享受与其稠密对手如cuBLAS相同程度的优化（例如数据重用）。此外，**cuSPARSE设计仅利用CUDA核心**，因此无法从GPU硬件特性的新进展中受益，例如最近的NVIDIA安培和霍普架构GPU上的张量核心单元。这种设计也是许多其他针对AI的加速器/单元的趋势，能够显著提升大多数传统深度学习应用中稠密线性代数算法的性能。

本工作专注于探索TCUs在加速基于GNN的图学习中的潜力，我们的设计/优化原则也将惠及其他类似的AI硬件，以应对稀疏深度学习工作负载。我们指出，使TCUs有效用于通用GNN计算是一项复杂的任务。我们的初步研究表明，简单地将TCU用于稀疏GNN计算甚至会导致性能低于现有的CUDA核心上的稀疏实现。面临几个挑战。
- 首先，直接用纯密集GEMM解决稀疏GNN计算问题是不切实际的，因为会产生非常大的内存开销（O(N2)，其中N是节点的数量）。
- 此外，遍历已经填满全部为零元素的矩阵切片将导致过多不必要的计算和内存访问。其次，简单地使用TCUs处理稀疏图邻接矩阵的非零矩阵切片仍然会浪费大部分TCU的计算和内存访问。这是因为**TCU输入矩阵Tile的维度设置是固定的**（例如，高度（16）×宽度（8）），而稀疏图邻接矩阵的非零元素分布不规律。因此，需要大量的零值填充以满足这种严格的输入约束。
- 第三，尽管最近的CUDA发布更新使TCUs能够利用某些类型的稀疏性，但它仅支持**块状SpMM**（结构化稀疏SpMM），其中非零元素必须首先适合良好形状的块，并且不同行的块数必须相同。这种输入限制使得在实际的GNN应用中处理高度不规则的稀疏图变得困难。

为此，我们引入了TC-GNN，这是第一个基于TCU的GNN加速设计，运行于GPU上。我们的主要思路是<font color='red'><b>让稀疏输入图适应TCU的密集计算</b></font>。
- 在输入层面，我们开发了一种新的**稀疏图转换(SGT)技术**，而不是逐个遍历所有稀疏矩阵块并确定是否处理每个块，该技术可以有效识别出非零块，并**将这些块中的非零元素浓缩成更少的“密集”块**。我们的主要观察是，真实世界图中的节点之间共享邻居是非常普遍的。因此，应用SGT可以有效合并不同节点之间共享邻居的不必要数据加载，以避免高成本的内存访问。SGT对任何类型的稀疏输入图模式都是通用的，并且始终能够产生与原始稀疏算法相同的正确结果。
- 在GPU内核层面，为了高效处理GNN稀疏工作负载，TC-GNN利用了<font color='red'><b>CUDA核心和TCU协同工作</b></font>的优势。主要设计思想是：
	- <font color='red'><b>CUDA核心</b></font>在细粒度线程级执行方面更为强大，适合管理<font color='red'><b>内存密集型数据访问</b></font>。
	- <font color='red'><b>TCU</b></font>在处理简单算术运算（如乘法和加法）方面更为强大，<font color='red'><b>适合计算密集型的GEMM操作</b></font>，这些操作在SGT生成的密集块上进行。
- 在框架层面，我们将**TC-GNN与流行的PyTorch框架**集成。因此，用户只需通过使用TC-GNN API与他们熟悉的PyTorch编程环境进行交互。这可以显著减少额外的学习工作，提高用户的生产力和代码的可移植性。

总之，我们总结了我们的贡献如下：
- 我们对现有解决方案（例如，CUDA核心上的SpMM）进行了详细分析，并识别了TCUs加速稀疏GNN工作负载的潜力。
- 我们引入了一种稀疏图转化技术。它可以使稀疏和不规则的GNN输入图适应TCUs的密集计算以实现加速。
- 我们为CUDA核心和TCUs的协作构建了一种TCU定制算法和GPU内核设计，以处理不同的稀疏GNN计算。
- 大量实验表明，TC-GNN在各种主流GNN模型和数据集设置上，平均比先进的GNN计算框架Deep Graph Library快1.70倍。




















