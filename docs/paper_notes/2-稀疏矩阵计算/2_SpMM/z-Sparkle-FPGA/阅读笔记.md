
[高效的 SpMM 深度学习加速器及其自动生成器](zotero://open-pdf/library/items/ZLTX6YKK)

## 0-Abstract

深度学习（DL）技术在视觉、语言、推荐系统等广泛智能任务中取得了突破性进展。稀疏矩阵乘法（SpMM）是大多数稀疏模型的关键计算内核。**传统计算平台，如CPU、GPU和具有规则处理单元的人工智能芯片，由于其固定结构和指令集，无法有效支持稀疏计算**。

本研究扩展了**Sparkle，这是一种专门为处理DL中的SpMM而开发的加速器架构**。在平衡数据加载过程中，实施了一些修改，以增强Sparkle架构的灵活性。此外，提出了一种Sparkle生成器，以适应多样化的资源约束，并促进灵活部署。借助Sparkle的结构参数和基于模板的设计方法，生成器能够在不同参数下自动生成Sparkle电路。

一个实例化的Sparkle加速器在特定配置下在Xilinx xqvu11p <font color='red'><b> FPGA平台</b></font>上实现。与最先进的SpMM加速器SIGMA相比，Sparkle加速器实例提高了稀疏计算效率约10%到20%。此外，Sparkle实例的性能比Nvidia Orin NX GPU高出7.76倍。评估了具有不同参数的更多加速器实例，证明Sparkle架构能够有效加速SpMM。

## 1-Introduction

计算机科学领域随着深度学习（DL）技术的快速发展发生了显著的转变。这一进展在软件和硬件领域都带来了新的机会和挑战。通用矩阵乘法（GEMM）是各种DL模型中的关键组成部分，特别是在卷积神经网络（CNN）和多层感知器的全连接层中。对于一般的CNN，卷积操作也可以通过Im2Col过程重新表述为GEMM。在像Transformer这样的基于注意力的神经网络中，**核心计算主要是注意力机制中的GEMM**。

随着深度学习模型变得越来越大，可能会出现过度参数化，导致准确性下降。<font color='red'><b>压缩方法，如剪枝和量化，已被应用于减少不必要的数据并促进数据稀疏性</b></font>。在对稀疏矩阵进行计算时，无关的零会浪费存储和计算能力。稀疏矩阵乘法（SpMM）在深度学习中比稀疏-稀疏矩阵乘法（SpGeMM）更常用，因为SpMM仅涉及一个稀疏矩阵。相比之下，**SpGeMM需要权重和激活操作数都为稀疏，这在训练中更为困难。在SpGeMM中实现数据重用需要复杂的互连和高内存带宽，但其加速性能并没有超过SpMM**。因此，**SpMM是提高深度学习模型速度和能效的首选**。

然而，SpMM加速器的设计仍然是一项技术上具有挑战性的任务。**传统硬件平台**，例如CPU、GPU和张量处理单元（TPU），并没有从稀疏性中获得显著的优势，因为它们**主要专注于密集和结构化的计算**。这些硬件架构将零视为正常数据，导致利用率低和资源分配效率不高。考虑到数据中零的随机分布，稀疏性需要对非零（NZs）进行复杂而独特的处理，以消除无效计算。**多样化的工作负载要求计算单元以数据驱动的方式灵活执行。这就需要具有适当表示、灵活数据流和可配置架构的SpMM加速器，以有效地分配和计算NZs**。

大规模深度学习模型需要大量的计算和存储资源，而较小的模型更适合资源受限的环境，如移动设备和边缘设备。SpMM加速器的架构应具有足够的可扩展性，以适应各种应用中的不同计算规模。研究一种自动生成特定规模加速器的设计方法是重要的。由于实现特定加速器的复杂性，架构中的小改动可能会导致错误，并在时间和资源方面产生 substantial 成本。有必要使用更高级的硬件设计方法和自动化技术来开发特定的加速器，以提高开发效率、改善设计质量，并确保系统可靠性。

在我们之前的会议论文中，我们介绍了Sparkle，一种旨在加速DL领域中SpMM的加速器。此项工作扩展了对Sparkle架构的探索。我们工作的关键重点是开发一个Sparkle加速器的自动生成器，使其能够实现高效和可配置的实现。本文的主要贡献如下：
- 我们提出了一种**可配置的Sparkle架构**，基于之前的Sparkle工作，通过优化NZ加载过程和数据平衡调度方法。通过对Sparkle进行详细的修改，我们增强了架构自动生成加速器的能力，适用于更广泛的应用，提升了整体灵活性。
- 我们开发了一个**用于Sparkle加速器结构的自动生成器**。该生成器考虑了一种基于寄存器传输级（RTL）模板的生成方法，定义了可以通过实验优化的可配置参数。Sparkle生成器代码可以在GitHub平台上获得。
- 我们对生成器生成的多个加速器实例进行了实验。我们评估和分析了在不同配置下加速器的资源利用率和性能。这些实验确认了Sparkle架构的灵活性和SpMM加速性能。
- 我们**使用生成器生成一个类似于我们之前工作的Sparkle加速器实例**。通过评估和实验，我们证明了Sparkle的有效性。平均而言，Sparkle的性能比NVIDIA Orin NX GPU高出7.76倍，并且在计算效率方面，Sparkle比最先进的SpMM加速器SIGMA高出约10%到20%。


## 2-Background and Related Work

### 2.1-GEMM 和 SpMM

GEMM操作在深度学习中发挥着关键作用。它的规律计算模式促进了并行处理，并实现了高效的硬件重用和资源的平衡分配。在大规模GEMM计算的情况下，工作负载的划分是基于使用的处理单元（PE）数量。这种划分确保工作负载在所有PE之间均匀分配。它使得可以进行同步计算，直到所有PE完成任务。图1(a)描述了在二维PE阵列上的GEMM计算过程。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250216202623.png)

在处理SpMM时，使用与稠密矩阵相同的计算配置和过程可能导致处理单元（PEs）资源利用效率低下，如图1(b)所示。尽管计算时间保持不变，但PEs未能利用稀疏性进行加速，从而导致资源利用率低。为了平衡PEs之间的工作负载，提出了一种方法，**将有效计算（不包含零）分配给每个PE**，如图1(c)所示。然而，**这种灵活的数据映射破坏了固定和规律的数据流，这需要在复杂互连中增加开销，包括数据重用的分配和生成部分和的归约**。

### 2.2-SpMM 加速器

近年来，计算机体系结构领域对SpMM的定制加速器的兴趣日益增加。为了对SpMM加速器进行分类，图2展示了各种加速器使用的不同计算方法，包括**内积计算、外积计算和其他计算模式**。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250216204922.png)

> [! info] 内积方法
  **A矩阵按行 点乘 B矩阵每一列，获得结果矩阵中的每一个元素。**
  
<font color='red'><b>内积计算方法</b></font>在图2(a)中描述。该方法需要对输入矩阵进行连续遍历以计算输出结果。最近的加速器或软件库主要集中于这种方法。例如，TPU利用一个具有二维MAC网格的脉动阵列，在GEMM中表现出色，但在SpMM的性能上不够理想。为了克服这一限制，SIGMA提出了一种名为转发加法器网络（FAN）的新型归约树微架构，能够高效地利用所有PEs。类似地，Tensordash设计了一种硬件调度器作为GPU中张量核心的一个组件，能够预测并忽略零。此外，ALRESCHA和Tensaurus从多种计算模式中提取出一个通用范式，包括SpMM，并与特殊稀疏存储格式（CISS和BCSR）**共同设计硬件**。

> [! info] 外积方法
 **A矩阵按列 叉乘 B矩阵的每一行，获得结果矩阵的一部分矩阵**，最终将每部分结果矩阵相加，得到最终结果矩阵。

与内积计算相比，图2(b)中的<font color='red'><b>外积方法</b></font>使得可以根据NZ条目选择有效计算。这种方法消除了数据索引匹配问题，更有效地利用了输入数据。虽然外积最大化了输入重用，但由于大量高度内存绑定的部分矩阵，它在输出重用上受到限制。SpArch和Spaghetti旨在**将部分乘积保留在芯片上**，消除重新获取部分矩阵的需求。

> [! info] 逐行相乘
**A矩阵按行 点乘 B矩阵，获得结果矩阵中的每一行结果**，最终将每行堆叠，得到最终的结果矩阵。

MatRaptor引入了一种新的计算模式，称为<font color='red'><b>逐行乘积</b></font>。在这种模式下，左侧矩阵的一行与第二个矩阵的所有行相乘并累加，以获得输出矩阵的相应行，如图2(c)所示。中间结果是单行数据，减少了对片上缓存的需求。然而，需要根据非零项的数量灵活分配计算资源，以确保处理单元之间的负载平衡。结合逐行乘积，MatRaptor提出了一种新的稀疏存储格式（$C^2SR$）和一种加速器，实现了数据重用与片上内存需求之间的权衡。

> [! warning] 💡
**A矩阵 点乘 B矩阵的每一列，获得结果矩阵中的每一列** （多个SpMV操作），最终每列堆叠，得到最终的结果矩阵。


### 2.3-硬件自动生成器

加速器比通用计算平台在能效和性能上更具优势。硬件自动化技术已成为实例化具有不同参数的特定电路的强大工具。在深度学习领域，**自动化加速器方法主要关注深度神经网络模型的推理过程**。DASH和相关研究利用来自算法的一般运算符作为计算组件。这些组件有助于基于计算数据流构建特定的加速器结构。为了自动生成加速器，一些研究探讨了硬件设计过程中的算法优化。例如，T2S-Tensor缓解了数据冲突并优化了更适合硬件加速器的流水线布局。其他研究如<font color='red'><b>VTA</b></font>则不仅限于设计加速器。它提供了一个包括驱动程序、即时运行时和优化编译器栈的解决方案。该系统涵盖了从高级深度学习框架到实际硬件设计和实施的全过程。

> Apache TVM 是一个开源的机器学习编译器框架，**专注于优化深度学习模型的推理性能**，并实现跨硬件平台的高效部署。它支持从 PyTorch、TensorFlow、ONNX 等高层框架导入模型，通过 Relay IR 进行计算图优化，并利用 AutoTVM 和 AutoScheduler 自动搜索最佳计算调度，以提高推理速度。TVM 兼容多种硬件，包括 CPU（x86, ARM）、GPU（CUDA, ROCm）、FPGA、ASIC 等，能够针对不同设备生成高效的可执行代码，使其在边缘计算、服务器推理等场景下表现优异。
> 相比之下，**PyTorch 主要用于深度学习模型的训练**，提供灵活的动态计算图和自动微分机制，使其更适合研究和实验。虽然 PyTorch 通过 TorchScript 进行推理优化，但其硬件适配性和自动优化能力不及 TVM，尤其在资源受限的设备上表现有限。TVM 通过自动算子优化和跨平台编译，使模型在多种硬件上高效运行，而 PyTorch 则更适用于开发和实验。因此，一个常见的策略是使用 PyTorch 进行训练，然后借助 TVM 进行推理优化，以兼顾模型开发和部署的效率。

类似的框架还有：**TensorRT（NVIDIA）**、**BladeDISC（Alibaba DAMO）** 等。

## 3-Motivation

<font color='red'><b>内积计算仍然是大多数SpMM加速器的主要方法</b></font>。它通过并行处理最大化计算效率。**SIGMA是实现SpMM加速的代表之一**，取得了显著的成果。然而，它的位图存储格式在加载NZ数据时不太方便。此外，Benes拓扑的非阻塞网络节省了路由资源，但增加了路由计算的开销。因此，需要对稀疏格式、数据流调度和计算结构进行持续优化，以设计出具有更高性能的SpMM加速系统。

深度学习领域正在快速发展，**算法模型在大模型和小模型之间逐渐区分**。大规模深度学习模型具有更多层次，通常具有更好的泛化能力。由OpenAI团队开发的GPT-2语言模型，起始配置为15亿参数。经过发展，GPT-3的参数规模达到了1750亿。另一方面，轻量级模型通常参数较少，计算量较小。它们能在资源有限的硬件上进行高效计算，特别是在移动和边缘设备上。MobileNetV3和ShuffleNetV2是典型的轻量级模型，参数量约为1到9 MB。在不同的应用中，硬件结构需要以不同的规模实现。大规模硬件可以应用于大规模模型数据的处理，而小规模硬件则可以在资源受限的情况下加速。

硬件开发的工作是一项复杂的任务，涉及设计和测试。**自动化工具和技术**对提高开发效率和硬件可重用性至关重要。Chisel是一种提供高级抽象的硬件描述语言。虽然它在灵活性方面具有优势，但其生态系统相对不便，缺乏一些工具和库。**高层次综合是一种将高级语言转换为硬件描述语言的技术**。还有一些通过高层次综合实现和实验的SpMM加速器。它要求设计师熟悉高级编程技能和硬件知识。同时，电路设计过程依赖于编译或综合。设计师必须熟练匹配高级电路描述和综合结果，这使得在电路和周期级别实现准确实施变得困难。研究还指出，低延迟或低能耗的特定领域架构中数据流架构的高层次综合优化仍然是一个挑战和机遇。**因此，硬件自动生成技术在架构层面需要模块化和高度参数化。此外，还应确保生成的电路是透明和可控的。**

为了提高开发效率和质量，基于生成器的数字系统设计方法越来越受欢迎。硬件自动生成可以通过参数在架构层面配置系统结构。它自动实现RTL级硬件系统，同时仍保持相对透明和可控的低级电路表示。与Chisel和HLS等通用工具相比，代码生成器是针对特定加速器架构量身定制的。虽然这种专门化在优化和效率方面具有优势，但它限制了生成器的适用性，仅适用于涉及目标架构的情境。


## 4-Configurable Sparkle Architecture

### 4.1-架构概览

**基于<font color='red'><b>内积计算</b></font>的Sparkle整体架构**如图3所示，包含几个关键组件。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250216213609.png)

首先，矩阵提取器分别加载左矩阵A和第二矩阵B。稀疏矩阵A以指针位图格式存储。矩阵提取器中的格式解压单元检索NZ及其对应的索引。为确保负载和平衡计算，稀疏矩阵采用带双缓冲和先进先出（FIFO）缓冲的存储机制。稠密矩阵实现了多个FIFO结构。稀疏矩阵的数据被视为静态，并以直通方式流入PE，而稠密矩阵则通过广播方法进行流动。这种方法促进了空间和时间上的数据重用。

每组PE被分配用于计算一个静态块，前面使用一种交叉开关形式的分配网络。交叉开关负责根据相应的索引对操作数进行配对，确保正确匹配。PE从交叉开关接收操作数并开始乘法计算。然后使用分层汇聚网络来累积来自PE的一批结果。每组PE配备了一个FAN汇聚网络，以在组内累积结果。一个中间层被引入以管理相邻PE组之间的结果。最后，使用输入扩展的FAN执行PE组间的和的汇聚操作，生成一个或多个对应于块行的结果组。

在这项工作中，我们通过详细的架构优化扩展了Sparkle。**这些修改主要集中在优化指针-位图格式中的NZ加载过程，以及实现数据平衡调度方法**。这些增强显著促进了Sparkle架构的通用性和可扩展性，提高了其与不同矩阵维度、稀疏模式和数据类型的兼容性。

### 4.2-位图压缩稀疏格式

稀疏存储格式通常在稀疏矩阵乘法中被采用，以减少存储开销和数据访问。不同的压缩格式在数据稀疏度的不同情况下其效率也有所不同。我们关注四种广泛使用的稀疏存储格式，包括坐标格式（COO）、压缩稀疏行格式（CSR）、压缩稀疏列格式（CSC）和位图格式。

> **存储结构**
> 	1. **位图（bitmap）：** 采用一串二进制位（0 或 1）表示稀疏矩阵的非零元素分布：
> 		- **1** 表示该位置有非零元素。
> 		- **0** 表示该位置是零元素。
> 	2. **非零元素数组（values）：** 按照位图中 **1** 的顺序存储非零元素的值。
> 

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250216214215.png)

为了分析这些格式在不同稀疏度下的存储开销，我们检查一个1024 × 768的矩阵，假设每个数据元素占用16位。图4展示了每个稀疏度下的存储空间，虚线表示存储同样大小的稠密矩阵的开销。位图与其他压缩格式的比较显示，位图每个非零元素仅需要1位元数据，而其他格式至少需要$log_2d$位的额外开销，其中$d$代表矩阵维度。值得注意的是，在低稀疏度下，COO、CSR和CSC格式的元数据开销明显高于位图，并且随着维度规模的增加，这种开销也在增加。尽管位图在压缩方面表现良好，但在矩阵分区后访问跨越非连续行或列的数据时会出现挑战。在这种情况下，定位非零元素变得更加复杂。例如，读取第x行的非零元素需要遍历前 ($x− 1$) 行并记录非零元素的数量以确定地址。这一迭代过程会引入显著的延迟和潜在的计算瓶颈。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250216220428.png)

考虑到位图在任意稀疏级别的低存储成本，我们提出了点间位图压缩格式。这种格式旨在改进稀疏数据的解压缩过程。指针位图压缩格式由三个关键组件组成，如图 5 所示：
- **位图数组**：该数组使用单独的位（0或1）表示矩阵中对应数据是零还是非零。  
- **指针数组**：指针数组与位图数组的每一组相关联。它存储该组中第一个非零数据值的地址。指针的值表示在此地址之前的非零值的数量。  
- **非零值数组**：该数组包含来自原始矩阵的压缩非零值。

![image.png|center|400](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250216220834.png)
<center> <font face='华文宋体' size='4'> 前面是数组大小，后面是单个数组元素所占位数的多少 </font> </center>

表 1 比较了不同稀疏存储格式在 $m \times k$ 矩阵上的元数据开销，其中 $NNZ$ 表示非零（NZ）元素的数量。这里，我们假设数据的稀疏度 $s$ 满足 $NNZ = m \times k \times s$，其中 $0 \leq s \leq 1$。与位图（bitmap）相比，指针-位图（pointer-bitmap）格式需要额外的存储开销，即 $m \times \lceil \log_2 NNZ + 1 \rceil$ 位来存储指针。然而，当满足条件 $\log_2 k \times (1 - s) > 1$ 时，指针-位图格式的存储开销低于 COO、CSR 和 CSC 格式。我们的分析表明，指针-位图格式在稀疏度 $s$ 小于或等于 $80\%$ 且矩阵维度 $k$ 超过 32 时表现出优越的存储效率。这些稀疏度和维度范围通常出现在当前深度学习（DL）的剪枝模型中。因此，指针-位图格式继承了位图的存储开销稳定性，同时占用的存储空间比 COO、CSR 和 CSC 更少。此外，该格式简化了数据解压缩过程，使其更适用于硬件实现。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250216223130.png)

为了确保与不同维度和稀疏模式的矩阵兼容，本研究中加载NZ数据的过程更加通用。这一点在图6中展示。**片上内存被划分为三组：指针、位图和用于存储数据的NZ值**。NZ数据地址是通过指针和位图数据计算得出的。当数据稀疏时，属于同一组位图的NZ数据可能会存储在两行RAM中。因此，读取需要两个周期。为了提高一个周期内NZ数据的命中率，RAM宽度设置为翻倍，即2 × (位图RAM宽度) k × 数据宽度。然而，仍然存在一组NZ可能跨越两行RAM的可能。在这种情况下，管道需要暂停一个周期以正确读取NZ。

### 4.3-分块 Pipeline 负载均衡

压缩存储格式可以帮助减少存储开销，但由于片上存储能力有限，存储大维度矩阵仍然是一项挑战。适当的矩阵划分和处理至关重要，有时需要在片上和片外存储空间之间传输数据。考虑到计算单元的结构大小，以优化数据重用并最小化内存访问延迟，我们提出了一种<font color='red'><b>两级矩阵块划分方法</b></font>，利用粗粒度和细粒度，如图7所示。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250216222955.png)

矩阵 $A$ 和 $B$ 被划分为多个块，以克服片上存储的限制。矩阵 $A$ 沿行方向被划分为 $\lceil M / mr \rceil$ 个块，每个块包含 $mr \times K$ 个元素。矩阵 $B$ 沿列方向被划分为 $\lceil N / nc \rceil$ 个大小为 $K \times nc$ 的块。计算过程中，每个矩阵的两个块被用于生成一个 $mr \times nc$ 大小的矩阵，并将其累加到最终的输出矩阵 $C$ 中。通过双缓冲机制，可以有效隐藏片外存储访问延迟，实现连续的流水线计算过程。为了适应片上缓冲区的存储空间，$mr$ 和 $nc$ 的取值依赖于 $K$ 的大小。当 $K$ 增大时，$mr$ 和 $nc$ 需要相应减小。在粗粒度的块划分后，$mr$ 和 $nc$ 还会受到处理单元（PEs）大小的间接影响，并且必须是精细计算块大小的整数倍。

矩阵块还可以根据处理单元（PEs）的数量进一步划分为更小的子块。当一组 $k$ 个 PEs 可以同时执行 $k$ 次乘法时，矩阵 $A$ 中的一行数据被分配到一组 PEs 作为固定操作数。矩阵 $B$ 在每个时钟周期提供相应的列数据，并将其连续地流式传输到 PEs 中。更具体地说，矩阵 $A$ 被划分为 $p \times k$ 的块，与 PEs 结构大小相同。然后，矩阵 $B$ 中的一列 $k$ 个数据被广播到每个 PE 组，以进行 $p$ 组计算。通过将矩阵 $A$ 的不同行分配给不同的 PE 组，并共享矩阵 $B$ 的相同列数据，实现了空间和时间重用。

伪代码（如图 7 所示）展示了**通过两级块划分策略获得的数据流和计算方法**。对于稠密情况，$p \times k$ 数据块可以满足 $p$ 组 $k$ 规模的 PEs 需求，且 PEs 仅需要从块 0 读取数据。而在稀疏情况下，仅非零（NZ）数据可以输入到 PEs 进行计算，这会导致部分 PEs 处于未充分利用状态。为了提高 PEs 的利用率，当 $p \times k$ 大小的数据块无法填充所有 PEs 时，需要使用来自下一个块的数据。

假设数据的稀疏度为 $s$，则需要 $1/(1 - s)$ 个细粒度块来满足 PEs 的计算需求。例如，在 $50\%$ 稀疏度的情况下，需要使用 $A$ 矩阵的两个数据块作为固定操作数。同样的原则适用于矩阵 $B$ 的操作数，其中第二个数据块中的额外 $k$ 个数据需要进行匹配并执行乘加运算（MAC）。因此，如果 $b$ 个数据块被用于矩阵 $A$，那么 $b \times k$ 维度的数据需要从矩阵 $B$ 的相应维度中分配。为了实现 PEs 之间的计算负载均衡，需要根据数据块的数量兼容尽可能多的计算情况。计算最多四个数据块可以有效处理稀疏度低于 $75\%$ 的数据，而计算五个数据块可以有效处理稀疏度低于 $80\%$ 的数据。

**稀疏和稠密的操作数被分开处理，以实现负载均衡**。这与我们之前的工作不同，在本研究中，我们引入了更多的多路复用器（multiplexer）和复杂的选择条件，以增强调度的灵活性。对于稀疏矩阵，由于非零元素（NZs）的数量是不确定的，因此需要将 NZs 转换为固定规模的格式。这是通过设置 **buffer0** 和 **buffer1** 来缓存部分 NZs 实现的。**当前指针（cur_ptr）** 和 **前一个周期的指针（lst_ptr）** 被用于读取和写入缓存数据。如果 **lst_ptr** 位于 **buffer0**，而 **cur_ptr** 位于 **buffer1**，则表示 **buffer0** 中的数据已准备好输出。这种情况在图 8 的第一个示例中进行了描述。由于两个数据块的数据不能压缩到同一个 PE 组中，因此在完成细粒度块读取后，会执行零填充（padding with zeros）。

![image.png|center|400](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250216223450.png)

如图 8 的第二种情况所示，通常情况下，每个周期只输出一个缓冲区（buffer）。然而，如果当前数据被写入到两个缓冲区，并且同时执行了填充操作，则 **buffer0** 和 **buffer1** 必须同时输出，如图 8 的第五种和第六种情况所示。在这种情况下，会发送一个 **stall 信号**，以暂停所有操作，直到缓冲区的数据被正确写入。读取缓存数据后，NZs 会被存储到 FIFO（先进先出队列）中。然后，FIFO 中的 NZs 继续形成 PEs 组大小，并准备直接参与计算。

![image.png|center|500](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250216224439.png)

多个 FIFO 组被用于高效地处理同一列维度下的细粒度块中的稠密数据。流式传输的矩阵数据直接从 RAM 读取到 FIFO 中，并广播到 PEs。如图 9 所示，显示了四个 FIFO 在两个周期内的输出过程。每个 PE 组根据稀疏矩阵数据的细粒度块编号，从相应的 FIFO 读取稠密数据。在某些情况下，跨两个周期的 PE 组可能会共享相同数据块的稠密数据。在图 9 中，第一个周期的最后几个 PE 组（PEGs）和第二个周期的前几个 PE 组（PEGs）都需要来自 **FIFO2** 的数据。因此，设置了一个**备用 FIFO** 来同步存储数据，以便在下一批次使用。通过预取和持续流式传输数据，该方法有效地减轻了计算过程中等待数据的性能瓶颈。

### 4.4-分层可配置的部分输出归约

稀疏数据在计算过程中引入不确定性，影响了乘加（MAC）操作的固定大小。一个 $k$ 规模的基于树结构的归约操作需要 $O(\log_2 k)$ 个周期来计算 $k$ 个值的总和，但无法执行小于 $k$ 规模的可变归约操作。**MAERI** 提出了基于传统二叉树结构的多功能归约拓扑。它集成了一个**三输入加法器**，用于从父节点和兄弟节点聚合数据，以进行累加或向上传输，其中根节点需要比下层节点双倍的带宽。在 MAERI 的基础上，**SIGMA** 引入了一种更高效的拓扑结构，称为 **FAN**。

**FAN** 采用多路复用器和**二输入加法器**作为树结构中的计算节点，以替代 MAERI 中的三输入加法器，从而减少功耗和面积开销。FAN 的操作行为取决于数据分组，它通过配置控制加法器和旁路通道来执行多个累加操作。通过根据数据动态更改配置，FAN 网络支持**可变规模的归约**。

当 MAC 规模小于 FAN 时，FAN 的性能优于标准的**二叉加法树**。对于 $p$ 个 FAN 并行运行的情况，每个 FAN 规模为 $k$，它们可以对一个$p \times k$ 的细粒度块**进行归约，从而利用 FAN 的灵活归约能力。** 本研究基于 FAN 拓扑设计了一个**三级归约网络结构**，对应于矩阵的块划分策略。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250216224616.png)

假设每个 FAN 的大小为 $16$，且有 $16$ 个 FAN 组，每个细粒度块的大小为 $16 \times 16$，则归约网络的结构如图 10 所示。目标矩阵可以划分为相同大小的多个块，例如 A、B、C 和 D。
- **第一层归约网络**：该层包含 $16$ 个 FAN，与一个 PE 组相连。每个 PE 组同步执行乘法计算，并利用 FAN 结构对结果进行归约。FAN 具有**可配置性**，可以单独由数据驱动，完成细粒度块级别的归约操作。
- **第二层归约网络**：用于管理相邻 PE 组。当稀疏数据按照紧凑连续的数据流调度时，相同的 MAC 操作数据可能会被分配到两个 PE 组。此时，需要对这些相邻的 PE 组进行特殊的累加操作。如图 10 所示，**组 1 和组 2 共同计算相同的输出 $A_d$**。因此，该层引入了一系列**加法器和转发单元**，负责将相邻 FAN 之间的部分和相加，并将完整的和传递到下一周期。
- **第三层归约网络**：该层处理 PE 组级别的输出。每个 PE 组在 PE 级别归约多个属于**细粒度块**的输出。对于某些情况，FAN 生成的部分和需要合并在同一块行中。但仍然可能会出现部分和属于不同的块行的情况。最极端的情况是，每一行仅包含一个非零（NZ）元素，如最后一个 PE 组所示。因此，该层引入了**高级 FAN** 结构，以归约 PE 组的部分和。
该三级归约网络依赖于 FAN 拓扑，采用**向量化输入、计算和数据传输**，并扩展了 16 倍的计算规模。在 PE 组级别，FAN 可以像传统加法树一样**累加相同行的数据**，同时也能**对不同大小的块行输出进行可配置归约**。


## 5-Automated Sparkle Generator

### 5.1-自动化设计和生成的关键阶段

通常，在基于特定架构设计生成器时涉及几个关键步骤：
- (1) 参数：根据应用需求和约束识别设计参数，例如计算资源和存储容量；
- (2) 模板：构建和形成可重用的参数化模板代码或文件，并使用识别出的参数； 
- (3) 配置：确认特定应用的参数值，这会影响模板内模块的生成； 
- (4) 生成：使用配置好的参数化模板生成目标代码，这可以自动化以降低设计成本； 
- (5) 验证：严格的验证程序通过模拟和比较方法验证设计功能； 
- (6) 优化：迭代地 refining 和优化设计以实现更好的性能。

过程的前两个阶段至关重要，因为它们承担了主要设计工作，特别是参数化模板。根据架构仔细划分模块非常重要。然后，每个子模块在 RTL 级别实现，并应验证其功能。接下来的两个步骤涉及参数配置和加速器生成，通常可以自动化。通过将所需参数输入生成器，可以生成相应的加速器。最后两个阶段主要集中于调试和优化。验证过程对于确保生成的设计符合预期的操作行为至关重要。

### 5.2-在Sparkle架构中用于资源和性能的参数

Sparkle架构主要由处理单元（PEs）及其互连结构组成。这些是决定加速器性能和资源的关键要素。更多的PEs可以提高性能，但它们也需要更多的逻辑资源来处理复杂的互连。PE组的数量称为NUM_PEGS，而每组内的PE数量称为NUM_PES，这两个因素对Sparkle架构至关重要。增加计算单元的数量会提升理论计算效率。NUM_PEGS和NUM_PES的值直接影响场可编程门阵列（FPGA）基础设施内数字信号处理器（DSPs）的数量。这两个参数还影响分布和归约网络的规模，这与逻辑资源有关。除了NUM_PEGS和NUM_PES的主要参数，Sparkle架构还包含一系列其他影响资源和性能的参数。这些参数在表2中列出。其中一个参数是输入和输出数据的宽度，分别用IN_DATA_WIDTH和OUT_DATA_WIDTH表示。它们影响加速器内的存储资源。另一个参数POINTER_WIDTH决定指针在指针位图稀疏格式中的宽度，对RAM的空间分配有影响，并间接影响粗粒度块大小。MAX_N_DIM是配置结果处理模块缓冲深度的重要参数，决定稀疏数据计算的迭代次数和与密集矩阵相关的细粒度块的可接受容量。MAX_N_DIM的最优值取决于矩阵维度和RAM大小。PARA_BLOCKS表示从密集矩阵并行处理的块数量，直接影响资源和加速器性能。PARA_BLOCKS的低值导致高稀疏数据的PE利用率低，而高值则会导致在高稀疏情况下的兼容性差和资源浪费。

### 5.3-Sparkle 生成器框架

Sparkle加速器生成器的框架如图11所示。首先，设置参数。然后，将配置文件输入生成器。生成器在高层语言环境中运行，通常是Python，这比硬件描述语言提供了更大的灵活性和便利性。生成器实例化一个加速器，包括RTL设计模块和加速器的测试平台。在Vivado开发环境中，设计模块和测试平台可以整合一些IP核进行仿真。这些仿真结果可以与参考结果进行比较，以验证加速器。此外，可以通过资源和时序分析工具评估加速器的性能。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250216225510.png)

设计模块的生成是生成器的核心组件。参数文件可以视为一个接口，允许用户进行定义。它作用于Sparkle架构的RTL模板，以产生设计模块。Sparkle架构被划分为一些子模块。功能模块根据数据流适当地划分和连接。参数的值主要决定RTL模板中子模块的规模。例如，加载过程中使用的FIFO或RAM的宽度和深度由这些参数决定。在计算模块中，计算单元的数量或它们之间的互连也与某些特殊参数相关。基于RTL模板有两种模块生成方法。第一种方法是通过Python直接为受到参数影响的子模块生成一个新的RTL文件。第二种方法是通过对现有的RTL模块进行一些较少受参数影响的修改来间接生成。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250216225544.png)

验证是确保加速器功能和可靠性的关键步骤。生成器不仅提供测试平台模块，还提供生成预期结果的参考模型。Vivado环境中顶层模块的实际仿真输出被输入到记分板以进行验证。两个结果的匹配表明加速器正常工作。图12展示了验证过程。然而，在出现问题的情况下，进行对子模块的进一步验证是至关重要的。子模块的测试平台和参考模型有助于准确识别问题。因此，系统的验证方法确保了加速器的性能，并能高效而迅速地识别出任何错误。


## 6-Experimental Evaluation

[pdf](zotero://open-pdf/library/items/ZLTX6YKK?page=17)


## 7-Conclusion

Sparkle采用分层分区方法，在一种新型压缩稀疏格式下实现平衡的数据处理，并辅以灵活的归约网络进行累加。本研究扩展了Sparkle，设计了Sparkle生成器，能够自动生成不同结构参数的加速器实例。一个具有32 × 32计算单元的加速器实例展示了比最先进的SpMM加速器更优越的计算效率，并且表现出比GPU更高的性能。生成并评估了更多的加速器实例，展示了Sparkle生成器的便捷性和Sparkle架构的灵活性。
































