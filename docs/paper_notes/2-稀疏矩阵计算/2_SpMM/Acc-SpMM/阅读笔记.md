
[pdf](zotero://open-pdf/library/items/Q4PBY27X)

## 0-Abstract

通用稀疏矩阵-矩阵乘法（SpMM）是科学计算和深度学习中的一个基本内核。新型矩阵计算单元如张量核心（TCs）的出现为SpMM加速带来了更多机会。然而，为了充分释放硬件性能的潜力，需要系统化的优化。

本文提出了Acc-SpMM，这是一个高性能的SpMM库，针对TCs进行了多项优化，包括基于**数据亲和性的重排序**、**内存高效的压缩格式**、**高吞吐量流水线**和**自适应稀疏感知负载均衡**。

与各种NVIDIA GPU架构上不同基准矩阵的最先进SpMM内核相比，Acc-SpMM在RTX 4090上实现了平均2.52倍（最高可达5.11倍）的性能提升，在A800上实现了平均1.91倍（最高可达4.68倍）的性能提升，在H100上实现了平均1.58倍（最高可达3.60倍）的性能提升，与cuSPARSE相比。

## 1-Introduction

通用稀疏矩阵乘矩阵（SpMM）是众多科学计算、数据挖掘和深度学习中的一个基本且昂贵的计算内核，如线性代数求解器、线性代数计算、图分析、图神经网络（GNNs）以及大规模深度学习模型。因此，优化SpMM有潜力影响各种应用。

为了提高在GPU通用计算单元上进行稀疏矩阵乘法(SpMM)的性能，进行了广泛的相关研究，利用了各种优化技术，包括稀疏存储格式、重排序算法、并行策略和内存访问优化。随着专用GPU硬件，特别是Tensor Cores (TCs)的出现，加速SpMM计算的机会更加广阔。然而，仍然存在阻碍完全释放TC潜力以实现SpMM显著加速的障碍。
- ❶ 由于其不规则的内存访问模式，SpMM被分类为内存访问密集型计算。稀疏存储格式不仅影响内存占用，还影响内存访问效率。研究表明，<font color='red'><b>稀疏矩阵格式的创新有效提升了SpMM的性能</b></font>。现有的稀疏存储格式要么压缩效率低，要么开销较高。 
- ❷ TCs被设计为在密集数据上操作，这可能与SpMM中的稀疏数据操作不自然匹配。同时，**切块(tile)是重排序算法实现中的一种常见技术**。随着切块内非零元素数量的增加，数据局部性得到改善，这可以提升SpMM的计算性能。在当前的重排序算法中，平衡数据局部性和重排序开销是一个挑战。 
- ❸ 指令级并行性(ILP)通常用于在GPU上重叠内存访问和计算，这种技术被称为流水线，**依赖于独立内存和计算操作的排列**。根据目前的知识，现有的内存访问优化方法相对简单且效率不高，通常表现为低内存带宽或大量流水线气泡。

为了解决上述挑战，我们提出了一种新方法Acc-SpMM，专门针对TCs，结合了系统性的技术和优化。本文的主要贡献如下：
- 我们提出了一种新颖的稀疏矩阵压缩<font color='red'><b>存储格式</b></font>，该格式在内存使用效率上表现优异，并且解压缩时开销很小。
- 我们设计了一种基于数据亲和性的<font color='red'><b>重排序算法</b></font>，时间复杂度为O(nlogn)，能够高效处理细粒度切块数据，同时满足数据局部性要求。
- 我们构建了一个高度优化的<font color='red'><b>SpMM内核</b></font>，该内核结合了先进的流水线方法和多级内存访问技术。我们提出的SpMM内核减少了气泡的数量，并通过多重内存访问重叠计算，增强了内存和计算吞吐量。
- 我们还提出了一种基于有效性能模型的自适应稀疏感知<font color='red'><b>负载均衡方法</b></font>，考虑到写回成本，显著提高了内存吞吐量和计算吞吐量。
- 我们开发了Acc-SpMM，它加速了通用的SpMM，相较于最先进的基于TC的SpMM方法，在RTX 4090上实现了高达5.11倍的加速，在A800上实现了高达4.68倍的加速，在H100上实现了高达3.60倍的加速。

我们对大规模幂律图矩阵中的Acc-SpMM进行了广泛评估，包括**图神经网络中的稀疏矩阵**以及**SuiteSparse矩阵集合中的稀疏矩阵**。我们还将提出的Acc-SpMM与TCGNN-SpMM、Sputnik、SparseTIR、DTC-SpMM以及cuSPARSE库中广泛使用的SpMM内核进行了比较，测试平台为最新的RTX4090（Ada Lovelace）、A800（Ampere）和H100（Hopper）GPU。实验结果表明，Acc-SpMM在各种真实世界矩阵上相较于其他方法实现了显著的平均加速。

## 2-Background and Motivation

### 2.1-NVIDIA GPU and Tensor Core

**NVIDIA GPU是高度并行的处理器架构，具有内存层次结构和多种处理单元，包括多个流式多处理器（SM）、片上L2缓存和高带宽DRAM。所有SM共享对L2缓存和DRAM的访问。每个SM都有一组流式处理器（SP），每个SP都有自己的寄存器文件、调度器和执行单元。位于同一SM中的所有SP共享一个私有的L1缓存，其部分可以配置为共享内存。每个SP都有浮点单元（FPU）和张量核心单元（TCU）。FPU是传统的CUDA核心，每个周期可以执行融合乘加（FMA）操作。TCU每个周期可以执行一个矩阵乘法和累加（MMA）操作，提供比FPU更高的性能。内核由SM以两级线程层次结构执行。在SIMT模式下，warp内的线程（32线程）同时执行相同的指令。**

> [! info] 这一段句子都不错！

TC是专门的核心，自Volta架构以来，使得混合精度训练成为可能。随着每一代的升级，新的GPU架构涉及更多的数据类型进行计算，包括TF32和FP64。在Volta和Turing上，每个SM有8个TCU，而在Ampere、AdaLovelace和Hopper上，每个SM的TCU数量减少至4。尽管自Ampere架构以来每个SM的TCU数量有所减少，但每个周期的吞吐量增加了4倍。因此，每个SM的TCU整体性能翻了一番。

CUDA提供了具有MMA语义的warp级API。C++中的Warp-level Matrix Multiply Accumulate (WMMA)执行具有连续内存约束的稠密矩阵乘法。<font color='red'><b>PTX级的MMA可以执行具有非连续内存的矩阵乘法，为稀疏矩阵乘法提供了更多可能性</b></font>。它们都是warp级矩阵乘法。m16n16k8 API的形状将一个形状为(16 × 8)的左侧矩阵与一个形状为(8 × 16)的右侧矩阵相乘。

> [! warning] 还是得去看源码

### 2.2-张量核上SpMM的动机

**高内存消耗**：一个形状为 m × n 的矩阵通常被称为稀疏矩阵，如果它的非零元数量相对于 O (mn) 足够小。压缩存储格式在 SpMM 中被使用，如压缩稀疏行 (CSR) 格式和坐标 (COO) 格式，以便能够处理非常大规模的稀疏矩阵。**SpMM 是一个受内存限制的内核，因为稀疏数据结构带来的不规则数据访问模式**。因此，稠密矩阵的内存访问效率可能会导致在各种压缩存储格式之间的性能差异显著。

**低密度和低局部性的矩阵**：稀疏矩阵中的nnzs总是以行主序模式存储，比如CSR和COO格式。然而，由于同一行内的nnzs可能具有广泛的列索引，导致相应的稠密矩阵的内存访问模式变得不连续。此外，稀疏矩阵必须被划分为更小的块，在我们的论文中称为TC块，以适应在矩阵乘法操作中TCUs的特性。因此，<font color='red'><b>SpMM的计算性能与TC块的密度（TCU的利用率）和TC块的数据局部性（内存访问效率）成正比</b></font>。

低管道利用率（流水线控制）：TC的效率依赖于TC管道的利用率。低利用率主要由两个因素导致。首先，访问全局内存效率低下。Sparse Matrix-Vector Multiplication (SpMM)涉及加载稀疏矩阵A、密集矩阵B，并存储密集矩阵C。由于默认情况下，<font color='red'><b>来自全局内存的所有内存访问都必须经过L1和L2缓存，导致缓存命中率可能降低，频繁出现缓存未命中</b></font>。其次，与数据访问请求相关的开销较高。默认情况下，<font color='red'><b>在内存访问操作期间，TCUs处于闲置状态</b></font>。尽管已经采用多种缓冲方法来重叠计算和内存访问，但在管道利用率方面仍有进一步改进的机会。

## 3-Acc-SpMM

### 3.1-概述

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250317183840.png)
<center> <font face='华文宋体' size='4'> 图 1. Acc-SpMM 的概述 </font> </center>

Acc-SpMM由四个关键组成部分构成，如图1所示：
- ❶ 基于数据亲和性的重新排序将稀疏矩阵拆分并重新排列为矩阵块，以增强TC块的密度并改善数据局部性。
- ❷ 内存高效压缩格式转换将稀疏矩阵块转换为我们提出的**BitTCF**格式，以提高内存访问效率。
- ❸ 高吞吐量流水线执行高效的运行时内核。通过高效的内存访问，高吞吐量流水线被充分利用以重叠内存访问，从而提高TC流水线利用率。
- ❹ 自适应稀疏感知负载均衡根据矩阵的稀疏性动态调整分配给每个线程块(TB)的TC块数量。

### 3.2-Data-affinity-based重新排序

TC自然适合稠密矩阵的计算，因此确保每个TC块内的数据密度对于实现最佳的TCU计算效率至关重要。同时，<font color='red'><b>提高数据局部性对增强GPU上层次缓存的利用至关重要</b></font>。基于上述内容并受到模块化重排算法的启发，我们提出了一种新颖的重排算法，以实现高数据密度和局部性。<font color='blue'><b>模块化广泛用于评估图中社区结构的质量</b></font>。$ΔQ$在公式中定义，其中$ΔQ$是模块化的改进，$m$是图中的总边数，$A_{ij}$是节点$i$和节点$j$之间的权重，$k_i$是节点$i$的度，$s_i$是涉及节点$i$的社区，$δ_{si,sj}$是克罗内克尔 delta 函数，当$s_i = s_j$时等于1，否则为0。图是通过使用稀疏矩阵作为邻接矩阵构建的，其中图中的每个节点对应于一行或一列的索引。如果矩阵中存在$nnz$，则对应节点之间的权重通常设置为1，否则设置为0。在树状图构建后，图被划分为不同的社区。通过最小化跨社区的边数并将具有相似边的顶点排列得更近，基于数据相关性的重排有效地使$nnzs$的排列在局部更紧凑。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409120143.png)

算法1概述了基于数据亲和性的重排序。给定一个稀疏矩阵，算法输出一个重新排序的稀疏矩阵。首先，我们通过粗化和细化从稀疏矩阵生成树形图（第1-8行）。然后，我们应用排序生成（第9-27行），将树形图转换为一个新的有序图。

**树状图构建**。树状图构建可以提高每个社区的密度。我们首先按度数升序选择源顶点v（第2行），然后找到源顶点v的邻居顶点u，使最大$Δ Q$（第4行），最后如果模块度改善，则将源顶点v与邻居顶点u合并（第5-7行）。

$$

\Delta Q = \frac{1}{2m} \sum_{i,j} \left( A_{ij} - \frac{k_i k_j}{2m} \right) \delta_{s_i, s_j}

$$

**排序生成**。排序生成可以改善每个TC块的数据局部性，并优化GPU上层次缓存的利用率。在构建完树状图后，进行深度优先搜索（DFS），首个访问的叶节点被选为源顶点v。接着生成顶点v的排序，然后从与v共享最大数量公共邻居的叶节点中选择一个顶点u。然后生成顶点u的排序，并将u设置为新的源顶点。这个过程会迭代进行，直到所有顶点都被访问。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409120449.png)

基于数据亲和性的重排序示意图如图2所示。在图2 (a)中构建原始图的树状图后，我们在图2 (b)中获得了两个社区。然后我们执行类似深度优先搜索的操作，从第一个社区选取源顶点5，并将其新的顺序索引设置为0。我们根据深度优先搜索遍历树状图中所有未访问的叶节点，尝试找到与源顶点5拥有最多公共邻居的顶点。顶点2、7和4都与顶点5有一个公共邻居顶点0，因此我们根据深度优先搜索的顺序选择顶点2作为下一个访问节点。随后，我们为顶点2分配顺序1，并将顶点2指定为源顶点。然后我们继续执行深度优先搜索，直到树状图中的所有节点都被访问。重新排序后，我们在图2 (c)中得到了重排序的图。我们根据重排序的图应用SpMM。与图2 (a)中的原始图所对应的邻接矩阵图2 (d)相比，通过重排序，我们在图2 (c)的重排序图中获得了更好的数据密度和局部性，表现为图2 (e)。

### 3.3-高效内存压缩格式

我们提出了一种基于ME-TCF的内存高效压缩格式，称为BitTCF，如图3所示。为方便说明，图3中展示了一个$4 × 4$的tile以演示稀疏矩阵A的划分。然而，我们实际上选择了$8 × 8$的tile形状。BitTCF利用四个数组来表示一个稀疏矩阵。
- ❶ `RowWindowOffset`表示每个RowWindow中起始TC块的偏移量。在8 × 8的TC块情况下，`RowWindowOffset`包含⌈M/8⌉ + 1个元素。
- ❷ `TCOffset`保存每个TC块中起始nnz的偏移量，包含NumTcBlock + 1个元素。
- ❸ `SparseAToB`保存TC块中每个nnz的原始列索引，包含NumTcBlock × 8个元素。
- ❹ 我们使用<font color='red'><b>一个uint64整数表示每个TC块中每个nnz的本地位置</b></font>，称为`TCLocalBit`，其中1表示nnz，0表示零元素。

总的来说，BitTCF需要$(⌈M/8⌉ + NumTCBlock × 11 + 2)×4$字节来表示一个稀疏矩阵，其中M是稀疏矩阵A的行数，NumTCBlock是矩阵A中的TC块数量。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409120843.png)

在ME-TCF中，TCLocalId的大小对应于nnzs的数量，表示每个nnz的原始位置。TCLocalId使用int8存储每个TC块中nnz的位置，在一个8 × 8的TC块中，至少有8个nnz，最多有64个nnz，这将需要至少64位，最多512位来存储TC块中每个nnz的位置，采用ME-TCF格式。

我们设计了TCLocalBit，使用uint64整数来存储TC块中所有nnz的位置。对于每个TC块，我们只需要64位来存储所有nnz的位置。为了更好地使用uint64进行压缩，我们将稀疏矩阵A划分为8 × 8的tiles。与ME-TCF相比，BitTCF在nnzs数量增加时可以有效节省内存。

同时，为了利用GPU的多级内存和缓存结构，我们为**BitTCF设计了一种内存访问模式**。在该模式中，<font color='red'><b>SparseAToB从全局内存加载到共享内存以便重用，而RowWindowOffset、TCOffset和TCLocalBit则直接从全局内存加载到寄存器中</b></font>。这种方法不会影响经常使用的稠密矩阵的缓存命中率。压缩格式和内存访问模式可以提高内存访问效率。此外，我们利用C++位操作实现解压缩。在解压缩阶段，我们使用按位操作来解压缩nnz位置的偏移量。我们使用两个warp进行解码，线程ID范围从0到63。每个线程负责解压缩特定位置的元素。每个线程检查当前的位置是否为nnz。如果是，则使用`__popcll` API计算该位置nnz的偏移量。否则，它直接将零写入共享内存。解压缩的开销很小。此外，解压缩可以与内存访问重叠。


### 3.4 高通量流水线

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409123102.png)

算法2概述了一种集成高吞吐量气泡双缓冲流水线的运行时内核。给定稀疏矩阵A、SparseAToB和密集矩阵B，该算法在应用SpMM后输出密集矩阵C。<font color='red'><b>运行时内核安排多个数组的内存位置（共享内存或寄存器）以最大化缓存命中率</b></font>。双缓冲流水线安排多个数组的内存访问模式以最大化TCU利用率。运行时内核。运行时内核的数据移动如图4所示。

![image.png|center|500](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409122953.png)

**运行时内核**。运行时内核的数据移动如图4所示。运行时内核：
- 将稀疏矩阵A的块从全局内存加载到共享内存（第21行）
- 将数组SparseAToB从全局内存加载到共享内存（第24行）
- 将稠密矩阵B的块直接从全局内存加载到寄存器（第21行），并在对稀疏矩阵A的每个RowWindow计算后，将稠密矩阵C的块从寄存器存储到全局内存（第37行）。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409122929.png)

如表1所示，我们可以**使用PTX级别的指令来控制缓存策略以提高缓存命中率**。由于异步拷贝指令的限制，我们将稀疏矩阵A缓存到L1和L2缓存中，使用.ca指令。由于我们需要多次访问稠密矩阵B，我们同样将其缓存到L1和L2缓存中，以提高稠密B矩阵的缓存命中率，使用.ca指令。在mma计算后，我们不需要再次重新加载稠密矩阵C，因此我们选择不在L2缓存中缓存结果稠密矩阵C，使用.wt指令。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409123308.png)

**最少气泡双缓冲流水线**。为了减少流水线气泡，我们提出了一个高吞吐量的流水线，如图5 (b)所示，它重叠了各种内存访问和TCU MMA操作，从而进一步提升了TC流水线的利用率。对于图5 (a)所示的DTC流水线，由于mma计算依赖于稠密矩阵B，因此在稠密矩阵B的GToReg之后存在隐式同步。因此，当稠密矩阵B的tiles被加载时，TCUs处于闲置状态，造成了显著的流水线气泡。这阻止了内存带宽的充分利用，导致TCU流水线利用率较低。我们提出的流水线方法预取了下一次mma计算所需的稠密矩阵B tiles，**进一步重叠数据加载与计算以减少流水线气泡**。为了应用高吞吐量流水线，我们在共享内存中设计了用于存储稀疏矩阵A tiles和SparseAToB数组的双缓冲区。我们还使用cp.async来异步处理计算和数据加载，最大化重叠。在图5中，GAP显示了我们提出的流水线和DTC流水线在两个TCMMA操作后的执行时间差异。我们提出的流水线显著提高了TCU流水线的利用率。

### 3.5 自适应稀疏负载均衡

由于不同稀疏矩阵的稀疏性各异，每个RowWindow中的TC块数量不均衡，这对SpMM计算的效率产生了显著影响。我们提出了一种自适应稀疏感知负载均衡方法，该方法根据各种稀疏特征动态调整每个TB的TC块，如图6所示。在没有负载均衡的情况下，**每个TB处理单个RowWindow的所有TC块**，并且只将乘法结果写回全局内存一次。相比之下，使用负载均衡时，引入了跨行写回，因为一个TB可能会计算来自多个RowWindow的TC块。此外，连接RowWindows会导致对稠密矩阵B和稠密矩阵C的额外内存访问。
![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409123920.png)
DTC-SpMM中的负载均衡方法如图6（a）所示。第二个TB（TB1）有一个TC块，导致一个加载时间、一个计算时间和一个写入时间。考虑到每个TB的数据写回时间并实施新的负载均衡方法后，我们设计了一种方法来**重新排列每个TB的TC块**，如图6（b）所示。在这种方法中，第二个TB填充了来自不同RowWindows的两个TC块，并且图6（a）中的第四个RowWindow被分割到两个TB中，从而导致额外的写回开销。由于负载均衡引入了额外的开销，决定是否应用负载均衡至关重要。对于一些稀疏矩阵，其计算已经很好地平衡，则不再需要进一步调整。**只有不均衡的矩阵需要进行调整以提高性能**。

我们定义了一个度量指标 $IBD$（如公式 (3) 所示），用于衡量稀疏矩阵中的负载不平衡程度。其中，$TCBlockPerRowWindow$ 表示每个 RowWindow 中的 TC block 数量，$AvgTCBlock$ 表示矩阵中所有 RowWindow 的 TC block 平均数，$NumOfRowWindow$ 表示 RowWindow 的总数。我们设定了 $IBD$ 的阈值，当 $IBD > 8$ 时，我们认为该矩阵负载高度不平衡，因此需要应用负载均衡方法。
$$

IBD = \frac{\sum |TCBlockPerRowWindow - AvgTCBlock|}{NumOfRowWindow}

\tag{3}

$$
综合考虑硬件特性和矩阵稀疏性，我们提出了一种自适应、面向稀疏性的负载均衡方法，该方法基于流水线评估动态地重新分配 TC block 到不同的线程块（TB）之间。我们构建了一个性能模型，如公式 (4) 所示，支持重新分配每个 TB 的负载、自动评估流水线时间并优化负载分布。

在公式 (4) 中，$T$ 表示使用我们提出的负载均衡方法时每个 TB 的 SpMM 计算时间：
- $LoadDenseTime$ 表示加载稠密矩阵 $B$ 的 tile 所需的时间，
- $MMATime$ 表示 TC 单元执行矩阵乘法（mma）的计算时间，
- $WBTime$ 表示将计算结果写回矩阵 $C$ 的时间。

此外，定义说明如下：
- $FeatureDim$ 表示稠密矩阵 $B$ 的维度；
- $TcBlockPerTB$ 表示每个线程块中的 TC block 数量；
- $Bandwidth$ 表示理论内存带宽；
- $FLOPS$ 表示理论的 TF32 浮点运算性能；
- $M$ 表示稀疏矩阵 $A$ 的行数；
- $K$ 表示稀疏矩阵 $A$ 的列数；
- $N$ 表示稠密矩阵 $B$ 的列数。

在将左右矩阵调换后，我们将维度参数设定为 $M = 8$，$K = 8$，$N = 16$。


公式 (4) 定义了每个线程块（TB）在执行 SpMM 时的总计算时间 $T$，它由三个阶段组成：加载稠密矩阵 $B$、执行矩阵乘法（MMA）、写回结果 $C$，表示为：
$$

T = LoadDenseTime + MMATime + WBTime

$$
展开为：
$$

T = \frac{K \times FeatureDim \times TcBlockPerTB}{Bandwidth}

+ \frac{M \times (2 \times K - 1) \times FeatureDim}{FLOPS}
    
+ \frac{K \times FeatureDim \times TcBlockPerTB}{Bandwidth}
    
    \tag{4}
    
    $$
其中：
- $K$：稀疏矩阵 $A$ 的列数；
- $M$：稀疏矩阵 $A$ 的行数；
- $FeatureDim$：稠密矩阵 $B$ 的维度；
- $TcBlockPerTB$：每个线程块中的 TC block 数量；
- $Bandwidth$：理论内存带宽；
- $FLOPS$：TCU 的理论 TF32 浮点计算性能。

根据公式 (4)，每个线程块中分配的 TC block 数量将被重新调整，以保证所有线程块的计算时间尽可能均衡。为实现这一重新分配，我们设置了每个线程块最多分配 32 个 TC block 的上限。


## 4-Experiments

### 4.1-实验设置

我们比较了Acc-SpMM与当前最先进的SpMM内核在TC和CUDA核心上的整体性能，包括TCGNN-SpMM、DTC-SpMM、SparseTIR、Sputnik和cuSPARSE库中广泛使用的SpMM内核。所有实验结果是在配置了CUDA版本11.8或更高版本的环境下获得的。除了表2中列出的10个代表性的真实矩阵外，还评估了来自SuiteSparse矩阵集合的总共414个矩阵，这些矩阵与DTC-SpMM中使用的矩阵一致。根据AvgL，我们将数据集分为两种类型：类型1矩阵，其AvgL较小；类型2矩阵，其AvgL较大。所有实验都在当前主流NVIDIA GPU架构上进行，具体细节见表3，包括RTX 4090（Ada Lovelace）、A800（Ampere）80GB PCIe和H100（Hopper）80GB SXM。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409142642.png)
我们使用张量浮点32（TF32）精度测量了128、256和512列的稠密矩阵B的平均性能。此外，我们还获得了来自SuiteSparse矩阵集合的414个矩阵在不同架构上的几何平均结果，并提供了这些矩阵的详细GFLOPS结果。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409142746.png)

### 4.2-全面评估

以在CUDA核心上使用cuSPARSE作为基准，我们提出的Acc-SpMM在TCs和CUDA核心上对表2中列出的10个矩阵以及SuiteSparse集合中的414个矩阵的性能都有显著提升。

![image.png|center|1200](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409143457.png)

在各种NVIDIA GPU架构上，我们提出的AccSpMM始终保持良好的性能。值得注意的是，在H100上，cuSPARSE-SpMM的性能已大幅提升，但Acc-SpMM仍实现了显著的加速。所有实验均在表3列出的GPU上进行，cuSPARSE作为基线，表示为灰色虚线。图7、图8和图9分别展示了在RTX 4090、A800和H100上的性能比较。Acc-SpMM在10个具有代表性的实际稀疏矩阵上实现了显著加速，RTX 4090上的平均加速为2.52×，A800上为1.91×，H100上为1.58×。对于类型-2矩阵，加速效果更为明显，在RTX 4090上达到5.11×，在A800上为4.68×，在H100上为3.60×。

在图7中，Acc-SpMM在所有矩阵的TCs和CUDA Cores上都优于其他方法。与SpMM kernels的CUDA Cores相比，Acc-SpMM利用了高效的内存压缩格式、重排序、流水线和负载均衡等技术，以充分利用TCUs，这些技术能在每个周期执行更多的FMA操作，从而使得性能优于CUDA Core实现，如Sputnik、SparseTIR和cuSPARSE。虽然TCGNN优化了压缩存储格式以更好地利用TCs，但仍然引入了显著的冗余。DTC-SpMM通过重排序提高了局部性，同时实施了流水线和负载均衡，但仍有进一步优化的机会。Acc-SpMM有效解决了这些局限性，与TCGNN和DTC-SpMM相比，进一步提高了性能。图8展示了在A800上的性能。值得注意的是，Sputnik在Reddit数据集上表现优越，这是由于其nnzs数量较高。通过有效管理不连续的内存访问，Sputnik显著减少了内存访问开销。此优化在A800上尤其有利，A800是原始架构的精简版本，导致性能明显提高。SpMM操作是一个受限于内存的操作。H100 SXM利用了HBM3内存，显著提升了内存带宽并加速了内存访问。此外，H100硬件架构针对稀疏性进行了优化，使动态稀疏性能够得到充分利用。因此，cuSPARSE在H100上显示了显著的性能提升，如图9所示。然而，由于在重新排序、压缩格式、流水线和负载平衡方面的优化，Acc-SpMM 保持了卓越的性能。

### 4.3-详细评测

#### 4.3.1-基于数据亲和性的重排序评估

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409143714.png)
与最先进的重排算法相比，我们提出的重排算法显著提高了TC块内的数据密度，如图10所示。我们将所提出的重排算法与六种常用算法进行比较：METIS、Louvain、SGT、LSH64、DTC-LSH和Rabbit Order。我们定义MeanNNZTC为每个TC块中的nnzs平均数，作为评估重排算法有效性的指标。我们提出的重排算法在所有评估的稀疏矩阵中实现了最高的MeanNNZTC。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409143839.png)

与DTC-LSH和Rabbit Order相比，我们的算法在所有10个矩阵以及SuiteSparse集合中的414个矩阵上有效提高了MeanNNZTC，平均增益分别为1.28倍和1.10倍。此外，它超越了METIS、SGT、Louvain和LSH64所取得的改进。如图10所示，随着AvgL的增加，改进变得更加显著。图11(a)展示了在大多数数据集上L1缓存命中率的显著提升，峰值增加达到17.56%。同样，如图11(b)所示，我们的算法显著提升了L2缓存性能，也实现了峰值增加4.93%。需要注意的是，为了与DTC-SPMM保持一致，我们只对稀疏矩阵进行重新排序，而不对密集矩阵进行相应的行重新排序。如果密集矩阵进行行重新排序，整体性能和缓存命中率将进一步提高。

#### 4.3.2-内存高效压缩格式的评估

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409144029.png)

图12展示了TCF、CSR格式、ME-TCF和BitTCF之间的压缩比比较。基于TCF的内存使用，TCGNN中使用的压缩格式BitTCF实现了最高的压缩比，平均比CSR格式高16.12%，比ME-TCF高4.21%。除了提高压缩比外，BitTCF还减少了格式转换开销，相比于ME-TCF在从CSR格式转换时降低了15%。存储格式开销的减少主要依赖于两个因素：使用TCLocalBit数组表示每个TC块内非零元素的位置，以及对原始矩阵中非零元素的重新排序，使其排列更加紧凑，从而显著减少生成的TC块数量。TCF存储有关零元素和非零元素的信息，而CSR只存储非零元素。ME-TCF通过使用int8存储每个非零元素的相对位置进一步压缩了重新排序的矩阵。在重新排序的矩阵基础上，BitTCF利用固定的uint64存储每个TC块内非零元素的位置，进一步提高了稀疏矩阵存储的压缩效果，相比于ME-TCF。**我们选择8×8的tile作为矩阵A的块大小，这也方便地使用uint64编码非零元素的位置**。此外，从CSR转换为BitTCF只需几秒钟。对于迭代应用，这种转换的开销很小，可以摊销。

#### 4.3.3-高通量流水线评估

在图13中，展示了我们提出的流水线与DTC-流水线之间的性能和加速比比较。紫色线表示DTC-流水线的Gflops，绿色线表示Acc-流水线的Gflops。与专门针对TC优化的DTC-流水线相比，我们提出的流水线实现了更高的性能。以DTC-流水线为基线，我们提出的流水线在所有10个稀疏矩阵上都显示出加速效果。当AvgL增大时，改进变得更加明显，对于类型1和类型2矩阵的平均加速比分别为1.06倍和1.16倍。由于一个TB在类型2矩阵中处理更多的TC块，因此它相比类型1矩阵在流水线气泡方面获得了更大的减少。结果是，流水线在类型2矩阵上的表现优于类型1矩阵。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409144305.png)

#### 4.3.4-自适应负载平衡评估

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409144635.png)

图14显示了在不平衡矩阵上有无负载均衡的吞吐量比较。对于类型1矩阵，每个TB通常只处理一个或两个TC块，这导致所有TB的工作负载相对平衡，如公式（3）所示。因此，类型1矩阵不需要负载均衡，我们的负载均衡实验主要集中在类型2矩阵上。蓝色矩形表示没有负载均衡的计算吞吐量，橙色矩形表示采用我们提出的负载均衡后的计算吞吐量。紫色线表示没有负载均衡的内存吞吐量，而绿色线则表示采用我们提出的负载均衡后的内存吞吐量。从图14可以得出结论，我们提出的负载均衡有效地提高了计算吞吐量和内存吞吐量。



#### 4.3.5-消融实验

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20250409150331.png)

为了评估不同优化技术对Acc-SpMM整体性能的有效性，我们以没有负载平衡的DTC-SpMM作为基线，在H100上进行消融实验，稠密矩阵B的列数为128，如图15所示。BitTCF不仅通过压缩提高了存储效率，还通过最小化从全局内存到寄存器的数据传输量来提升计算性能。基于数据亲和性的重排序增加了MeanNNZTC，从而减少稀疏矩阵中的TC块数量，有效提高了性能。然而，对于蛋白质数据集，我们的重排序方法提高了MeanNNZTC，但降低了L1和L2缓存的命中率，如图11所示。类似地，对于FY-RSR数据集，我们的方法降低了L1缓存命中率。因此，应用重排序方法后，这两个数据集的性能稍有下降。缓存策略有效利用了L1和L2缓存，提高了缓存命中率，从而提升了性能。流水线利用GPU的高内存访问带宽，能够在mma计算与各类内存访问之间实现多个重叠，包括提取矩阵A的块和索引。对于类型1矩阵，流水线带来的性能提升相对于类型2矩阵不够明显。这主要是因为在类型2矩阵中，每个TB被分配了更多的TC块进行计算，从而在迭代过程中显著减少了流水线气泡，导致更显著的性能提升。负载平衡方法确保TB之间的计算时间更加平衡，并创新性地将写回时间纳入流水线性能模型，显著提高了模型的准确性，改善了负载平衡的有效性。


## 5-Related Work

给定一个m-by-k的稀疏矩阵A和一个k-by-n的密集矩阵B，SpMM计算A乘B并得到一个m-by-n的密集矩阵C。随着GPU架构的进步和技术更新，出现了多种优化SpMM在GPU上的方法，针对CUDA核心和张量核心。广泛使用的NVIDIA cuSPARSE库实现了多个稀疏矩阵的基本线性代数子程序，为CUDA核心和张量核心提供高性能的SpMM内核。Hone等人提出了RS-SpMM和ASpT，使用自适应分块将稀疏矩阵划分为密集块和稀疏块，从而实现共享内存中密集矩阵的重用。Jiang等人基于ASpT提出了一种行重排序方法，能够为稀疏矩阵获取更多密集矩阵块。Huang等人提出了GE-SpMM，通过重用稀疏矩阵来提高全局数据访问的效率。Gale等人提出了Sputnik，引入了一种一维分块方案和反向偏移内存对齐，以提高稀疏矩阵的内存访问效率。Pang等人提出了行分解方法RoDe，将稀疏矩阵分成负载平衡的块，并利用子块流水线提高内存访问效率。近年来，人们对加速TC上的SpMM越来越关注。cuSPARSELt是一个支持稀疏张量核心的库，对结构稀疏性施加严格限制，特别遵循2:4模式。Chen等人提出了VectorSparse，一种Sputnik的演变，利用一维稀疏元素组来增强数据局部性。Castro等人提出了CLASP，采用新列向量剪枝感知的SpMM实现来改善数据局部性。Chen等人提出了基于张量核心的一维八元组分块方法以提高内存访问效率。Li等人提出了Magicube，采用新颖的稀疏压缩格式SR-BCRS，以低精度整数加速SpMM。Wang等人提出了TC-GNN，这是第一个在GPU的TC上加速GNN的框架，具有优化的一般SpMM内核，通过有效的结构操作利用图稀疏模式对稀疏矩阵进行分块。

## 6-Conclusion and Future Work

在本文中，我们提出了Acc-SpMM，这是一个基于系统优化的高性能SpMM库。这些优化涉及高层算法、低层内存访问优化和指令流水线。通过对10个代表性的大规模幂律图矩阵在GNNs中的性能评估以及414个来自Suite Sparse Collection的稀疏矩阵在最近主流NVIDIA GPU架构上的评估，Acc-SpMM在RTX 4090上实现了2.52倍的平均加速（最高可达5.11倍），在A800上实现了1.91倍的平均加速（最高可达4.68倍），在H100（SXM）上实现了1.58倍的平均加速（最高可达3.60倍），与cuSPARSE相比。未来，我们计划重新排列稀疏矩阵的列，同时重新排列稠密矩阵的行，从而进一步提高缓存命中率。我们还将优化重新排序算法的实现以减少开销。此外，我们将把SpMM操作符集成到DGL中，以便在GNNs中进行实际使用，使我们的方法更加系统，并提高在端到端应用中的可行性。











