<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-paper_notes/TCU相关/RT-GNN/阅读笔记" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">阅读笔记 | BUAAer-xing Blog</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://buaaer-xing.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://buaaer-xing.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://buaaer-xing.github.io/docs/paper_notes/TCU相关/RT-GNN/阅读笔记"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="阅读笔记 | BUAAer-xing Blog"><meta data-rh="true" name="description" content="pdf"><meta data-rh="true" property="og:description" content="pdf"><link data-rh="true" rel="icon" href="/img/icon.png"><link data-rh="true" rel="canonical" href="https://buaaer-xing.github.io/docs/paper_notes/TCU相关/RT-GNN/阅读笔记"><link data-rh="true" rel="alternate" href="https://buaaer-xing.github.io/docs/paper_notes/TCU相关/RT-GNN/阅读笔记" hreflang="en"><link data-rh="true" rel="alternate" href="https://buaaer-xing.github.io/docs/paper_notes/TCU相关/RT-GNN/阅读笔记" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://YOUR_APP_ID-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"阅读笔记","item":"https://buaaer-xing.github.io/docs/paper_notes/TCU相关/RT-GNN/阅读笔记"}]}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="BUAAer-xing Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="BUAAer-xing Blog Atom Feed">




<link rel="search" type="application/opensearchdescription+xml" title="BUAAer-xing Blog" href="/opensearch.xml">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.98cc3bd4.css">
<script src="/assets/js/runtime~main.b1a1434e.js" defer="defer"></script>
<script src="/assets/js/main.8b44110d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/icon.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/icon.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Home</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/paper_notes_intro">论文笔记</a><a class="navbar__item navbar__link" href="/docs/week_report/week_report_intro">周报汇总</a><a class="navbar__item navbar__link" href="/docs/blogs_intro">个人博客</a><a class="navbar__item navbar__link" href="/docs/my_papers_intro">发表论文</a><a class="navbar__item navbar__link" href="/blog">相关内容</a><a class="navbar__item navbar__link" href="/resume">个人简历</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/BUAAer-xing/BUAAer-xing.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/paper_notes_intro">目录</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/paper_notes/HYCOM/HYCOM概述">HYCOM</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/paper_notes/稀疏矩阵计算/SpMV/AlphaSparse/论文原件">稀疏矩阵计算</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/docs/paper_notes/TCU相关/RT-GNN/阅读笔记">TCU相关</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/docs/paper_notes/TCU相关/RT-GNN/阅读笔记">RT-GNN</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/paper_notes/TCU相关/RT-GNN/阅读笔记">阅读笔记</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/paper_notes/未分类/intro">未分类</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/paper_notes/z-模版/论文原件">z-模版</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">TCU相关</span></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">RT-GNN</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">阅读笔记</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>阅读笔记</h1></header><p><a href="zotero://open-pdf/library/items/QXB4XSIT" target="_blank" rel="noopener noreferrer">pdf</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="0-abstract">0-Abstract<a href="#0-abstract" class="hash-link" aria-label="Direct link to 0-Abstract" title="Direct link to 0-Abstract">​</a></h2>
<p>图神经网络（Graph Neural Networks, GNNs）在各种基于图的学习任务中取得了显著成功，这主要得益于其能够充分利用先进的GPU。然而，GNN目前面临着在GPU中同时使用Tensor Cores（TCs）和CUDA Cores（CDs）时的诸多挑战。这些挑战因GNN中由于真实世界图的高稀疏性和不规则非零分布所导致的重复、低效和冗余的聚合操作而进一步加剧。</p>
<p>为了解决上述问题，我们提出了RT-GNN，这是一种基于高级TC和CD单元融合的GNN框架，通过利用邻接矩阵的特性消除这些冗余。</p>
<ul>
<li>首先，提出了一种新颖的GNN表示技术——<strong>层次嵌入图（hierarchical embedding graph, HEG）</strong>，通过分层管理中间聚合结果，可以优雅地避免中间聚合中的冗余。</li>
<li>其次，为了解决图的固有稀疏性问题，RT-GNN通过一种新的基于<font color="red"><b>块级行乘法</b></font>的方法，根据<strong>稀疏性</strong>将HEG中的块（<strong>即“tiles”</strong>）分别分配到TC和CD上，从而使TC和CD可以并行工作。</li>
</ul>
<p>实验结果表明，HEG在冗余消除性能方面比HAG平均加速了19.3倍，尤其是在ARXIV数据集上实现了高达72倍的加速。此外，在整体性能上，RT-GNN比最新的GNN框架（包括DGL、HAG、GNNAdvisor和TC-GNN）平均高出3.1倍，并且在任务准确性上保持甚至有所提升。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-introduction">1-Introduction<a href="#1-introduction" class="hash-link" aria-label="Direct link to 1-Introduction" title="Direct link to 1-Introduction">​</a></h2>
<p>图神经网络（Graph Neural Networks, GNNs）因其在建模和探索图结构数据方面的出色能力，越来越多地被应用于各种基于图的学习任务，例如计算机视觉和推荐系统。现有的典型GNN主要采用<strong>递归邻居消息传递方案</strong>，该方案分为两个阶段：<font color="red"><b>聚合</b></font>和<font color="red"><b>更新</b></font>。每个节点从其k跳邻居中聚合消息并更新自身的特征。GNN层的最终激活结果被用于下游预测任务。<strong>稀疏操作和稠密操作交替进行，分别对应于聚合阶段的图处理和更新阶段的神经网络操作</strong>，这使得GNN的预测计算变得困难。</p>
<p>将GNN扩展以有效处理大型真实图数据集仍然是一个棘手的挑战。现有的GNN框架，如Deep Graph Library (DGL)和PyTorch-Geometric (PyG)，主要构建在流行的神经网络框架之上。这些框架<strong>通常将GNN计算分解为两个阶段：稀疏矩阵乘法（SpMM）和通用矩阵乘法（GEMM）。具体来说，SpMM主要用于图处理，而GEMM用于神经网络操作</strong>。
📒：GNN计算操作分为两个阶段</p>
<ul>
<li>图处理：使用SpMM进行处理</li>
<li>神经网络操作：使用GEMM进行处理</li>
</ul>
<p>在GNN计算中，图处理的聚合通常是最耗时的部分。我们的实验表明，由于不规则图邻接矩阵的极度稀疏性（例如  ，平均稀疏性超过99.7%），这一部分甚至占据了整个GNN计算时间的72%以上。图操作涉及为每个节点聚合其所有邻居并将它们加载到内存中，这被认为是一个内存密集型操作。当两个节点的邻居之间存在显著重叠时，节点的聚合也会有显著重叠，从而导致冗余计算。</p>
<p>此外，现有的GNN加速库cuSPARSE在使用SpMM进行聚合时，只设计用于利用CUDA Cores (CDs)。由于邻接矩阵的高维度和稀疏性以及特征矩阵的高密度，与cuBLAS库进行的GEMM更新操作相比，这种方法耗时更多。现代GPU配备了高级硬件（如Tensor Core, TC），可以利用某些类型稀疏性的优势。但cuSPARSE的一个变体仅支持阻塞的SpMM，且要求不同行的非零块数量必须相同。现有的GNN框架大多倾向于关注传统的CUDA Core资源，而忽视了先进的Tensor Core。</p>
<p>为解决由于内存容量、数据传输和硬件利用率瓶颈带来的限制，许多研究已经尝试从算法和系统层面进行优化探索。</p>
<p><img decoding="async" loading="lazy" src="https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20241215181557.png" alt="image.png|center|1000" class="img_ev3q"></p>
<p>从算法的角度来看，研究人员发现，现实世界的图通常遵循幂律分布，其中同一社区内的节点往往共享共同的邻居。<strong>在聚合过程中，大量节点之间的重叠邻居会导致大量冗余计算和内存访问，以及不必要的数据传输</strong>。我们的实验验证了，重复和冗余计算在聚合阶段占用了84%的时间成本。如图1(b)所示，当处理节点d、e和f时，它们的共同1跳邻居a、b和c被加载和传输了三次。现有的一些方案，例如HAG和GraphACT，通过遍历具有高冗余的邻居对，利用分层聚合图来减少冗余计算。一个新的节点abc作为中间聚合结果被聚合，仅需要被加载到内存一次，如图1(c)所示。然而，HAG方法  耗时较长，并且在遍历过程中可能会生成大量的中间数据（即处理过程中产生的临时数据）。其他方案，例如Betty，没有搜索有效的中间聚合结果，而是保留了所有结果，其中一些是冗余的，并受到内存限制。一些采样算法也得到了广泛研究，但由于仅在部分节点上进行训练，这些方法可能会影响模型的收敛性和准确性。在某些应用中，例如天文学中稀有快速射电暴的预测，训练需要涉及所有节点，这使得采样方法变得不可行。因此，本研究聚焦于全图训练。</p>
<p>从系统角度来看，研究人员也开发了多种专门的GNN方案。</p>
<ul>
<li>(1) 一些研究旨在充分利用GPU的计算能力。Wang等人（GNNAdvisor）和Zheng等人通过线程映射和内核优化挖掘GPU资源的潜力。这些方案主要用于调度传统CUDA Core（CD单元）的资源，而TC-GNN则仅将数据块调度到Tensor Core（TC单元），忽略了CUDA Core上的矩阵计算。这些研究<strong>未充分考虑TC和CD在计算模式（如稠密块和稀疏块）方面的特性差异，因此在同时具备TC和CD的GPU上往往产生次优结果</strong>。</li>
<li>(2) 其他研究聚焦于稀疏矩阵乘法（如SpMM）原语，以提高计算和内存效率。GE-SpMM基于内积方法计算GNN，但在使用压缩稀疏行（CSR）格式的邻接矩阵和压缩稀疏列（CSC）格式的特征矩阵时，会产生格式转换开销，并且输入的索引匹配效率较低。I-GCN采用行级乘法，其关键计算为标量-向量乘法，但面临特征矩阵的数据局部性差和数据重用率低的问题。行级乘法中的标量-向量乘法和内积中的索引匹配并未充分利用TC单元的优势。TC单元支持固定矩阵大小的矩阵乘法原语D=A×B+C，并能够在单个GPU时钟周期内高效完成矩阵乘法。</li>
</ul>
<p>在这项工作中，我们从更全面和协同的角度分析了上述算法和系统的问题。我们确定了聚合过程中冗余计算和数据传输，以及稀疏和稠密矩阵块与系统核心单元之间的不匹配，是训练开销的主要来源。现有方案仍然面临以下挑战：</p>
<ol>
<li><strong><font color="red"><b>冗余消除算法的高复杂性</b></font></strong>：这些算法通过遍历整个图来获得中间聚合结果，导致生成过多的中间数据。而其他研究则保存了所有的中间聚合结果，但这些结果实际上应该被有选择地检索。</li>
<li><strong><font color="red"><b>缺乏利用TC和CD之间并行性的能力</b></font></strong>：现有方案未能充分利用TC和CD之间的并行性，导致在任何时刻TC或CD都会处于空闲状态。由于未能将TC和CD识别为独立资源，现有方案往往产生次优结果。同时，通用GPU将其warp调度置于黑箱操作中，进一步限制了优化的可能性。</li>
<li><strong><font color="red"><b>现有矩阵乘法方法的局限性</b></font></strong>：诸如内积、外积、行级乘法和列级乘法等用于计算GNN的矩阵乘法方法，涉及索引匹配和标量-向量乘法，这些方法无法充分利用TC单元。此外，cuSPARSE库对TC单元的可扩展性较差。</li>
</ol>
<p>为了解决上述挑战，我们通过邻接矩阵的性质构建了一种新颖的<strong>层次嵌入图（Hierarchical Embedding Graph，HEG）</strong>，在节点聚合阶段消除冗余。HEG比原始图具有更少的邻居节点。此外，我们利用<strong>基于块的行级乘法来处理图的内在稀疏性</strong>，以及混合TC和CD单元上的不同计算模式。在我们的方案中，<font color="red"><b>线程块中的活动warp可以同时使用TC和CD，从而实现TC和CD内核的融合</b></font>。总结如下，我们的主要贡献包括：</p>
<ul>
<li>我们分析了GNN训练中的内存和数据传输瓶颈，并引入了一种层次嵌入图（HEG），通过分层管理中间聚合结果来消除冗余。为保证HEG的性能，我们将其线性加速设计为一个图划分问题。</li>
<li>我们提出了一种结合贪心搜索的新节点度匹配方法，基于节点的度数搜索有效的中间聚合节点，从而进一步消除中间聚合结果中的冗余。</li>
<li>我们提出了一种针对TC和CD优化的GNN计算方案Fuser，<strong>将HEG分解为稠密块和稀疏块，并同时将其部署到TC和CD单元上执行</strong>，而不会产生在线延迟开销。该方案能够适应动态输入。</li>
<li>我们在不同规模的图上进行了广泛实验。就冗余消除性能而言，HEG相比HAG平均加速19.3倍，相较于Betty，中间聚合所需内存减少了平均102倍。在整体性能上，RT-GNN相比DGL、HAG、GNNAdvisor和TC-GNN平均性能提升了3.1倍。</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-background">2-Background<a href="#2-background" class="hash-link" aria-label="Direct link to 2-Background" title="Direct link to 2-Background">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="21-图卷积网络">2.1-图卷积网络<a href="#21-图卷积网络" class="hash-link" aria-label="Direct link to 2.1-图卷积网络" title="Direct link to 2.1-图卷积网络">​</a></h3>
<p>图神经网络（GNN）将深度表示推广到图数据上，旨在显式提取给定节点的高质量表示。目标节点的表示通过迭代传播邻居信息进行学习，直到达到稳定的固定点。图卷积网络（GCN）是GNN中最常见的实现之一。在每次迭代中，GCN根据第<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span>层（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">k&gt;0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0</span></span></span></span>）的嵌入信息，计算第<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span>层中节点<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>的特征嵌入，如下列公式(1)和(2)所示：</p>
<p><span class="katex-error" title="ParseError: KaTeX parse error: \tag works only in display equations" style="color:#cc0000">a_v^{(k)} = Aggregate(h_u^{(k-1)} \mid u \in N(v)) \tag{1}</span></p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msubsup><mi>h</mi><mi>u</mi><mi>k</mi></msubsup><mo>=</mo><mi>U</mi><mi>p</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>e</mi><mo stretchy="false">(</mo><msubsup><mi>a</mi><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo separator="true">,</mo><msubsup><mi>h</mi><mi>u</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">h_u^k = Update(a_v^{(k)}, h_u^{(k-1)}) \tag{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1461em;vertical-align:-0.247em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">u</span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10903em">U</span><span class="mord mathnormal">p</span><span class="mord mathnormal">d</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.938em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.938em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">u</span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span><span class="mbin mtight">−</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="tag"><span class="strut" style="height:1.188em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">2</span></span><span class="mord">)</span></span></span></span></span></span>
<p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>u</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">h_u^{(k-1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1614em;vertical-align:-0.1166em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em"><span style="top:-2.5834em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">u</span></span></span><span style="top:-3.2198em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span><span class="mbin mtight">−</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1166em"><span></span></span></span></span></span></span></span></span></span>表示第<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span>层中节点<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>的邻居<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">N(v)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mclose">)</span></span></span></span>的隐藏表示，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>a</mi><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">a_v^{(k)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1614em;vertical-align:-0.1166em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em"><span style="top:-2.5834em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span></span></span><span style="top:-3.2198em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1166em"><span></span></span></span></span></span></span></span></span></span>表示第<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span>层中的特征聚合。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>u</mi><mn>0</mn></msubsup></mrow><annotation encoding="application/x-tex">h_u^0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">u</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span></span></span></span>通过特征矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">X_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>初始化。聚合操作通过收集第<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span>层中节点<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>邻居的激活结果来实现。该过程也可以看作是一种特殊的乘法，其中包含稀疏邻接矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi>R</mi><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><mi>V</mi><mi mathvariant="normal">∣</mi><mo>×</mo><mi mathvariant="normal">∣</mi><mi>V</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">A \in R^{(|V| \times |V|)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.888em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.22222em">V</span><span class="mord mtight">∣</span><span class="mbin mtight">×</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.22222em">V</span><span class="mord mtight">∣</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>和稠密特征矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>∈</mo><msup><mi>R</mi><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><mi>V</mi><mi mathvariant="normal">∣</mi><mo>×</mo><mi mathvariant="normal">∣</mi><mi>F</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">X \in R^{(|V| \times |F|)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.888em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.22222em">V</span><span class="mord mtight">∣</span><span class="mbin mtight">×</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.13889em">F</span><span class="mord mtight">∣</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>V</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|V|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mord">∣</span></span></span></span>表示<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span>中节点的数量，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>F</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|F|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.13889em">F</span><span class="mord">∣</span></span></span></span>表示<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span>中特征的维度。</p>
<p>这里，邻接矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span>中第<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">m</span></span></span></span>行的非零值表示节点<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">m</span></span></span></span>的邻居，且第<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">m</span></span></span></span>行的所有非零值会与特征矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span>中对应的特征向量进行相乘，通过逐元素操作聚合节点<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">m</span></span></span></span>的邻居。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="22-tensor-core">2.2-Tensor Core<a href="#22-tensor-core" class="hash-link" aria-label="Direct link to 2.2-Tensor Core" title="Direct link to 2.2-Tensor Core">​</a></h3>
<p>现代GPU在GNN加速方面变得显著强大，它们集成了大量的CD（CUDA Cores）和TC（Tensor Cores）以提高并行性能，例如在NVIDIA Tesla A100中，包含6912个CD和432个TC。最近发布的主流GPU（如NVIDIA Volta和Ampere架构）倾向于集成不同类型的计算核心。A100的流式多处理器（Streaming Multiprocessor，SM）包括四个第三代TC，这些核心每个时钟周期可以执行256个FP16/FP32混合精度的融合乘加（FMA）操作。这一特性提高了操作数共享效率，提升了整体计算性能，并支持更强大的新数据类型。</p>
<p>TC被广泛用于加速通用矩阵乘法（GEMM），因为它们支持矩阵级别的计算原语<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>=</mo><mi>A</mi><mo>×</mo><mi>B</mi><mo>+</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">D = A \times B + C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span></span></span></span>。在最新的Ampere架构中（如Tesla A100），矩阵在块级别的尺寸分别为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo stretchy="false">(</mo><mi>M</mi><mo>×</mo><mi>K</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">A(M \times K)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">A</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="mclose">)</span></span></span></span>、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo stretchy="false">(</mo><mi>K</mi><mo>×</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">B(K \times N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mclose">)</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo stretchy="false">(</mo><mi>M</mi><mo>×</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">C(M \times N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mclose">)</span></span></span></span>，并且遵循矩阵数据类型的特定规则。矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span>通常需要特定精度类型（例如Bfloat16 (BF16) 或TensorFloat-32 (TF32)），而矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span></span></span></span>则存储为FP32格式。在TC上使用TF32进行计算时，要求<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mi>N</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">M = N = 16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">16</span></span></span></span>且<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">K = 8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">8</span></span></span></span>。与矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span></span></span></span>相关的非Tensor操作仍然通过FP32数据路径进行处理。</p>
<p>对于与矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span>相关的输入数据，TC会使用TF32（其数值范围与FP32相同）来替代FP32，从而提高计算速度，最终生成标准的FP32输出。换句话说，TF32作为FP32的一个高效替代方案，可以在不影响数值精度的情况下加速计算过程。最新版本的CUDA（&gt;=11.0）在Ampere架构上进一步加速了张量运算，并且易于集成到深度学习应用的深入加速中。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-motivation">3-Motivation<a href="#3-motivation" class="hash-link" aria-label="Direct link to 3-Motivation" title="Direct link to 3-Motivation">​</a></h2>
<p>GNN结合了图处理与神经网络的执行方式，使得GNN具有更强的表达能力。在本节中，我们对工作负载的内存特性、数据传输和计算进行了分析，评估现有的冗余消除算法，并研究GPU中的TC（Tensor Core）和CD（CUDA Core）单元。基于这些分析，我们观察到现有解决方案存在计算效率低和数据重用率低的问题。我们从软件和硬件架构两个层面探究了性能限制的根本原因，并提出了一种新的协同设计方案以提升GNN的性能。</p>
<p>我们选取了一个广泛使用的两层GCN模型，对GNN计算的工作负载情况和运行时特性进行了深入探索。所有测试均在NVIDIA Tesla A100上进行，选取了六个具有代表性的数据集进行实验。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="31-现有冗余消除算法的工作量">3.1-现有冗余消除算法的工作量<a href="#31-现有冗余消除算法的工作量" class="hash-link" aria-label="Direct link to 3.1-现有冗余消除算法的工作量" title="Direct link to 3.1-现有冗余消除算法的工作量">​</a></h3>
<p><img decoding="async" loading="lazy" src="https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20241217083853.png" alt="image.png|center|800" class="img_ev3q"></p>
<p>从算法角度来看，我们可以对聚合和数据传输的工作负载做出以下两点观察：</p>
<p><strong>观察1</strong>
在高度不规则的稀疏图中（稀疏度超过99.7%），聚合操作是耗时的主要原因，节点之间存在大量重叠邻居，导致大量冗余计算。图2(a)展示了一个训练周期中数据传输、聚合和其他操作的时间分布。实验结果表明，在训练周期中，聚合阶段的计算和数据传输操作平均占总时间的72%以上，尤其是数据传输，由于稀疏图的不规则数据加载，甚至占到了67%的时间。冗余计算的成本构成了聚合操作的84%。值得注意的是，在计算过程中，某些图数据集相较于原始图的规模，被重复加载了四次，从而产生冗余。</p>
<p><strong>观察2</strong>
现有的冗余消除算法具有较高的计算复杂度，会生成大量中间数据。例如，<strong>HAG</strong>经过重构和测试后，图2(b)显示了冗余消除过程中生成的中间数据的峰值内存使用量。HAG会为每个节点遍历节点  对，并通过<strong>UNORDERED_MAP</strong>哈希结构逐一累积相同节点对的冗余。不同节点对的冗余数据都存储在该结构中，导致在大规模数据集上生成大量中间数据，并引发冲突。HAG在<strong>PRODUCTS</strong>和<strong>REDDIT</strong>数据集上出现了内存溢出（OOM）问题，过高的内存使用量使HAG无法处理大规模图数据。</p>
<p>此外，其他算法（如<strong>Betty</strong>）会存储所有检测到的中间聚合结果，而这些结果本应被有选择地检索。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="32-异构核对-gnn-矩阵处理的挑战">3.2-异构核对 GNN 矩阵处理的挑战<a href="#32-异构核对-gnn-矩阵处理的挑战" class="hash-link" aria-label="Direct link to 3.2-异构核对 GNN 矩阵处理的挑战" title="Direct link to 3.2-异构核对 GNN 矩阵处理的挑战">​</a></h3>
<p>从架构角度来看，我们对GNN训练过程中的并发性有以下观察：</p>
<p><img decoding="async" loading="lazy" src="https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20241217084617.png" alt="image.png|center|800" class="img_ev3q"></p>
<p><strong>观察3</strong>：现有方案主要针对传统CUDA Core (CD单元) 进行调度，当应用于配备异构核心（Tensor Core (TC) 和 CUDA Core）的GPU时，往往会产生次优结果。例如，DGL仅使用CD单元通过cuSPARSE进行计算和数据传输。而TC-GNN则将GNN计算中的矩阵乘法完全部署在TC单元上，CD单元仅用于数据传输和其他操作。图3显示了在TC-GNN运行GCN模型时，两种单元在Streaming Multiprocessor (SM) 上的活动时间线。由于<strong>现有解决方案未将TC和CD视为独立资源，无法同时高效调度两个单元，导致任意时刻要么TC单元空闲，要么CD单元空闲</strong>。此外，当两类单元都处于空闲状态时，可能是由于内核启动开销和事件记录与内核执行之间的上下文切换所致。</p>
<p>为了充分利用潜在的并发性，作者探索了几种架构加速的可行方案。通过对不同调度策略的实验发现，<font color="red"><b>如果线程块中的不同活跃warp同时为两种类型的单元（TC和CD）提供服务，TC和CD便可以实现并行执行</b></font>。<strong>线程块间的warp分歧不会导致不必要的计算，因为每个warp具有确定性的执行路径。因此，这为TC和CD的融合提供了机会，可以通过同时分配硬件资源，在warp之间重叠独立数据块来实现并行</strong>。然而，TC和CD直接融合时存在资源竞争问题，导致效率降低。因此，作者在内存管理的基础上进一步探索了这两类单元的最大并行性，以提升数据重用率。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-rt-gnn-design">4-RT-GNN Design<a href="#4-rt-gnn-design" class="hash-link" aria-label="Direct link to 4-RT-GNN Design" title="Direct link to 4-RT-GNN Design">​</a></h2>
<p>基于上述观察，我们提出了一个名为RT-GNN的GNN框架，该框架通过融合先进的Tensor Core（TC）和CUDA Core（CD）单元，利用邻接矩阵的特性消除聚合过程中的冗余计算。</p>
<p>我们首先提出了层次嵌入图（HEG）来分层管理中间聚合结果，这一方法能够完美替代传统的冗余消除过程。此外，通过结合贪心搜索和度匹配的方法，进一步消除冗余的中间聚合结果。除此之外，我们提出了一种基于块的行级乘法方法，以及针对不同计算模式的TC与CD融合方案，以为GNN计算提供高效支持。</p>
<ul>
<li>提出层次嵌入图来分层管理中间聚合结果</li>
<li>结合贪心搜索和度匹配方法，进一步消除冗余的中间聚合结果</li>
<li>基于块的行级乘法方法，提出不同计算模式的TC和CD融合方案</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="41-heg表示">4.1-HEG表示<a href="#41-heg表示" class="hash-link" aria-label="Direct link to 4.1-HEG表示" title="Direct link to 4.1-HEG表示">​</a></h3>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="42-tc和cd定制并行gnn计算">4.2-TC和CD定制并行GNN计算<a href="#42-tc和cd定制并行gnn计算" class="hash-link" aria-label="Direct link to 4.2-TC和CD定制并行GNN计算" title="Direct link to 4.2-TC和CD定制并行GNN计算">​</a></h3>
<p>为了在HEG中利用异构核心（Tensor Cores 和 CUDA Cores，简称TC和CD）的数据局部性和潜在并行性，我们提出了一种基于<strong>块级行乘法</strong>的方法，以解决观察3中指出的问题。我们方案的基本构建单元是<strong>图块</strong>，即将图划分为数据块，也称为tiles。这种基于块的方法有两个主要优势：</p>
<ul>
<li>(1) 减轻了因过多内存占用带来的压力；</li>
<li>(2) 通过在Tensor Core和CUDA Core（TC和CD）上实现块级并行，充分挖掘并行计算潜力。</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20241217091949.png" alt="image.png|center|800" class="img_ev3q"></p>
<p>图的稀疏性通常对最终结果产生负面影响，这启发我们首先进行以下改进：</p>
<ul>
<li><strong>轻量级图重排</strong>：我们利用图重排来提高节点局部性，如图6所示，这仅对现有系统进行最小化修改。轻量级图重排有两个好处：<!-- -->
<ul>
<li>(1) <strong>数据局部性得到了改善</strong>。稠密和稀疏块能够促进TC和CD单元的基于块的计算，从而显著提高并行性能。</li>
<li>(2) <strong>图块数量得到减少</strong>。我们使用启发式<strong>度排序</strong>方法，对邻接矩阵中的节点按入度进行排序。这样，非零值显示出更高的聚集性特征（见图6(b)）。入度高的节点趋向于移动到左侧，右侧则留下更多空行，适用于稀疏块划分。因此，加载的图块总数从四个减少到三个，如图6(b)所示。</li>
</ul>
</li>
<li><strong>针对TC和CD优化的乘法</strong>：我们采用<strong>基于块的行乘法</strong>，用于邻接矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span>和特征矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span>的乘法运算，并在其中融合了Tensor Core (TC) 和CUDA Core (CD) 的内核执行。矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span>被划分并约束为更小的块。在我们的方法中，通过合并具有相似邻居的相邻节点的工作负载，加载的稀疏特征块可以被重用。我们利用HEG中的稀疏趋势（从左到右逐渐变得更加稀疏），<strong>  方便地将不同类型的计算模式（稠密或稀疏）与不同类型的计算核心（TC或CD）匹配，以充分发挥各自的优势</strong>。</li>
</ul>
<p>在<strong>第一阶段</strong>，如图7(b)所示，数据被加载。邻接矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span>的稠密块和稀疏块可以通过分别从<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span>的左右两侧扫描对应的行，同时并行地部署到Tensor Core（TC）和CUDA Core（CD）。与此同时，特征矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span>中两个窗口的两个块（绿色块）被加载到共享内存中。此外，我们根据块级的warp组织方式自定义共享内存布局，以减少对全局内存的不必要访问。映射到同一tile的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span>中的块按顺序存储，从而通过基于tile的内存访问提供高空间局部性。</p>
<p>在<strong>第二阶段</strong>，进行数据计算。矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span>中的tile与<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span>中对应的tile相乘，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span>矩阵的行索引由<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span>矩阵非零值的列索引决  定。TC单元从左到右处理<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span>中的稠密块，而CD单元同时从右到左处理非零的稀疏块。这种方式提高了指令级并行性。相比之下，TC-GNN仅在一行中将tile分配给TC单元进行计算，如图7(a)所示，这会产生大量内存事务，且无法超越TC和CD同时处理tile的效率。在TC-GNN中，当TC单元计算tile时，CD单元处于空闲状态。</p>
<p>TC和CD上的块级并行促进了训练过程中的负载平衡，有效缓解了由于矩阵稀疏性导致的不平衡问题。基于块级行乘法方法的主要优势如下：</p>
<ol>
<li><strong>更快的计算速度</strong>：我们的方案利用了先进的TC和CD硬件单元，在TC和CD上并行进行tile乘法运算。相比之下，传统的行级乘法仅能利用CD单元进行标量-向量乘法。</li>
<li><strong>高数据重用率和低内存需求</strong>：合并相邻节点的计算负载可以提高数据局部性。此外，由于图7(b)中的两个输出窗口同时计算，输出矩阵 的缓冲区大小可以减少到两个窗口。而现有方法（如外积法）通常需要将整个输出矩阵存储在内存中。</li>
<li><strong>减少同步开销</strong>：我们的方案允许流式多处理器（SM）异步计算输出矩阵的不同tile，避免频繁同步读写操作。</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="43-基于ptb的柔性融合">4.3-基于PTB的柔性融合<a href="#43-基于ptb的柔性融合" class="hash-link" aria-label="Direct link to 4.3-基于PTB的柔性融合" title="Direct link to 4.3-基于PTB的柔性融合">​</a></h3>
<p>现有方案无法有效利用Tensor Core（TC）和CUDA Core（CD）之间的并行性，<font color="red"><b>导致在任何时刻TC或CD总有一方处于空闲状态</b></font>。这些方案未能将TC和CD识别为独立资源，因此往往导致次优的结果。我们提出了一种<strong>Fuser</strong>异构核融合方案，该方案能够并行融合并调度TC和CD单元。每个窗口通过<font color="red"><b>持久线程块（Persistent Thread Block, PTB）</b></font>分配给一个流式多处理器（SM），在PTB中，每个线程块被视为一个工作单元。使用PTB时，每个持久线程块被分配相应的任务，这些任务对应于原始线程块。PTB会始终保持SM处于忙碌状态，直到分配的任务全部完成才退出。</p>
<p>通过对不同调度方案进行多次测试，我们发现，<font color="red"><b>如果线程块中的不同warp同时在Tensor Core（TC）和CUDA Core（CD）单元上执行任务，这两种核心可以实现并行化</b></font>。<strong>在同一个线程块中，多个warp可以并行激活。warp之间的分歧不会引发不必要的计算，因为每个warp都有确定的执行路径</strong>。这为优化提供了机会，通过合理地分配硬件资源并将独立的数据块分配给多个warp，从而实现重叠计算。</p>
<p>基于持久线程块（PTB）的简单方案将TC和CD单元以1:1的比例进行融合，但这可能会降低TC单元的性能。<strong>实际上，每个TC在流式多处理器（SM）中每个时钟周期可以完成256次融合乘加（FMA）操作，而每个CD每个时钟周期只能执行一个标量乘法操作，显然不适合密集计算</strong>。因此，我们提出了一种<strong>灵活融合比</strong>的方案，以增强Fuser。<strong>根据TC和CD的处理速度，将图块按灵活比例分配到TC和CD单元上。这个灵活比例由TC上第一个右侧图块与CD上第一个左侧图块的执行时间比例决定</strong>。</p>
<p><img decoding="async" loading="lazy" src="https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20241217101620.png" alt="image.png|center|800" class="img_ev3q"></p>
<p>在实际应用中，邻接矩阵的图块会从双端队列中动态地调度到TC和CD单元上，如图8所示。在GPU的定制共享内存中，通过一个数组来模拟双端队列的实现，并一次性将所有数据加载到队列中。这是内核实现的一部分。数组中warp所需数据的位置由对应warp的索引编号决定。TC单元从较小的索引到较大的索引获取数据，而CD单元则相反。数据不需要再次入队。</p>
<p>warp会根据灵活融合比被分配到TC或CD分支。这里，<strong>分支</strong>是具有相同确定执行路径的一组warp。<font color="red"><b>TC和CD的分支划分基于灵活融合比</b></font>。例如，如果窗口中的TC与CD的灵活融合比为2:1，那么TC分支的数量将是CD分支的两倍，TC和CD分支的总数为3。<strong>为了更好地组织这些分支，我们确保每个分支中的warp数量相同</strong>。</p>
<p>为避免数组访问冲突，我们为每个分支分配窗口中对应的索引范围。索引范围的数量由TC和CD分支总数确定，而索引范围的长度则由窗口长度除以分支数量得到。索引范围的数量设置为3，且彼此之间不重叠。每个分支在TC或CD上以相同的索引范围长度和warp数量进行处理，所需时间几乎相同。因此，所有分支可以并行完成图块的计算。最后，TC单元会被分配去处理那些位于已定义索引范围之外的剩余图块。由于不同分支之间的时间差异很小，我们仅需要进行warp级同步，从而实现跨warp的显式同步，具体细节在第4.5节中描述。</p>
<p>关键步骤是从队列中将足够多的<strong>稠密块</strong>分配给Tensor Cores（TCs），使其尽可能满载工作，同时将合适的<strong>稀疏块</strong>分配给CUDA Cores（CDs），让它们与TCs协同工作。我们优先使用TC单元以获得更高的吞吐量，因 为TC在矩阵乘法方面比CD更强大。<strong>Fuser</strong> 最大限度地利用了TC和CD的并行能力，使两类单元能够同时完成各自的计算任务。与现有方法相比，Fuser表现更优，因为在这些方法中，TC或CD在任何时刻都会出现空闲状态。</p>
<p>此外，我们基于资源估算（详见第4.4节），为单个流式多处理器（SM）分配了足够的线程块。需要注意的是，即使在自适应Fuser中，仍然存在隐性的资源争用问题。此外，Fuser能够适应动态输入，在有限资源下最大限度地挖掘出各单元的并行潜力。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="44-成本函数与内存感知调度策略">4.4-成本函数与内存感知调度策略<a href="#44-成本函数与内存感知调度策略" class="hash-link" aria-label="Direct link to 4.4-成本函数与内存感知调度策略" title="Direct link to 4.4-成本函数与内存感知调度策略">​</a></h3>
<p>我们引入了一个代价函数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span></span></span></span>来表征GNN模型的训练开销。该代价函数用于估算特定窗口中所有节点的聚合和更新操作所消耗的资源。最初，RT-GNN通过代价函数估算窗口的工作负载，然后执行<strong>面向内存的调度</strong>，以最大化流式多处理器（SM）的占用率，如图8所示。RT-GNN能够动态分配工作负载，并解决多个SM单元之间的负载不平衡问题。</p>
<p><strong>并发线程块</strong>的核心思想是<font color="red"><b>保持大量的warp处于激活状态，以隐藏由于块乘法引起的延迟</b></font>。共享内存是限制线程块并行度的主要因素。此外，SM中的寄存器也是GPU上稀缺的资源。在本研究中，我们通过使用<code>_launch_bounds_()</code> 将无法存放在寄存器中的数据放入本地内存，以提高SM的占用率。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="45-同步">4.5-同步<a href="#45-同步" class="hash-link" aria-label="Direct link to 4.5-同步" title="Direct link to 4.5-同步">​</a></h3>
<p>我们注意到输出矩阵在写入前读取时会发生<strong>竞争条件</strong>，这需要适当的线程同步。一般来说，现有矩阵乘法内核在完成一个块的计算后会调用 <code>__syncthreads()</code> 来同步所有线程。然而，调用 <code>__syncthreads()</code> 可能会导致<strong>死锁</strong>、性能下降和结果错误等问题。因此，我们利用基于<font color="red"><b>底层PTX代码 bar.sync</b></font> 的<strong>批量同步接口</strong>，该接口作为块内的<strong>warp级别屏障</strong>。我们仅在同一分支完成计算时显式进行<strong>跨warp同步</strong>。由于在跨warp中没有两个线程写入相同的输出区域，因此在处理分支内的每个warp之后不需要进行同步。
<img decoding="async" loading="lazy" src="https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20241217103349.png" alt="image.png|center|800" class="img_ev3q">
如图9所示，一批在Tensor Core (TC) 和CUDA Core (CD) 上以2:1速度比完成计算的warp，仅在每个单元上同步一次。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="46-实现">4.6-实现<a href="#46-实现" class="hash-link" aria-label="Direct link to 4.6-实现" title="Direct link to 4.6-实现">​</a></h3>
<p>现有的框架（如PyTorch和TensorFlow）通常<font color="red"><b>将图的拓扑结构转换为稀疏邻接矩阵来实现GNN模型</b></font>。基于PyTorch框架，我们聚焦于聚合阶段，并通过提出的TC与CD融合方案，实现了冗余消除和基于块级行的稀疏矩阵乘法，以加速不规则的聚合计算。我们应用以下算法模块，为GNN计算提供高效支持：</p>
<ul>
<li><code>_Part_Parallel_</code> 会将一个GNN图 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">G</span></span></span></span> 自动转换为等价的 HEG <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mtext>’</mtext></mrow><annotation encoding="application/x-tex">G’</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">G</span><span class="mord">’</span></span></span></span>，并获取公共邻居以减少冗余计算。</li>
<li><code>_Fuser_</code> 会自动编译优化后的核函数，将TC与CD融合。</li>
<li><code>_RT-GNN_aggregate_</code> 将 HEG <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mtext>’</mtext></mrow><annotation encoding="application/x-tex">G’</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">G</span><span class="mord">’</span></span></span></span> 和节点特征作为输入，计算所有节点的前向传播值。前向传播会自动调用优化后的核函数 <code>_Fuser_</code>，无需用户干预。</li>
<li><code>_RT-GNN_aggregate_grad_</code> 通过编译后的优化核函数 <em>Fuser</em> 计算梯度，以进行反向传播并更新模型。
我们的实现对现有的GNN编程模型进行了最小的  改动。我们使用了一种聚合和更新的编程模型，这是GAS（Gather-Apply-Scatter）编程模型的一个变体。这种变体被广泛应用于GNN模型，如DGL、HAG、NeuGraph等。当应用上述改进后，只需修改少量代码，即可使GNN应用程序从所有RT-GNN优化中获益。</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="5-evaluation">5-Evaluation<a href="#5-evaluation" class="hash-link" aria-label="Direct link to 5-Evaluation" title="Direct link to 5-Evaluation">​</a></h2>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="6-related-work">6-Related work<a href="#6-related-work" class="hash-link" aria-label="Direct link to 6-Related work" title="Direct link to 6-Related work">​</a></h2>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="7-conclusion">7-Conclusion<a href="#7-conclusion" class="hash-link" aria-label="Direct link to 7-Conclusion" title="Direct link to 7-Conclusion">​</a></h2>
<p>在本研究中，我们提出了<strong>RT-GNN</strong>，一种结合冗余消除的基于<strong>块级行乘法</strong>的方法。首先，我们构建了一种新颖的图表示方法<strong>HEG</strong>，通过贪心搜索与新的度匹配方法相结合，有效地消除冗余。其次，为了在训练和推理过程中提高数据局部性和数据重用率，我们提出了<strong>基于块级行乘法</strong>的方法，通过内核融合实现Tensor Core (TC) 和 CUDA Core (CD) 单元在不同计算范式下的并行执行。此外，与PyTorch框架的集成进一步实现了端到端的高可编程GNN计算。大量实验表明，RT-GNN在各种GNN模型和数据集上，相较于DGL、HAG、GNNAdvisor和TC-GNN等最先进框架表现出优越的性能，并突显了其性能优势。</p>
<p>在未来的工作中，我们提出的HEG可以结合<strong>运行时训练的分层流水线机制</strong>部署到GPU上，并通过<strong>异步流水线</strong>和<strong>资源感知的动态Tensor Core与CUDA Core内核融合</strong>进行优化，从 而加速模型的并行计算。</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://buaaer-xing.github.io/docs/paper_notes/3-TCU相关/RT-GNN/阅读笔记.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/paper_notes/稀疏矩阵计算/Other/HiCOO/阅读笔记"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">阅读笔记</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/paper_notes/未分类/intro"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">说明</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#0-abstract" class="table-of-contents__link toc-highlight">0-Abstract</a></li><li><a href="#1-introduction" class="table-of-contents__link toc-highlight">1-Introduction</a></li><li><a href="#2-background" class="table-of-contents__link toc-highlight">2-Background</a><ul><li><a href="#21-图卷积网络" class="table-of-contents__link toc-highlight">2.1-图卷积网络</a></li><li><a href="#22-tensor-core" class="table-of-contents__link toc-highlight">2.2-Tensor Core</a></li></ul></li><li><a href="#3-motivation" class="table-of-contents__link toc-highlight">3-Motivation</a><ul><li><a href="#31-现有冗余消除算法的工作量" class="table-of-contents__link toc-highlight">3.1-现有冗余消除算法的工作量</a></li><li><a href="#32-异构核对-gnn-矩阵处理的挑战" class="table-of-contents__link toc-highlight">3.2-异构核对 GNN 矩阵处理的挑战</a></li></ul></li><li><a href="#4-rt-gnn-design" class="table-of-contents__link toc-highlight">4-RT-GNN Design</a><ul><li><a href="#41-heg表示" class="table-of-contents__link toc-highlight">4.1-HEG表示</a></li><li><a href="#42-tc和cd定制并行gnn计算" class="table-of-contents__link toc-highlight">4.2-TC和CD定制并行GNN计算</a></li><li><a href="#43-基于ptb的柔性融合" class="table-of-contents__link toc-highlight">4.3-基于PTB的柔性融合</a></li><li><a href="#44-成本函数与内存感知调度策略" class="table-of-contents__link toc-highlight">4.4-成本函数与内存感知调度策略</a></li><li><a href="#45-同步" class="table-of-contents__link toc-highlight">4.5-同步</a></li><li><a href="#46-实现" class="table-of-contents__link toc-highlight">4.6-实现</a></li></ul></li><li><a href="#5-evaluation" class="table-of-contents__link toc-highlight">5-Evaluation</a></li><li><a href="#6-related-work" class="table-of-contents__link toc-highlight">6-Related work</a></li><li><a href="#7-conclusion" class="table-of-contents__link toc-highlight">7-Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/paper_notes_intro">论文笔记</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/blogs_intro">个人博客</a></li><li class="footer__item"><a class="footer__link-item" href="/blog">相关内容</a></li><li class="footer__item"><a class="footer__link-item" href="/resume">个人简历</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://t.me/cx_cst" target="_blank" rel="noopener noreferrer" class="footer__link-item">Telegram<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://blog.csdn.net/qq_45575167" target="_blank" rel="noopener noreferrer" class="footer__link-item">CSDN</a></li><li class="footer__item"><a href="https://github.com/BUAAer-xing" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 BUAAer-xing, 此网站使用 Docusaurus 进行构建✨</div></div></div></footer></div>
</body>
</html>