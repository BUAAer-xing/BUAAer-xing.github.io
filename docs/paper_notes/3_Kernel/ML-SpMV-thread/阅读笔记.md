# 阅读笔记

## Abstract

由于内存访问不规则和工作负载不平衡，GPU 上的稀疏矩阵向量乘(SpMV)优化一直是一个挑战。现有的大多数解决方案根据经验公式将固定数量的线程分配给一行或多行稀疏矩阵。然而，这种方法并没有给出最优的线程配置，导致了显著的性能损失。<font color='red'><b>本文提出了一种新的基于机器学习的 GPU 环境下 SpMV 线程分配策略，预测了矩阵近似最优的线程配置。</b></font>进一步，我们**根据非零元素的分布将不规则稀疏矩阵分割成块**，并**预测每个块的最佳线程配置**。一个新的 SpMV 内核被设计用来加速不同块的执行。

实验结果表明，本文提出的基于机器学习的线程配置方法能够为大多数矩阵选择接近最优的线程配置。通过**矩阵划分**和**分块预测**，提高了 SpMV 算法对不规则矩阵的处理效率。最后，我们深入研究训练后的模型，**找出稀疏矩阵的特征与其最佳线程配置之间的联系**。

📒：也是进行了分块处理，以及通过机器学习算法，来为每个块预测最佳的线程配置，同时，提出一个新的SpMV kernel 来加速不同块的执行。

## Introduce

许多领域中的实际性分析和设计问题，如结构分析、社会科学和电路设计等，都可以转化为稀疏线性系统去求解。迭代求解器，如共轭梯度（CG）和广义最小残差法（GMRES），是解决这些系统的最常见和流行方法。它们严重依赖于SpMV，在解决线性系统时占总时间的80% 。因此，有必要开发高效的SpMV算法来减少现有求解器的执行时间并提高整体性能。

由于其大规模并行性和高内存带宽，图形处理单元（GPUs）已成为具有前景的计算设备。然而，在实践中生成的矩阵往往非常稀疏。电路网表中一个单元通常连接到几个其他单元。因此每一行中非零元素（NNZ）的数量通常微不足道。有一些节点具有数百个输出端口，例如电源和时钟输入  。<font color='skyblue'><b>在GPU上加速SpMV面临着不规则内存访问和各行之间工作负载不平衡等重大挑战。</b></font>

目前的研究表明，**SpMV 算法的性能受到输入矩阵中非零元素分布的影响**。<font color='red'><b>没有稀疏压缩格式可以在所有稀疏矩阵和设备中提供最高的性能。</b></font>因此，近年来出现了几种格式选择方法。然而，压缩的稀疏行(CSR)仍然是最流行的压缩格式，并被许多应用程序广泛使用。将 CSR 转换为一种新的格式会带来大量的开销，这在实践中是不可忽视的。

为了解决工作负载不平衡的问题，研究人员开发了几种基于 GPU CSR 的新算法来提高 SpMV 的性能。**由于不同行中的 NNZ 可能大不相同，因此很难找到一种有效的并行策略来充分利用底层硬件的计算能力**。在 SpMV 并行计算中，首先将一个线程分配给一个矩阵行。然而，这对于处理长行来说是低效的，因为 GPU 中的线程运行在单指令多线程(SIMT)范例中，而分配给短行的其他线程必须等待处理长行的线程。后来，提出了 CSR-vector算法，分配几个线程来处理每一行，这样可以比单个线程更快地处理长行。这个设计中的一个关键问题是确定在 SpMV 实现中分配给一行的线程数。CSR 向量的现有实现要么设置固定数量的线程，要么根据经验公式计算线程数量。**在 CUSP的 CSR 向量的最新实现中，线程的数量基于每行输入矩阵的平均 NNZ**。另一方面，Reguly 等人也提出了类似的规则。然而，我们对2000多个稀疏矩阵的实验表明，<font color='skyblue'><b>现有的线程配置与最佳线程配置之间仍然存在显著的性能差距。</b></font>

根据以上观察，在本文中，我们提出了一种基于机器学习（ML）的方法来预测**最佳线程配置（分配给行的线程数）**。我们使用简单易计算的矩阵特征作为<font color='red'><b>分类模型</b></font>的输入，并使用超过3,000个矩阵对模型进行训练和测试。分析表明，我们的模型在测试准确率方面超过80%，并且对大多数矩阵实现接近最优结果。此外，**不规则矩阵根据其非零元素在不同部分的分布被虚拟地划分为几个子矩阵块**，并针对每个块单独和相互独立地预测相应的最佳线程配置。<font color='red'><b>不规则矩阵的划分生成多个局部正则块</b></font>，从而可以选择每个块的最佳线程配置。

📒：类似使用Tile SpMV + ML 的研究思路

本文的主要贡献如下：

- 提出一种基于机器学习的方法，用于预测每个稀疏矩阵的最佳线程配置。实验结果显示，预测模型在两台不同的GPU设备上实现了高于80%的准确率。此外，预测的线程配置与最佳性能非常接近，在两台机器上对CSR-vector 使用16个线程进行每行操作时，算术平均加速比分别为1.21倍和1.24倍。
- 提供一种<font color='red'><b>非零值分布感知矩阵</b></font>（😲起名字真的必须得高大上呀😲)划分方法，并设计一个<font color='red'><b>新的SpMV内核</b></font>，该内核<font color='red'><b>采用基于块的ML线程配置方法</b></font>。实验结果表明，通过**矩阵划分**和**基于块的预测**显著改善了不规则矩阵的性能。在两个GPU上实现了算术平均加速比分别为3.00倍和2.67倍。
- 深入探讨经过训练的ML模型，以<font color='red'><b>找出稀疏矩阵特征与其最佳线程配置之间的联系</b></font>。
	- 此外，我们推导出一个**轻量级推理模型**，在实际使用中仅需几微秒来进行预测！！！

本文的大致框架如下：

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240317140255.png)
<center> <font face='华文宋体' size='4'> 本文的文章结构图 </font> </center>

## Problem statement

### CSR-vector 算法（Revisiting）

CSR-vector将一组协作线程<mark> <b>TpR（每行线程数）</b></mark>分配给处理每个矩阵行。它将TpR限制为2、4、8、16或32，以利用<font color='red'><b>warp中线程的同步执行</b></font>。这些协作线程首先访问同一行中的非零元素和乘以向量的对应元素进行乘法运算。然后，使用对数并行归约来生成输出。最后，<font color='red'><b>每个TpR线程中的第一个将结果写回全局内存</b></font>。CSR-vector 在内存访问和线程并行性方面比 CSR-scalar 更有效，后者为每行分配一个线程。然而，对于一些不规则矩阵，CSR 向量中 TpR 过小对于访问局部性没有什么好处，可能会导致类似于 CSR-scalar的性能损失。类似地，过大的 TpR 可能导致某些线程处于空闲状态，从而导致计算资源的浪费。**因此，TpR 的设置值得仔细考虑。**

Bell首次提出 CSR-vector，TpR 值为32。最新的 CUSP 库提供了 CSR-vector的实现，其中 TpR 计算为：
$$TpR\_mean=\min(2^{\lceil\log_2^{\lfloor\frac{nnz}m\rfloor}\rceil},32)$$
其中 $m$ 和 $nnz$ 表示稀疏矩阵的行数和非零元素。

除此之外，Reguly 等还提出了 TpR 的经验式:
$$TpR\_SqMean=\min(2^{\lceil\log_2^{\lfloor\sqrt{\frac{nnz}m}\rfloor}\rceil},32)$$

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240317153520.png)
<center> <font face='华文宋体' size='4'> 图 1 在两个GPU上，对于两个矩阵circuit_3和hcircuit，使用不同TpR的CSR向量进行运行时比较 </font> </center>

在两个GPU平台（Tesla V100和P100）上测试了具有不同TpR的CSR-vector的性能，用于来自SuiteSparse矩阵集合中的两个稀疏矩阵（circuit\_3和hcircuit）。

图1(a)和(b)显示了circuit\_3的实验结果，其中两台机器上最快的TpR均为32。$TpR\_mean$ 和 $TpR\_SqMean$ 分别捕获了两台机器上最差和第二差的TpR。

对于矩阵hcircuit，如图1(c)和(d)所示，在Tesla V100和P100上，最快的TpR分别为16和8。

上述观察表明，一方面，现有的 TpR 设置和最佳设置之间仍然存在显著的性能差距。另一方面，矩阵的最优 TpR 可能随着 GPU 的变化而变化，但是 $TpR\_mean$ 或 $TpR\_SqAverage$ 会得到相同的 TpR。因此，给定一个输入矩阵，在给定的 GPU 平台上找到一个策略来推导出最优或接近最优的 TpR，有可能进一步提高 CSR 矢量的性能。

###  CSR-adaptive 算法

虽然 CSR-vector 通常优于 CSR-scalar ，但它没有考虑到非零元素在不同行中的不均匀分布，因此，对于一些不规则矩阵的性能很差。**CSR-adaptive 旨在处理SpMV中的不平衡负载**。

首先，将矩阵划分为包含一个或多个行的子矩阵块，并且每个子矩阵块中的NNZ大致相同。与CSR-vector类似，CSR-adaptive 为每个子矩阵块分配了固定数量的线程（例如512）。 CSR自适应显着提高了具有长行稀疏矩阵的计算效率。**然而，由于子矩阵块中的非零元素被一个线程块中所有线程按元素进行处理，并且每个线程执行一行的部分计算，因此线程块中所有线程使用共享内存而不是寄存器来缓存中间结果**。一旦所有线程完成了逐元素乘法和线程块同步，就会指定一个线程来减少每行的部分结果并将最终结果写回全局内存。因此，在计算长行和短行时需要执行几次共享内存读取和写入操作。

此外，在当子矩阵块中的行数远小于线程块中的线程数时，在归约阶段会有适度数量的空闲线​​程序。例如，稀疏矩阵iprob通过CSR-adaptive 划分为25个区域。除了包含一条长行之外，每个区域能够128条记录。当将线​​程序组大小固定为256时，在归约阶段每个​​线​​程序组中有一半的线​程序处于空闲状态。

总之，**CSR-adaptive 以额外的内存访问和同步开销为代价，实现了线程块之间的负载平衡**。此外，在逐行减少阶段，中等数量的线程可能处于空闲状态。**因此，采用 CSR-vector 和 CSR-adaptive 相结合的策略更有利于提高系统的整体性能。**


## Global thread configuration prediction

### 方法概述

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240317171156.png)
<center> <font face='华文宋体' size='4'> 表 1 四个稀疏矩阵的示例 </font> </center>
寻找稀疏矩阵的最佳TpR设置是具有挑战性的。这主要原因在于**不同行中非零元素的复杂和多样分布**。表1展示了四个稀疏矩阵的示例。矩阵mk12-b2中每行的NNZ为3，配置TpR_SqMean选择了最佳的TpR设置为2，而策略TpR_mean得出了接近最优的TpR设置为4。矩阵rdb450l中每行的NNZ落在区间内，这两种线程配置产生不同结果。对于n2c6-b7矩阵，每行有8个非零元素，两种配置都未能选取到最佳的TpR设置。对于每行NNZ差异较大的矩阵来说，由这些配置给出的TpR可能导致显著性能损失。例如，hcircuit矩阵中最短行只有一个非零元素，而最长行有1,399个非零元素，但平均每行NNZ仅约为4。TpR_mean和TpR_SqMean提供了一种线程配置，在计算长行时成为性能 bottleneck，并且远远逊色于最优解。

上面的观察表明，最佳线程配置不仅与每行的 NNZ 有关，而且还与其他一些特性有关。此外，<font color='red'><b>矩阵特征与最优线程配置之间的关系并不直接，难以用公式建立关系</b></font>。因此，**本文提出了一种基于机器学习技术的线程配置预测策略**。图2给出了我们的方法的概述。

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240317171527.png)
<center> <font face='华文宋体' size='4'> 图 2 使用 AutoML 框架训练和预测最佳 TpR 设置 </font> </center>
我们首先选择稀疏矩阵的一些特征，然后用最优 TpR 标记每个矩阵。然后，将这些标记矩阵输入不同的 ML 模型进行训练。接下来，选择在测试集中达到最高精度的模型。最后，给定一个输入稀疏矩阵，训练后的 ML 模型根据其特征预测最优 TpR 设置。

### 数据集和特征设置

训练集和测试集来自 SuiteSparse 矩阵集合 ，这是一个普遍的稀疏矩阵数据集，在现有的稀疏矩阵计算优化工作中被广泛使用。该集合包含2,893个稀疏矩阵和1,020个非对称稀疏矩阵。除了一些非常大或复杂类型的稀疏矩阵，2,833个稀疏矩阵和978个非对称稀疏矩阵的换位构成了最终的数据集。加入不对称矩阵的换位增加了数据集的大小，从而提高了模型的精度。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240317171929.png)
<center> <font face='华文宋体' size='4'> 图 3 不同标号矩阵的分布 </font> </center>
为了找到每个矩阵的最佳 TpR 设置，使用所有5个线程配置(包括2、4、8、16和32的 TpR)对整个数据集运行 CSR- vector。<font color='red'><b>记录每个矩阵100次 SpMV 迭代的平均运行时间</b></font>。每个矩阵都用对应于最短运行时间的 TpR 对应标记。图3总结了不同标签的矩阵分布情况。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240317172041.png)
<center> <font face='华文宋体' size='4'> 表2 用于模型训练的矩阵特征 </font> </center>
表2总结了我们在模型训练和测试中使用的特征集。最右边的一栏显示，所有特征对于给定的输入矩阵都很容易计算。<font color='red'><b>作为预处理开销的一部分，矩阵特征的计算必须尽可能简单，以免抵消最佳 TpR 设置的好处。</b></font>

AutoGluon 是来自 Amazon 的 AutoML 的一个开源实现，专注于自动堆栈装配、深度学习和现实世界的应用程序。AutoGluon 将多个模型组合在一起，形成一个新的多层混合模型。我们使用 AutoGluon 找到一个最佳的模型，可以适应我们的数据集。 ^2d373c

项目地址：[https://github.com/autogluon/autogluon](https://github.com/autogluon/autogluon)

📒：模型都没有自己进行搭建，直接使用的开源的！！！！！，绝了。

**训练**：为每个GPU准备数据集，其中所有选定的矩阵都表示为具有十一个特征的数据点。每个点都标记了在GPU平台上实现最佳性能的TpR。然后，对数据集中的所有数据点进行洗牌，并随机选择75%作为训练集，剩余25%作为测试集。

**测试**：测试集用于评估经过训练的模型的性能。相同的测试集用于两台机器以观察性能改进。

各种ML算法可用于解决多类分类问题，但找到最佳算法是耗时且成本高昂的。因此，我们使用AutoML工具同时运行我们的数据集与许多不同算法组合以找到最优算法。


## Blockwise thread configuration prediction

### 概述

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240317173452.png)
<center> <font face='华文宋体' size='4'> 图 4 两个例子矩阵的缩略图 </font> </center>
图4显示了两个稀疏矩阵的缩略图。这两个矩阵都有几十个长行和许多行短。具体来说，对于exdata\_1矩阵，每一个长行的NNZ大约为1,500个，而每一个短行的NNZ大约为3个。shermanACa矩阵中每一长行有大约2,500个非零元素，在每一短行中有6个非零元素。

如果分配给每一行的线程数量太少，则长行将不可避免地成为SpMV计算的 bottleneck。然而，当分配给每一行很多线程时，在计算短行时会出现几个空闲线程。CSR-vector在长和短列之间取得了折衷，但这对于长和短列都不是最佳选择。CSR-adaptive比CSR-vector更多地分配了线程来处理长列，但它在乘法运算中引入了更多共享内存访问和同步操作。因此，该算法仍可以改进以克服这种低效率。

针对这一挑战，本文<font color='red'><b>提出根据不同行中非零元素的分布情况对输入稀疏矩阵进行分区，然后预测不同块的最佳线程配置</b></font>。我们使用上述基于 ML 的方法来预测每个短行块的最佳线程配置。

### 矩阵划分

**由于对所有块使用CSR格式，因此上文提到的分区是虚拟进行的，只需要扫描一次CSR的行指针数组**。因此，<mark> <b>在设计中没有格式转换和数据重新组织</b></mark>。详细的分区算法在Algorithm 1中呈现。使用标量`nBlk`来表示已分区块数。 `rowBlk` 和 `nnzBlk` 是两个数组，分别存储每个块的行索引和非零元素数量。 使用位向量(bool类型，1比特的数) `isLong` 来区分长行和短行块。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240319101939.png)

<center> <font face='华文宋体' size='4'> 算法 1 关于矩阵块的划分算法 </font> </center>
考虑图4中展示的矩阵shermanACa作为例子，它被划分为三个块。 `isLong`= $(2)_{10}$ = $(010)_{2}$ 表示第一个和第三个块是两个短行块，而第二个是一个长行块。如果当前行很长，并且上一行很短，则是时候开始一个新的块（lines 10-15），反之亦然（lines 16-22）。<font color='red'><b>通过这种方式，具有相似非零元素分布的连续行被打包到一个块中</b></font>。


### 线程配置预测

本文的<font color='red'><b>训练数据集是通过切片常规稀疏矩阵生成的</b></font>。原因在于算法1中分区块比具有长行的原始块更加规则。为了找到适合切片块的最佳线程配置，我们多次使用不同𝑇𝑝𝑅运行SpMV。最后，我们生成了一个包括4,772个数据样本的训练集，这些样本从2,386个常规稀疏矩阵中切片而来。

![image.png|center|400](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240317175602.png)
<center> <font face='华文宋体' size='4'> 图 5 训练数据集中最佳线程配置的分布情况 </font> </center>
图5总结了训练数据集中最佳线程配置的分布情况。<font color='skyblue'><b>我们的测试数据集包含不规则稀疏矩阵，并且有326个由221个稀疏矩阵分割出来的短行块。</b></font>此外，用于全局线程配置预测的相同矩阵特征集被使用。我们还使用AutoML来找到最合适模型。


### SpMV kernel 优化

我们还设计了一种基于CSR-vector 和CSR-adaptive 的新SpMV内核，以提高性能。在分区后，每个块中非零元素的分布是规则的。因此，本文的主要想法是
- 使用CSR-vector 计算短行块，为每行分配固定数量的线程，并将中间结果缓存到寄存器中。
- 使用CSR-adaptive计算长行块，为每行分配一个线程块，并将中间结果缓存在共享内存中。
通过这种方式，在每个块中平衡工作负载，并且长行不会成为性能瓶颈。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240319103544.png)
<center> <font face='华文宋体' size='4'> 代码 1 ：新SpMV内核的代码 </font> </center>

新SpMV内核在代码1中给出。数组`nThdBlk`存储了**分配给每个矩阵块的起始和停止线程块索引**。在启动内核之前根据每个矩阵块中的NNZ和线程块总数进行计算。具体来说，如果块`i`是长行块，则 `nThdBlk[i]` 等于`rowBlk[i+1] - rowBlk[i]` 。
否则，使用下面的方程计算。

$$ nThdBlk[i]=max(\lfloor\frac{nnzBlk[i]\times(N\_BLKS-N\_BLKS_l)}{nnz-nnz_l}\rfloor,1)$$

$N\_BLKS$ 是线程块总数，$N\_BLKS_l$是分配给长行块的线程块数量，$nnz_{l}$是长行块中NNZ的总数。

本文的方法与 CSR-vector 的主要区别在于，<font color='red'><b>本文使用的 ML 算法来预测最优的TpR而不是经验公式</b></font>。此外，该方法考虑 NNZ 分布，并为块分配不同数量的线程。与 CSR-adaptive 相比，该方法使用固定数量的线程来处理短行块中的每一行，而不是使用线程块中的所有线程进行元素计算。它将中间结果保存在寄存器中，从而避免了大量的共享内存读写和块级同步。


## Performance evaluation

### 环境设置

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240319104549.png)
<center> <font face='华文宋体' size='4'> 表 3 实验环境设置 </font> </center>
以下所有实验均在两台配备不同 GPU 的机器上进行。表3列出了这些机器的硬件和软件设置，包括 CPU、主机内存、 GPU、设备内存、 OS、编译器和相关库。此外，表中还提供了比较算法或调用的接口。后续实验中评定的浮点精度为双精度。

### 全局线程配置预测

#### 模型准确性

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240319104937.png)
<center> <font face='华文宋体' size='4'> 表 4 ML 在两台机器上全局线程配置预测的准确性和总体性能 </font> </center>
本文的测试集包含953个稀疏矩阵，它们是从3.2节中提供的整个数据集中随机选取的，训练后的机器学习模型没有看到它们。准确度计算为测试集中正确预测矩阵的百分比。实验对数据集进行了四次重组，生成了四组训练和测试集。表4总结了两台机器的测试精度，并列出了实现 AutoGluon 所给出的最高测试精度的型号。如表中所示，在 V100和 P100上的四次洗牌中，最高的测试准确率分别约为85% 和94%

#### 性能比较

对于某些矩阵，虽然最优TpR不同于预测的TpR，但它们的相应性能可能相同或非常接近。<font color='red'><b>预测 TpR 的最终目标是提高 SpMV 的性能。</b></font>因此，在这一部分中，本文关注从训练后的机器学习模型中获得的性能。**本文参考了中用于评估机器学习模型性能的两个标准，即最佳状态下的性能损失(PLUB)和性能增益(PGO)**。设 $n$ 表示测试集中稀疏矩阵的个数，$t_p$ 表示具有预测 TpR 的 CSR- vector 的执行时间，$t_{s}$ 表示具有所有可能的 TpR 的 CSR- vector 的最短执行时间。给出了 PLUB 的计算方法为：

$$PLUB(\%)=100\times\frac1n\sum_{i=1}^n\frac{t_p^i-t_s^i}{t_s^i}$$
让 $t_k$ 代表具有TpR的执行时间，那么预测的TpR相对于$k$的性能增益，简称为 $𝑃𝐺𝑜(𝑘)$，计算如下：

$$PGO(k)(\%)=100\times\frac1n\sum_{i=1}^n\frac{t_k^i-t_p^i}{t_p^i}$$

表4显示了在两台机器上收集的性能结果:

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240319110230.png)
<center> <font face='华文宋体' size='4'> 表 4 性能结果 </font> </center>
基于ML的方法的PLUB在两台机器上最高为1.20%，这意味着具有ML预测线程配置的CSR向量实现了接近最大性能的99%。

CSR-vector 使用ML预测的线程配置，平均性能提升范围从18%到168%，而不仅仅是使用固定的TpR或基于经验公式的自适应TpR。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240319111343.png)
<center> <font face='华文宋体' size='4'> 图 6 基于机器学习的线程配置相对于其他配置在具有不同NNZ分布的稀疏矩阵上的平均性能增益 </font> </center>
在以下评估中，将讨论第一个打乱顺序的测试集。图6显示了基于机器学习的线程配置相对于其他配置，在具有不同NNZ分布的稀疏矩阵上的平均性能增益。

可以观察到，在前五个固定规则（TpR=2、4、8、16或32）中，基于机器学习的线程配置在具有适度NNZ（落入区间$(1E4,1E8]$) 的稀疏矩阵上比在极小（$≤1E4$）或极大矩阵($>1E8$) 上获得更高性能增益。

与Tesla V100相比，在Tesla P100上，ML-based thread configuration 对TpR为16、32和mean 带来更高性能收益。原因是Tesla V100提供了更多计算核心和硬件架构改进，如独立线程调度。因此，与Tesla P100 相比，在Tesla V100 上SpMV 的执行时间以及由不同线程配置引起的性能差异较小。根据方程式(5)，可以预期与Tesla P100 相比，在 Tesla V100 上PGO 会减少。此外，表5总结了三种线程配置在具有不同NNZ分布的矩阵上的平均PLUB 。可以观察到 TpR_mean 和 TpR_SqMean 的 PLUB 变化类似于它们PGO 的变化趋势。与最佳配置相比，基于机器学习的线程配置带来了相当小的性能损失。


![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240319114138.png)
<center> <font face='华文宋体' size='4'> 图 7 不同 PLUB 间隔下全局线程配置预测错误的分布 </font> </center>

在第一次洗牌中，ML 模型的准确率在两台机器上都没有超过85% ，但它的 PLUB 只有大约1% 。图7给出了预测误差的原因，并给出了预测误差在不同 PLUB 区间的分布情况。可以观察到，在两台机器上，大多数错误预测落在区间内(0,2% ] ，少数错误预测落在(2% ，6% ] ，而只有少数稀疏矩阵上的 PLUB 大于6% 。尽管 ML 模型在两台机器上对接近16% 和20% 的矩阵做出了错误的决策，但预测的非最优线程配置仍然达到了接近最优的性能。

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240319115310.png)


本文还将性能与其他四种流行的 SpMV 实现进行了比较，包括 Ginkgo ，ViennalCL ，cuSPARSE和基于 Merge 的 SpMV。在接下来的部分中，分别使用 ML 预测和最优线程配置来表示 CSR-vector。图8显示了它们在两台测试机上的性能比较。表6总结了它们对 CSR-vector的平均加速比，TpR 为16。本文还比较了每个算法表现出最佳性能的稀疏矩阵的数量，如表7所示。对这一系列实验的观察总结如下:
- 除了具有最佳线程配置的CSR-vector外，ML预测的线程配置在Tesla P100上实现了最高的整体性能。基于合并的SpMV对一些极端不规则稀疏矩阵（V100上的68个矩阵和P100上的45个矩阵）表现出良好性能。因此，在两个GPU上，其算术平均加速比高于几何平均加速比。在Tesla V100上，基于合并的SpMV实现了更高的算术平均加速比，但几何平均加速比较ML略小。
- 在Telsa V100上，cuSPARSE对某些稀疏矩阵优于ML，但后者实现了更好的整体性能。具体来说，在两台机器测试集中分别有259和128个稀疏矩阵时，cuSPARSE分别获得最佳性能。然而，在两台机器测试集中约48%和59%稀疏矩阵时，ML获得最佳性能。
- ML预测线程配置达到与最佳配置非常接近的性能水平。如表6所示，它们的算术和几何平均加速度显示出非常小差距，并且所有GPU设备之间差异都不到2%。

#### 块级线程配置预测

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240319120126.png)
<center> <font face='华文宋体' size='4'> 图 9 不同线程配置下全局和分块 SpMV 的性能比较 </font> </center>
为了研究矩阵划分带来的性能增益，首先测试了不同线程配置(包括 TpR 为16、 TpR_mean 和 TpR_SqAverage)下不进行矩阵划分的 CSR 向量的性能。然后，我们测试了新的 SpMV 算法的性能与不同的线程配置，包括 TpR_mean，TpR_Sqmean，ML，和最优。图9给出了它们的性能比较。

从上面可以观察到，本文提出的矩阵划分算法比非划分算法提供了显著的性能增益。原因是 CSR 向量分配给长行的线程数不超过32个，而 TpR_mean 和 TpR_Sqmean分配的线程数较少。因此，长排块的计算严重降低了 SpMV 的性能。**通过将稀疏矩阵划分为长行块和短行块，并给它们分配不同数量的线程，减轻了长行块中线程饥饿和短行块中线程闲置所造成的性能损失**。如图9所示，对于大多数矩阵，分块 TpR_mean 和 TpR_SqAverage 的表现优于全局 TpR_mean 和 TpR_SqAverage。此外，基于 ML 的线程配置实现了几乎所有矩阵的最优性能。表8总结了它们的平均加速速度。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240319120435.png)
<center> <font face='华文宋体' size='4'> 表 8 不同线程结构的 SpMV 在 TpR 为16的 CSR-vector上的平均加速比 </font> </center>

使用训练好的模型来预测短行块的最佳线程配置，并且在两台机器上的测试准确率如表9所示。这里，准确率是指PLUB在区间$[0,2\%]$ 内的矩阵比例，这是一种宽松的准确度。分区后每个短行块的大小小于原始矩阵，每个短行块中非零元素的分布更加规则，导致不同线程配置之间存在轻微性能差距。

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240319121110.png)
<center> <font face='华文宋体' size='4'> 表 9 两台机器上基于 ML 的分块线程配置预测的精度和整体性能 </font> </center>

例如，在预测线程配置和最优配置下，对于矩阵TSC\_OPF\_300，SpMV性能之间仅相差0.01%。因此，宽松准确度可以更好地评估训练好的ML模型性能。如表9所示，在Tesla V100上训练好的ML模型准确率为84%，在Tesla P100上为82%。此外，在两台机器上PLUB都没有超过2%。另外，表9显示了基于ML线程配置与其他线程配置平均PGO值，并且提出方法实现了比现有解决方案更好的性能。


![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240319121241.png)
<center> <font face='华文宋体' size='4'> 图 10 在不同的 PLUB 间隔中分块线程配置错误预测的分布 </font> </center>

图10总结了错误预测在不同 PLUB 间隔中的分布。可以观察到，在两台机器上，超过60% 的错误预测落在区间内(0,2% ] ，只有约10% 的错误预测落在区间内(10% ，100% ]。尽管本文提到的训练模型对某些矩阵给出了错误的预测，但是使用预测的线程配置会导致较小的性能损失。相比之下，对于 TpR\_mean 的线程配置，54% 和32% 的错误预测分别落在 Tesla V100和 P100的区间内(10% ，100% ]。使用 TpR\_Sqmean，53% 和52% 的错误预测分别落在两台机器上的区间(10% ，100%]内。

本文还将模型性能与几种流行的SpMV算法进行了比较。图11总结了在两台机器上测试集的结果。按照我们提出的方法的性能对实验结果进行了排序，以便清晰地进行比较。此外，表10给出了平均加速比，并列出每种算法达到最佳性能的矩阵数量在表11中列出。
![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240319121834.png)

总结如下观察点：

- 在这两台机器上，使用 ML 预测的线程配置的块状 SpMV 在几乎所有稀疏矩阵上都达到了接近最优的性能。此外，对于大多数矩阵，ML 算法的性能优于其他比较算法。
- 从表10可以看出，基于 Merge 的 SpMV 比我们在 Tesla V100上的方法获得了更高的算术平均加速比。由于严格的负载平衡，它在少量稀疏矩阵上表现出优异的性能。然而，该方法的几何平均加速比为2.64 x，高于基于合并的 SpMV 的2.05 x，表明该方法比基于合并的 SpMV 更加稳定和鲁棒。

## Discussion

###  更加深入的研究模型

LightGBM和XGBoost根据表4在两个平台上都实现了高准确性。由于<font color='red'><b>优秀的模型都是基于决策树（DTs）构建的集成模型</b></font>，通过实验发现单个DT也可以达到相当不错的准确性和整体性能。以Tesla P100上全局线程配置预测为例，训练了具有不同最大深度的DTs，观察测试集上性能变化。结果见图12。
![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240320172205.png)
<center> <font face='华文宋体' size='4'> 图 12 平均加速比随决策树最大深度的增加而变化 </font> </center>
可以观察到：
- (1) 最大深度为2的DT优于TpR\_mean和TpR\_SqMean；
- (2) 当最大深度大于5时，使用预测线程配置与使用最佳配置时的性能接近。

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240320172431.png)
<center> <font face='华文宋体' size='4'> 图 13 可视化的5层决策树，用绿色填充的节点表示由决策树决定的线程配置 </font> </center>

观察<font color='red'><b>稀疏矩阵的特征与其最佳线程配置之间的关系</b></font>，本文深入研究了5级决策树，图13展示了其可视化结果。可以观察到，当稀疏矩阵的每行最大NNZ（𝑚𝑎𝑥）大于278.5时，决策树给出的线程配置也相对较大，在大多数情况下为16或32，在少数情况下为8（当 𝑣𝑎𝑟 ≤ 2634.3298、 𝑑 ≤ 0.0001 和 𝑚𝑎𝑥_𝑜 ≤ 693.1261）。然而，DT根节点的左分支在大多数情况下倾向于选择相对较小的线程配置。另一个重要观察是5级DT不使用特征 𝑐, 这意味着在本文中研究的线程配置问题中, 特征 c 对于影响较小。 <font color='red'><b>根据这棵树, 稀疏矩阵最重要的特性是从根部到叶子节点依次为: 最大值、密度(非零元素)、方差(每行非零元素)、m、n、nnz和平均值。</b></font>
- 最大值（max）：每行非零元素的最大数目
- 密度（d）：非零元素的密度
- 方差（var）：每行非零元素的方差
- 行数（m）：矩阵的行数
- 列数（n）：矩阵的列数
- 非零元的数目（nnz）：矩阵中非零元的总数
- 平均值（mean）：每行非零元素的平均值

### 开销分析

本文还收集了使用不同线程配置方法的单精度CSR向量的性能。通过比较两种精度的结果，我们发现在Tesla V100和Tesla P100上分别有72%（2,747/3,811）和83%（3,151/3,811）稀疏矩阵具有相同的最佳线程配置。至于这些具有不同最佳TpR设置的稀疏矩阵，它们在两台机器上的性能差距均不超过2％。<font color='red'><b>这证明，在给定GPU上，当执行具有不同浮点精度的SpMV计算时，稀疏矩阵对线程配置具有类似偏好。因此，我们的模型可以应用于双精度和单精度计算。</b></font>至于硬件架构，只有少数几种主流GPU架构可能具有极其不同的结构特征，并因此对相同稀疏矩阵呈现出不同性能。例如，Volta架构首次引入了warp中各个线程之间的独立线程调度，这是一个非常重要的特征。<font color='red'><b>因此，本文建议为不同GPU建立单独模型。收集在不同线程配置下最优的SpMV性能条件大约需要两到三小时，并且模型训练仅需几分钟甚至几秒钟就可完成。</b></font>而且，在特定GPU上收集到数据并训练好模型后可以被所有使用相同GPU用户共享。这项工作可以由GPU制造商或第三方一次完成并供全球用户共享使用。基于以上理由，我们认为所提出解决方案是非常宝贵的。

![image.png|center|1000](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20240320173507.png)

在实际应用中，本文提出的方法包括两个方面的开销：**计算稀疏矩阵特征**和**使用训练模型预测最佳线程配置**。在测试集上的统计结果显示，计算矩阵特征的成本大约是一个具有32个𝑇𝑝𝑅的CSR-vector SpMV。至于模型推断的开销，本文观察到了基于LightGBM模型在不同参数配置下准确性和推断成本的变化。**本文使用m2cgen工具将训练好的模型导出为C代码，并收集其500次推理平均时间**。图14和图15展示了随着估计器数量和DTs最大深度变化时模型准确性和推理成本的变化情况。可以看到达到最高准确性的模型推理仅需数微秒。

在实际应用中，<font color='red'><b>m2cgen可以帮助将 ML 模型转换为 Java、 C、 Python、 Go、 JavaScript 等本地代码。因此，我们提出的基于机器学习的线程配置方案最终可以表示为一个允许用户方便地调用的函数。</b></font>**给定一个输入稀疏矩阵，我们首先计算它的特征，然后把这些特征作为函数的输入。该函数的输出是我们的模型预测的线程配置。最后，将预测的线程配置输入到 SpMV 内核中以引导线程分配。**


## Related work

### CSR-based SpMV 算法

由于格式转换可能在预处理中引入更多开销，近年来有许多研究人员致力于加速基于CSR的GPU上的SpMV。Reguly等人建议了两种CSR向量的线程配置。第一种是TpR_SqMean的线程配置。第二个是运行时调整规则。我们对2000多个稀疏矩阵进行的调查显示，具有最佳线程配置的SpMV平均约快3倍，比具有最差配置快高达20倍。这表明固定规则给出的不恰当TpR会导致迭代过程中持续性能损失。至于运行时调整规则，在初始迭代中选择不当甚至最差的TpR会显著影响整体性能。Greathouse等人提出了一个CSR- adaptive 算法来解决CSR-vector 负载不平衡问题。Ashari等人提出了一种自适应SpMV算法ACSR，以减少线程分歧和缓解负载不平衡问题。Merrill等人设计了一个基于合并的SpMV算法来解决SpMV中负载不平衡和大量预处理问题。它将基于CSR 的 SpMV 构架为两个列表（行指针和列索引数组）之间逻辑合并，并将合并均匀地分配给每个处理元素，从而保持严格的负载平衡状态。Flegar 等人提出了用于不规则稀疏矩阵的CSR-I，它还将工作负载均匀地分配给所有线程。他们提出了一种基于偏离均值比率的决策策略，以选择稀疏矩阵的不同SpMV 算法 。SURAA使用不同SpMV算法处理排序和分组子方阵，这些算法包括CSR-scalar、 CSR-vector（设置分配给每行的线程数大或者等于32）、以及动态内核（其基本组件是将32个线程分配到每行上的 CSR-vector）。此外,还在GPU上提出了基于CSR优化SpMV方法。

### ML-based 算法

SpMV 算法的性能取决于多种因素，包括输入稀疏矩阵的大小、非零元素的分布模式、采用的编码格式以及硬件设备的结构特点。现有的启发式方法不能完全解决所有这些问题，也没有一种算法可以完全规则矩阵。随着机器学习技术的不断发展和广泛应用，基于机器学习的 SpMV 优化问题逐渐引起了研究人员的关注，近年来已有大量的工作报道。

#### 格式/算法选择

随着越来越多的稀疏格式和 SpMV 算法被提出，研究人员提出了利用 ML 模型预测不同稀疏矩阵的合适格式以及相应的 SpMV 算法。Arm strong 等使用强化学习来确定最有效的稀疏格式。然而，在他们的工作中只考虑了三种稀疏矩阵压缩格式。Li 等和 Sedaghati 等提出使用基于决策树的模型来预测最佳格式。Benatia 等和 Mehrez 等使用多类 SVM 来选择最佳格式。Nisa 等测试了多种基本和集成的 ML 算法，包括 DT，SVM，多层感知器(MLP)和极端梯度提升(XgBoost)。由于稀疏矩阵的存在，赵等提出了利用深度学习模型寻找最佳格式的方法。考虑到矩阵特征提取、模型预测和格式转换所带来的开销可能会抵消某些应用场景中的好处，Shen 等人提出了一个两阶段的延迟和光预测方案。在第一阶段，他们使用两层长期短期记忆(LSTM)网络来预测 SpMV 迭代的总次数。只有当预测值大于给定的阈值时，才会调用第二阶段。使用 XGBoost 模型预测第二阶段的最佳压缩格式。Dufrechou 等用树分类器根据矩阵的特征集预测最佳方法，并分别考虑运行时间和能源消耗的两个标准来指示最佳方法。这些工作表明，选择合适的稀疏格式可以大大提高 SPMV 的性能。然而，将稀疏矩阵从一种格式转换为另一种格式是一个不可避免的问题，这必然会引入额外的格式转换开销。因此，我们可能不会从单一的 SpMV 计算或迭代算法与较少的迭代获得显着的性能增益。与此不同的是，Elafru 等使用基于 DT 的分类器来预测输入稀疏矩阵的主要性能瓶颈，然后选择相应的基于 CSR 的算法来执行 SpMV。

#### 性能预测

大规模的 SpMV 计算需要多个 GPU 的协同计算。为了设计一个有效的任务调度算法，我们需要知道每个矩阵分区的 SpMV 计算的执行时间。因此，SPMV 的性能预测具有重要意义。类似于格式选择，研究人员提出了各种基于机器学习的方法来预测 SpMV 的性能。Bena tia 等利用支持向量机(SVM)和 MLP 算法对不同稀疏格式的 SpMV 核函数的性能进行了预测。Nisa 等补充了其他 ML 模型，包括 XGBoost 和 MLP 集成。后来，Barreda 等利用 CNN 作为基本算法，设计了 SpMV 执行时间和能量消耗的离线估计器。

#### 参数配置预测

在一些 SpMV 算法中，参数设置对于算法的性能至关重要。例如，在使用分块或分片压缩格式的 SpMV 算法中，每个块或分片符号的大小显著影响压缩矩阵的大小和相应内核的性能。最佳参数设置随矩阵和硬件的不同而不同。在给定的硬件平台上，很难设计特定的规则来确定每个稀疏矩阵的最优参数设置。Ahmed 等提出了一个**基于 ML 的工具来预测 BCSR 最快的块大小设置**。他们使用不同的机器学习方法(包括决策树、随机森林、梯度提升、岭回归和 AdaBoost)来训练和测试这个工具。

#### 不同之处

与上面提到的基于 ML 的工作相比，我们的算法使用了最流行的 CSR 格式，并且不需要对稀疏矩阵进行任何繁重的格式转换。重点研究了基于 CSR 的 SpMV 算法的线程配置问题。据我们所知，我们的工作是第一次尝试将基于 ML 的方法应用于线程配置优化。**此外，现有的工作采取一个整体矩阵的线程配置，没有考虑非零元素的分布在每个稀疏矩阵。我们的分块方法将不规则矩阵分割成不同的块，这样我们就可以使用 ML 模型为每个块找到接近最优的线程配置**。

## Conclusion

提出了一种新的基于机器学习的 GPU 上 SpMV 线程分配策略。基于一个易于计算的特征集，它可以预测给定稀疏矩阵的近似最优线程配置。此外，提出了一种不规则稀疏矩阵的优化方法。**首先根据非零元素的分布将这些矩阵划分为多个块。然后，根据每个块的特点预测最优的线程配置**。我们还设计了一个新的 SpMV 内核来加速不同块的计算。我们对两种不同 GPU 架构的实验评估表明，该方法可以显著提高性能。最后，我们讨论了稀疏矩阵的特征与其最优线程配置之间的联系，并推导出一个在实际应用中只需花费几微秒就可以进行预测的轻量级推理模型。