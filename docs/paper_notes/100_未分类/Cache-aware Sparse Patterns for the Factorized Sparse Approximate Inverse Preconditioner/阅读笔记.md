# é˜…è¯»ç¬”è®°

è®ºæ–‡åŸæ–‡åœ°å€ï¼šâ€œCache-aware Sparse Patterns for the Factorized Sparse Approximate Inverse Preconditionerâ€ ([pdf](zotero://open-pdf/library/items/M7WDJ8JS?page=1&annotation=IJIXDC86))

## Abstract 

å¯¹äºæ±‚è§£è¯¥çº¿æ€§æ–¹ç¨‹$Ax=b$ï¼Œå½“$A$çŸ©é˜µæ˜¯å¯¹ç§°ä¸”æ­£å®šæ—¶ï¼Œé€šå¸¸çš„è§£å†³åŠæ³•æ˜¯ä½¿ç”¨ï¼šå…±è½­æ¢¯åº¦æ³•ï¼ˆConjugate Gradientï¼‰ ï¼Œå¯ä»¥ç®€ç§°ä¸ºCGã€‚

è€ŒCGçš„æ±‚è§£æ•ˆç‡å’ŒAçŸ©é˜µçš„ç‰¹å¾å€¼åˆ†å¸ƒæœ‰å…³ï¼Œä¸ºäº†åŠ é€Ÿè¯¥æ–¹ç¨‹çš„æ±‚è§£ï¼Œå¯ä»¥è€ƒè™‘å¯¹Aè¿›è¡Œçº¿æ€§å˜æ¢ï¼Œä½¿å¾—çŸ©é˜µAçš„ç‰¹å¾å€¼åˆ†å¸ƒçš„æ›´ä¸ºå¯†é›†ï¼Œè¿™é‡Œå°±å¼•å…¥äº†ä¸€ä¸ªæ–¹æ³•ï¼Œå°±æ˜¯ä½¿ç”¨æŸç§çŸ©é˜µå¯¹Aè¿›è¡Œçº¿æ€§å˜æ¢ï¼Œä»è€Œå¾—åˆ°æ›´ä¸ºå¯†é›†çš„ç‰¹å¾å€¼åˆ†å¸ƒã€‚ ^42dc2d

â€œPart of its effectiveness relies on finding a suitable pre-conditioner that accelerates its convergence.â€ ([Laut ç­‰, 2021, p. 1](zotero://select/library/items/WHYB98JZ)) ([pdf](zotero://open-pdf/library/items/M7WDJ8JS?page=1&annotation=4TW5ZLCR)) CGçš„æ•ˆç‡éƒ¨åˆ†å–å†³äºæ‰¾åˆ°ä¸€ä¸ªåŠ é€Ÿå…¶æ”¶æ•›çš„é€‚å½“é¢„å¤„ç†å™¨ã€‚ä¹Ÿå°±æ˜¯è¿™ä¸ªçŸ©é˜µ$M$ã€‚

â­ï¸å¯»æ‰¾è¿™ä¸ªçŸ©é˜µçš„æ–¹æ³•ï¼š**Factorized Sparse Approximate Inverse (FSAI) pre-conditioners** are a prominent and easily parallelizable option. ï¼Œ è¿™ä¸ªå’Œåˆ©ç”¨$A^{-1}$ è¿›è¡Œå˜æ¢çš„æ–¹æ³•ç±»ä¼¼ï¼Œä½†åœ¨è®¡ç®—æœºä¸Šè¿è¡Œæ›´ä¸ºå¿«é€Ÿä¹Ÿä¾¿äºè¿›è¡Œå¹¶è¡Œè®¡ç®—ã€‚å› ä¸ºè¿™ä¸ªä¸éœ€è¦è¿›è¡Œæ±‚é€†è¿ç®—ã€‚

â­ï¸è¿™ç¯‡æ–‡ç« ä¸»è¦è´¡çŒ®ï¼šâ€œwe introduce **complementary architecture-aware criteria** to increase the numerical effectiveness of the **pre-conditioner**. In particular, **we define cache-aware pattern extensions** that do not trigger additional cache misses when accessing vector ğ‘¥ in the ğ‘¦ = ğ´ğ‘¥ Sparse Matrix-Vector (SpMV) kernel.â€ ([Laut ç­‰, 2021, p. 1](zotero://select/library/items/WHYB98JZ)) ([pdf](zotero://open-pdf/library/items/M7WDJ8JS?page=1&annotation=D8N6NFT5)) â€æˆ‘ä»¬å¼•å…¥äº’è¡¥çš„é¢å‘æ¶æ„çš„æ ‡å‡†ï¼Œä»¥å¢åŠ é¢„æ¡ä»¶å™¨çš„æ•°å€¼æ•ˆæœã€‚ç‰¹åˆ«æ˜¯æˆ‘ä»¬å®šä¹‰äº†ç¼“å­˜æ„ŸçŸ¥çš„æ¨¡å¼æ‰©å±•ï¼Œåœ¨ä½¿ç”¨çŸ¢é‡xè®¿é—®y = Ax**ç¨€ç–çŸ©é˜µ-å‘é‡(SpMV)æ ¸**æ—¶ä¸è§¦å‘é™„åŠ çš„ç¼“å­˜æœªå‘½ä¸­ã€‚ ^1de814

	Sparse Matrix-Vector (SpMV) kernel æ˜¯æŒ‡æ‰§è¡Œç¨€ç–çŸ©é˜µå‘é‡ä¹˜æ³•çš„è®¡ç®—æ ¸å¿ƒã€‚å®ƒæ˜¯ä¸€ç§å¸¸è§çš„è®¡ç®—æœºç§‘å­¦å’Œæ•°å€¼è®¡ç®—ä¸­çš„æ“ä½œï¼Œç”¨äºå°†ç¨€ç–çŸ©é˜µä¸å‘é‡ç›¸ä¹˜ã€‚SpMV kernel çš„ç›®æ ‡æ˜¯é«˜æ•ˆåœ°æ‰§è¡Œè¿™ç§ä¹˜æ³•è¿ç®—ï¼Œä»¥ä¾¿åœ¨å¤„ç†å¤§è§„æ¨¡ç¨€ç–çŸ©é˜µæ—¶èƒ½å¤Ÿæä¾›è¾ƒå¿«çš„è®¡ç®—é€Ÿåº¦å’Œè¾ƒä½çš„å†…å­˜å ç”¨ã€‚SpMV kernel çš„å®ç°æ–¹å¼å¯ä»¥æ ¹æ®å…·ä½“çš„ç¡¬ä»¶æ¶æ„å’Œä¼˜åŒ–ç›®æ ‡è€Œæœ‰æ‰€ä¸åŒã€‚

- introduce **complementary architecture-aware criteria**
- define **cache-aware pattern extensions**

â­ï¸è®ºæ–‡çš„æ”¹è¿›æ•ˆæœï¼šâ€œAs a result, we obtain very significant reductions in terms of average solution time ranging between 12.94% and 22.85% on three different architectures - Intel Skylake, POWER9 and A64FX - over a set of 72 test matrices.â€ ([pdf](zotero://open-pdf/library/items/M7WDJ8JS?page=1&annotation=CHL8QXC6)) å› æ­¤ï¼Œæˆ‘ä»¬åœ¨ä¸‰ç§ä¸åŒæ¶æ„ï¼ˆIntel Skylakeã€POWER9å’ŒA64FXï¼‰ä¸Šå¯¹72ä¸ªæµ‹è¯•çŸ©é˜µè¿›è¡Œäº†è¯„ä¼°ï¼Œå¾—å‡ºäº†å¹³å‡è§£å†³æ—¶é—´çš„æ˜¾è‘—å‡å°‘ï¼ŒèŒƒå›´åœ¨12.94%è‡³22.85%ä¹‹é—´ã€‚

## Introduction

åœ¨æ±‚è§£çº¿æ€§æ–¹ç¨‹æ—¶
- â€œDirect methods like the sparse LU factorizations are not useful in this context due to their memory requirements and the significant number of steps they take.â€ (Laut ç­‰, 2021, p. 1) ğŸ”¤ç›´æ¥çš„æ–¹æ³•ï¼Œå¦‚ç¨€ç–LUåˆ†è§£ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ä¸å¤ªæœ‰ç”¨ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦å¤§é‡çš„å†…å­˜ï¼Œå¹¶ä¸”éœ€è¦ç›¸å½“å¤šçš„æ­¥éª¤ã€‚ğŸ”¤
- â€œThus, iterative methods are the best option and, in particular, Krylov methods are very commonly applied due to their excellent convergence properties.â€ (Laut ç­‰, 2021, p. 1) ğŸ”¤å› æ­¤ï¼Œè¿­ä»£æ–¹æ³•æ˜¯æœ€ä½³é€‰æ‹©ï¼Œç‰¹åˆ«æ˜¯ç”±äºå…¶ä¼˜å¼‚çš„æ”¶æ•›æ€§è´¨ï¼ŒKrylovæ–¹æ³•éå¸¸å¸¸ç”¨ã€‚ğŸ”¤

â­ï¸ç‰¹åˆ«çš„ï¼šå½“çº¿æ€§æ–¹ç¨‹$Ax=b$ä¸­çš„AçŸ©é˜µä¸ºå¯¹ç§°æ­£å®šçŸ©é˜µæ—¶ï¼Œâ€œWhen dealing with symmetric and positive definite matrices a popular Krylov method, Conjugate Gradient (CG), is typically applied.â€ (Laut ç­‰, 2021, p. 1) ğŸ”¤å½“å¤„ç†å¯¹ç§°å’Œæ­£å®šçŸ©é˜µæ—¶ï¼Œé€šå¸¸ä¼šä½¿ç”¨ä¸€ç§æµè¡Œçš„å…‹é‡Œæ´›å¤«æ–¹æ³•â€”â€”**å…±è½­æ¢¯åº¦æ³•ï¼ˆCGï¼‰**ã€‚ğŸ”¤

â€œThe performance of the SpMV is significantly influenced by irregular memory access patterns on ğ‘¥ driven by the locations of the sparse matrix non-zero coefficients.â€ ([Laut ç­‰, 2021, p. 1](zotero://select/library/items/WHYB98JZ)) ([pdf](zotero://open-pdf/library/items/M7WDJ8JS?page=1&annotation=4MU3PQ4H)) ğŸ”¤ç¨€ç–çŸ©é˜µå‘é‡ä¹˜æ³•ï¼ˆSpMVï¼‰çš„æ€§èƒ½å—**ç¨€ç–çŸ©é˜µéé›¶ç³»æ•°çš„ä½ç½®**æ‰€é©±åŠ¨ï¼Œè¿›è€Œæ˜¾è‘—å—åˆ°åœ¨ x ä¸Šçš„ä¸è§„åˆ™å†…å­˜è®¿é—®æ¨¡å¼å½±å“ã€‚ğŸ”¤ 
![[Cache-aware Sparse Patterns for the Factorized Sparse Approximate Inverse Preconditioner##^42dc2d]]


é™¤äº†æ¯ä¸ªå•ç‹¬æ ¸å¿ƒçš„æ€§èƒ½ç‰¹æ€§å¤–ï¼Œå¦ä¸€ä¸ªå¼ºçƒˆå½±å“CGè§£çº¿æ€§æ–¹ç¨‹ç»„å®¹é‡çš„å› ç´ æ˜¯çŸ©é˜µAçš„æ¡ä»¶æ•°ã€‚

åœ¨è¿™ä¸ªèƒŒæ™¯ä¸‹ï¼Œ**é¢„æ¡ä»¶å™¨é€šå¸¸ç”¨äºæ”¹å–„CGçš„æ”¶æ•›æ€§**ã€‚ä»ç®€å•çš„Block-Jacobi [34]åˆ°å¤æ‚çš„å¤šç½‘æ ¼æŠ€æœ¯[18]ï¼Œå·²æå‡ºå¤§é‡æ—¨åœ¨å®ç°æœ‰æ•ˆé¢„å¤„ç†çš„æ–¹æ³•ã€‚

ç¨€ç–è¿‘ä¼¼é€†(SAI)é¢„å¤„ç†å™¨åŒ…æ‹¬ä¸€ç§è¿‘ä¼¼çš„é€†çŸ©é˜µ$Mâ‰ˆA^{âˆ’1}$ï¼Œå¹¶å—åˆ°æŸç§ç¨€ç–æ¨¡å¼çš„é™åˆ¶[11, 12]ã€‚

â­ï¸éšåï¼Œæ±‚è§£ç­‰ä»·å’Œæ›´å¥½æ¡ä»¶çš„ç³»ç»Ÿ$MAx = Mb$ã€‚

å®é™…ä¸Šï¼Œåº”ç”¨SAIé¢„å¤„ç†å™¨åŒ…æ‹¬ä½¿ç”¨é¢å¤–çš„SpMVæ ¸ï¼Œä½¿å…¶é«˜åº¦å¹¶è¡ŒåŒ–ã€‚åœ¨CGçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œå¯¹ç§°æ­£å®šé—®é¢˜åº”ç”¨å› å¼åˆ†è§£ç¨€ç–è¿‘ä¼¼é€†(FSAI)ï¼Œè¿™æ„å‘³ç€é€šè¿‡å› å¼åˆ†è§£ $G^TG$ æ¥é€¼è¿‘ $A^{âˆ’1}$ï¼Œè€Œä¸æ˜¯å•ä¸€çŸ©é˜µã€‚

â­ï¸FSAIçš„ä¸€ä¸ªéå¸¸é‡è¦çš„æ–¹é¢æ˜¯å…¶**ç›¸åº”ç¨€ç–æ¨¡å¼çš„å®šä¹‰**ã€‚å°½ç®¡ç›®å‰æœ€å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆåªè€ƒè™‘æ•°å€¼æ–¹é¢ï¼Œä½†æœ¬æ–‡è¯æ˜ï¼Œåœ¨å®šä¹‰FSAIç¨€ç–æ¨¡å¼æ—¶ï¼Œä½å±‚æ¶æ„ç›¸å…³çš„æ¦‚å¿µä¹Ÿåº”è¯¥è¢«è€ƒè™‘è¿›å»ã€‚â€œWhile state-of-theart solutions define this pattern by exclusively taking into account numerical considerations, we demonstrate in this paper that low-level architecture-aware concepts should also be taken into account when defining the FSAI sparse pattern.â€([pdf](zotero://open-pdf/library/items/M7WDJ8JS?page=1&annotation=6CKVTN5P)) ğŸ”¤å½“ä»Šæœ€å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆä»…é€šè¿‡è€ƒè™‘æ•°å­—å› ç´ æ¥å®šä¹‰è¿™ä¸€æ¨¡å¼ï¼Œè€Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­å±•ç¤ºï¼Œ**å½“å®šä¹‰FSAIç¨€ç–æ¨¡å¼æ—¶ï¼Œè¿˜åº”è€ƒè™‘ä½çº§æ¶æ„æ„è¯†çš„æ¦‚å¿µ**ã€‚ğŸ”¤  

### æœ¬æ–‡çš„è´¡çŒ®

æœ¬æ–‡**æå‡ºå¹¶è¯„ä¼°äº†ä¸€ç§æ‰©å±•åŸºäºFSAIçš„ç¨€ç–æ¨¡å¼çš„æ–¹æ³•**ã€‚

è¯¥æ–¹æ³•åŸºäºä¸¤ä¸ªåŸºæœ¬æ¦‚å¿µï¼š
- é¦–å…ˆï¼Œ**ä¸€ç§ç¼“å­˜æ„ŸçŸ¥ç®—æ³•**ç”¨äºæ‰©å±•ç¨€ç–æ¨¡å¼ï¼Œä»è€Œé™ä½CGè¿­ä»£æ¬¡æ•°ï¼ŒåŒæ—¶ä¿æŒæ¯æ¬¡è¿­ä»£çš„æˆæœ¬è¾ƒä½ã€‚
	- è¿™ç§ç¼“å­˜æ„ŸçŸ¥ä¼˜åŒ–ä¾èµ–äºç¼“å­˜å±‚æ¬¡ä½“ç³»ç»“æ„çš„åº•å±‚ç»†èŠ‚ï¼Œå¦‚ç´¢å¼•æœºåˆ¶æˆ–è™šæ‹Ÿå†…å­˜ç®¡ç†æ–¹æ³•ã€‚
- å…¶æ¬¡ï¼Œä¸€ç§**è¿‡æ»¤ç¼“å­˜æ„ŸçŸ¥FSAIæ¨¡å¼æ‰©å±•ä¸­æœ€å°æ¡ç›®**çš„æ–¹æ³•ï¼Œè€Œä¸é™ä½å…¶æ”¶æ•›æ€§èƒ½ã€‚

æœ¬æ–‡ç›¸å¯¹äºç°æœ‰æŠ€æœ¯çš„è´¡çŒ®å¦‚ä¸‹ï¼š
1. We propose a technique to obtain cache-friendly FSAI sparse patterns. 
	- By considering some low-level aspects of the cache hierarchy architecture, **our algorithm is able to extend sparsity patterns in a way the number of iterations is reduced while the cost per iteration remains low enough to increase performance**. 
	- Our approach is architecture independent as it just requires the cache line size as architecture input.
2. We propose a robust approach to **filter out small entries of the inverse approximation** without degrading the numerical properties of the FSAI pre-conditioner.
3. We **evaluate our proposals via an extensive evaluation campaign considering 72 matrices of the SuiteSparse Collection** [13] that fit in the available memory resources of a single node. 
	- Our experiments consider three high-end systems: a 48-core Skylake machine, a 40-cores POWER9, and a 48-cores A64FX.
	- In Skylake, our approach reduces timeto-solution by 15.02% on average. 
	- In POWER9 and A64FX, these improvements are 12.94% and 22.85%, respectively.


## Background

### Conjugate Gradient ï¼ˆCGï¼‰

### Sparse Approximate Inverse Pre-conditionerï¼ˆSAIï¼‰

ç¨€ç–è¿‘ä¼¼é€†ï¼ˆSparse Approximate Inverseï¼ŒSAIï¼‰é¢„æ¡ä»¶å™¨åŸºäºé€†çŸ©é˜µä¸­æœ‰è®¸å¤šå¯ä»¥å¿½ç•¥çš„å°å…ƒç´ çš„å‡è®¾ï¼Œåªä¿ç•™æœ€å¤§çš„å…ƒç´ ï¼Œå¹¶å› æ­¤è¾¾åˆ°ç¨€ç–é€¼è¿‘çš„æœ‰æ•ˆæ€§ã€‚

åœ¨SAIæ–¹æ³•çš„è®¾ç½®è¿‡ç¨‹ä¸­ï¼Œå¯»æ‰¾ä¸€ä¸ªæ»¡è¶³å›ºå®šç¨€ç–æ¨¡å¼Sçš„é€†çŸ©é˜µçš„è¿‘ä¼¼$Mâ‰ˆA^{âˆ’1}$ã€‚è€ƒè™‘ç­‰æ•ˆä½†æ¡ä»¶æ›´å¥½çš„ç³»ç»Ÿ$MAx=Mb$ã€‚

â€œWhen dealing with symmetric and positive definite problems, to preserve the system symmetry, the Factorized Sparse Approximate Inverse (FSAI) preconditioner is applied and ğ´âˆ’1 is approximated by a factorization ğºğ‘‡ ğº instead of a single matrix ğ‘€, which means that two SpMV products are necessary instead of one. ğº is a sparse lower triangular matrix approximating the inverse of the Cholesky factor, ğ¿, of ğ´.â€ (Laut ç­‰, 2021, p. 2) 

ğŸ”¤å½“å¤„ç†å¯¹ç§°ä¸”æ­£å®šé—®é¢˜æ—¶ï¼Œä¸ºäº†ä¿æŒç³»ç»Ÿçš„å¯¹ç§°æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†Factorized Sparse Approximate Inverseï¼ˆFSAIï¼‰é¢„å¤„ç†å™¨ï¼Œå¹¶ä¸”ç”¨â­ï¸å› å¼åˆ†è§£ $G^TG$ æ¥é€¼è¿‘ $A^{âˆ’1}$ â­ï¸ï¼Œè€Œä¸æ˜¯å•ç‹¬çš„çŸ©é˜µMï¼Œè¿™æ„å‘³ç€éœ€è¦ä¸¤ä¸ªSpMVä¹˜ç§¯è€Œä¸æ˜¯ä¸€ä¸ªã€‚Gæ˜¯ä¸€ä¸ªç¨€ç–çš„ä¸‹ä¸‰è§’çŸ©é˜µï¼Œè¿‘ä¼¼è¡¨ç¤º**Açš„Choleskyåˆ†è§£å› å­Lçš„é€†**ã€‚$Gâ‰ˆL^{-1}$ğŸ”¤

![[Choleskyåˆ†è§£å› å­]]

å¦‚ä½•å¯»æ‰¾Gï¼š
$$ğ‘šğ‘–ğ‘›_{ğº âˆˆ S} ||ğ¼ âˆ’ ğºğ¿||^2_ğ¹ $$
å…¶ä¸­ï¼Œ$A=LL^T$ å’Œ $Gâ‰ˆL^{-1}$

â€œWe apply state-of-the-art techniques [11] to find ğº without explicitly evaluating ğ¿, i. e., only using the initial matrix ğ´.â€ (Laut ç­‰, 2021, p. 2) ğŸ”¤æˆ‘ä»¬ä½¿ç”¨æœ€å…ˆè¿›çš„æŠ€æœ¯[11]æ¥æ‰¾åˆ°æ²¡æœ‰æ˜ç¡®è¯„ä¼°Lçš„Gï¼Œå³åªä½¿ç”¨åˆå§‹çŸ©é˜µAã€‚ğŸ”¤

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20230811190257.png)

## Accelerating FSAI

â€œThis section describes a high-level view of our cache-aware sparse pattern extension strategy to boost the FSAI performance.â€ ([Laut ç­‰, 2021, p. 2](zotero://select/library/items/WHYB98JZ)) ([pdf](zotero://open-pdf/library/items/M7WDJ8JS?page=2&annotation=MIPAEIIK)) ğŸ”¤æœ¬èŠ‚æè¿°äº†æˆ‘ä»¬çš„**é«˜çº§ç¼“å­˜æ„ŸçŸ¥ç¨€ç–æ¨¡å¼æ‰©å±•ç­–ç•¥ï¼Œä»¥æé«˜FSAIçš„æ€§èƒ½**ã€‚ğŸ”¤

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20230811190947.png)


ç®—æ³•2æ˜¾ç¤ºäº†é€šè¿‡æ·»åŠ ä¸€ä¸ªæ­¥éª¤ï¼Œå³å¢åŠ **the cache-friendly extension of the sparse pattern**(ç¨€ç–æ¨¡å¼çš„ç¼“å­˜å‹å¥½æ‰©å±•)ï¼Œå¹¶é€šè¿‡ä¸€ä¸ª**æ›´å¤æ‚çš„è¿‡æ»¤å¤„ç†åº”ç”¨äºGçš„è¿‘ä¼¼é¢„è®¡ç®—ç»“æœ**ï¼Œæ¥æ›¿æ¢Gçš„ç­›é€‰å’Œé‡æ–°ç¼©æ”¾æ­¥éª¤çš„FSAIçš„é‡æ–°åˆ¶å®šã€‚

- **The step** added in line 3 **extends the sparse pattern** of ğº considering architecture-aware criteria **to add additional entries** that reduce the CG iteration count while incurring a minimal overhead in terms of iteration cost.
	- Note that we propose an extension of the sparse pattern, therefore **the set of matrices** considered in the Frobenius minimization problem of Equation 3 **increases**. Consequently, **the new inverse approximation is more accurate.**
	- åœ¨ç¬¬å››éƒ¨åˆ†è¿›è¡Œè§£é‡Š
- **The step** added in line 4 **replaces the filtration** that Algorithm 1 performs after the computation of ğº. 
	- In this new filtration strategy, entries of the sparse pattern are filtered out based on an approximate pre-calculation of ğº.
	- åœ¨ç¬¬äº”éƒ¨åˆ†è¿›è¡Œè§£é‡Š

## Cache-Friendly Fill-in

â€œIn this section we propose a **cache-friendly fill-in approach** to **extend the sparse pattern of the FSAI pre-conditioner**. We propose **architecture-aware techniques** to extend the sparse pattern in a way that we achieve significant reductions in terms of iteration count while minimizing the iteration cost overhead. In particular, we propose a method to extend the FSAI sparse pattern without increasing the number of cache misses.â€ ([Laut ç­‰, 2021, p. 3](zotero://select/library/items/WHYB98JZ)) ([pdf](zotero://open-pdf/library/items/M7WDJ8JS?page=3&annotation=KA4BIUZ4)) 

ğŸ”¤åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¼“å­˜å‹å¥½çš„å¡«å……æ–¹æ³•æ¥æ‰©å±•FSAIé¢„å¤„ç†å™¨çš„ç¨€ç–æ¨¡å¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¶æ„æ„ŸçŸ¥æŠ€æœ¯ï¼Œä»¥åœ¨å‡å°‘è¿­ä»£è®¡æ•°çš„åŒæ—¶æœ€å°åŒ–è¿­ä»£æˆæœ¬å¼€é”€çš„æ–¹å¼æ¥æ‰©å±•ç¨€ç–æ¨¡å¼ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨ä¸å¢åŠ ç¼“å­˜æœªå‘½ä¸­æ¬¡æ•°çš„æƒ…å†µä¸‹æ‰©å±•FSAIç¨€ç–æ¨¡å¼çš„æ–¹æ³•ã€‚ğŸ”¤

Since FSAI is applied via the SpMV kernel ğ‘¦ = ğ´ğ‘¥, we must consider the **access patterns** for all the involved data structures containing ğ‘¦, ğ´, and ğ‘¥.

- è®¿é—®AçŸ©é˜µï¼š
	- Assuming that we traverse the sparse matrix ğ´ **in row order** and that we **store it using the Compressed Sparse Row (CSR) format**, the accesses on the data structures containing ğ´ display a very simple stride-1 pattern. Since **this pattern is easily predictable** by hardware pre-fetchers, there is some flexibility for extending ğ´ without suffering a prohibitive performance penaltyã€‚
- å¯¹äºyï¼š
	- å’Œè®¿é—®Aç±»ä¼¼
- å¯¹äºxï¼š
	- The most problematic accesses are those coming from accesses to **vector ğ‘¥**, which follow a random pattern and thus **can not be easily predicted by the pre-fetcher**.
### Cache Alignment of Vector ğ‘¥

Our approach drives the extension of the FSAI sparse pattern by taking into account **the cache alignment of vector ğ‘¥.**

â€œIn other words, the idea is to add new coefficients in ğ´ that fully exploit the spatial locality on memory accesses to vector ğ‘¥.â€ ([Laut ç­‰, 2021, p. 3](zotero://select/library/items/WHYB98JZ)) ([pdf](zotero://open-pdf/library/items/M7WDJ8JS?page=3&annotation=Y6NHCKMD)) 

æ¢å¥è¯è¯´ï¼Œæ„æ€æ˜¯åœ¨çŸ©é˜µAä¸­æ·»åŠ æ–°çš„ç³»æ•°ï¼Œå……åˆ†åˆ©ç”¨å¯¹å‘é‡xçš„å†…å­˜è®¿é—®çš„ç©ºé—´å±€éƒ¨æ€§ã€‚ è¯¥æ–¹æ³•ä¾èµ–äºé€šè¿‡ä½¿ç”¨ç›¸åº”çš„è™šæ‹Ÿåœ°å€æ¥ç¡®å®šç¼“å­˜è¡Œä¸­å­˜å‚¨çš„å‘é‡å…ƒç´ xiçš„ç›¸å¯¹ä½ç½®

æˆ‘ä»¬å¯ä»¥é€šè¿‡è™šæ‹Ÿåœ°å€çš„åç§»ä½æ¥ç¡®å®šæŸä¸ªxiæ‰€åœ¨cacheè¡Œçš„ç›¸å¯¹ä½ç½®

æ¯”å¦‚ï¼š64Bçš„cacheè¡Œï¼Œå­˜å‚¨doubleå˜é‡ï¼ˆ8Bï¼‰ï¼Œå¯ä»¥å­˜å‚¨8ä¸ªï¼Œå½“æˆ‘ä»¬éœ€è¦ç¡®å®šxiæ‰€åœ¨cacheè¡Œçš„ç›¸å¯¹ä½ç½®æ—¶ï¼Œå°±å¯ä»¥å°†è™šæ‹Ÿåœ°å€mod8æ¥è¿›è¡Œè®¡ç®—ï¼Œä»è€Œå¾—å‡ºç›¸å¯¹ä½ç½®ã€‚è€Œå½“cacheè¡Œçš„å¤§å°å˜å¤§æ—¶ï¼Œæˆ‘ä»¬éœ€è¦åŠ¨æ€è°ƒæ•´è¿™ä¸ªmodçš„æ•°å€¼ï¼Œæ¯”å¦‚ï¼Œå½“cacheè¡Œå¤§å°å˜ä¸º256Bæ—¶ï¼Œå­˜å‚¨doubleå˜é‡ï¼Œå¯ä»¥å­˜å‚¨64ä¸ªï¼Œæ•…æ­¤æ—¶modçš„å¤§å°å°±åº”è¯¥ä¸ºï¼š64ã€‚

Our approach is **architecture independent**  æ¶æ„æ— å…³æ€§
å› ä¸ºï¼š
- i) it can be adapted to any cache line size by simply **adjusting this value** before applying the cache-friendly fill-in procedure;
- ii) it can be adapted to any cache indexing mechanism **using virtual addresses**; and, finally,
- iii) it can be applied to **any Instruction** Set Architecture (ISA).

### Algorithm for Cache-Friendly Fill-In

æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®—æ³•ï¼Œç”¨äºç”Ÿæˆä¸€ä¸ªé€šç”¨FSAIç¨€ç–æ¨¡å¼çš„ç¼“å­˜å‹å¥½çš„å¡«å……ã€‚

æˆ‘ä»¬ç®—æ³•çš„è¾“å…¥æ˜¯åˆå§‹ç¨€ç–æ¨¡å¼Så’Œå°†åœ¨[[Cache-aware Sparse Patterns for the Factorized Sparse Approximate Inverse Preconditioner##^1de814|SpVM]]æ“ä½œä¸­ä½¿ç”¨çš„æ•°ç»„xã€‚

![image.png|center|600](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20230823214637.png)


ç®—æ³•3æ˜¾ç¤ºäº†æˆ‘ä»¬çš„ç¼“å­˜å‹å¥½å¡«å……ç®—æ³•çš„ä¼ªä»£ç ã€‚

ä¸»å¾ªç¯è¿­ä»£åˆå§‹æ¨¡å¼è¡Œï¼Œå¹¶ä¸”å¯¹äºæ¯ä¸€è¡Œï¼Œéå†å…¶æ‰€æœ‰éé›¶æ¡ç›®ï¼ˆè¡Œ4-13ï¼‰ã€‚

å¯¹äºæ¯ä¸ªæ¡ç›®jï¼Œå®ƒé€šè¿‡ä½¿ç”¨ç¬¬4.1èŠ‚ä¸­æè¿°çš„è¿‡ç¨‹æ¥æ ‡è¯†ç›¸åº”çš„xjç»„ä»¶çš„ç¼“å­˜è¡Œï¼ˆè¡Œ9ï¼‰ï¼Œç„¶åé€šè¿‡æ’å…¥ä»¥å‰ä¸å­˜åœ¨çš„æ¡ç›®æ¥æ‰©å±•ç¨€ç–æ¨¡å¼ï¼Œè¿™äº›æ¡ç›®å¯¹åº”äºå­˜å‚¨åœ¨åŒä¸€ç¼“å­˜è¡Œä¸­çš„å‘é‡xçš„éƒ¨åˆ†ï¼ˆè¡Œ10å’Œ11ï¼‰ã€‚

è¯¥ç®—æ³•å¯ä»¥ä½¿ç”¨åŸºäºçº¿ç¨‹çš„æ–¹æ³•ï¼ˆå¦‚OpenMPæˆ–Posixçº¿ç¨‹ï¼‰è½»æ¾å¹¶è¡ŒåŒ–ã€‚

### Applying the Cache-Friendly Fill-In to FSAI

the FSAI pre-conditioner approximates $A^{-1}$ by the factorization $ğº^Tğº$

The sparse pattern of the original ğº matrix is extended using the $algorithm 3$ to obtain an extended ğºğ‘’ğ‘¥ğ‘¡ matrix.

â€œTherefore, when multiplying by ğºğ‘‡ ğ‘’ğ‘¥ğ‘¡ , the entries generated by the extension are accessed in consecutive rows. In conclusion, the spatial locality optimization for the ğºğ‘’ğ‘¥ğ‘¡ product results in a temporal locality optimization of the ğºğ‘‡ ğ‘’ğ‘¥ğ‘¡ product.â€ ([Laut ç­‰, 2021, p. 4](zotero://select/library/items/WHYB98JZ)) ([pdf](zotero://open-pdf/library/items/M7WDJ8JS?page=4&annotation=A9T4KEJU)) 

â­ï¸â­ï¸å› æ­¤ï¼Œå½“é€šè¿‡$G^T_{ext}$è¿›è¡Œä¹˜æ³•è¿ç®—æ—¶ï¼Œæ‰©å±•äº§ç”Ÿçš„æ¡ç›®æ˜¯æŒ‰è¿ç»­è¡Œè®¿é—®çš„ã€‚æ€»ä¹‹ï¼Œå¯¹äº$G_{ext}$ ä¹˜ç§¯çš„ç©ºé—´å±€éƒ¨æ€§ä¼˜åŒ–å¯¼è‡´äº† $G^T_{Ext}$ ä¹˜ç§¯çš„æ—¶é—´å±€éƒ¨æ€§ä¼˜åŒ–ã€‚

### Cache-Friendly Fill-In Process Example

![æˆªå±2023-09-01 11.09.35.png](https://jsd.cdn.zzko.cn/gh/NEUQer-xing/Markdown_images@master/images-2/%E6%88%AA%E5%B1%8F2023-09-01%2011.09.35.png)

## Filtering Out Small G Entries

ä¸ºäº†ä½¿å¾—å¾—åˆ°çš„è¿‘ä¼¼ç»“æœæ›´åŠ é«˜æ•ˆï¼Œä¸€ç§æ¯”è¾ƒå¸¸è§çš„æ–¹æ³•æ˜¯é€šè¿‡æ»¤é™¤ä¸€äº›åœ¨Gä¸­ç»å¯¹å€¼è¾ƒå°çš„æ¡ç›®ã€‚

ä½†è¿™ä¸ªæ–¹æ³•è™½ç„¶å¯ä»¥ä½¿å¾—è¿‘ä¼¼ç»“æœè®¡ç®—æ›´åŠ é«˜æ•ˆï¼Œä½†æ˜¯å®ƒåŒæ ·ä¹Ÿé™ä½äº†è¯¥å‰å¤„ç†å™¨çš„æ•°å­—è´¨é‡ã€‚

è€Œä¸”ï¼Œé€šè¿‡è¿‡æ»¤æ‰å°çš„æ¡ç›®ï¼Œè¯¥ç¨€ç–çŸ©é˜µçŠ¶æ€å³è¢«ä¿®æ”¹ï¼Œæ‰€ä»¥å¾—åˆ°çš„å¼å­å¹¶ä¸ä¸€å®šå¯ä»¥æœ€å°åŒ–å¼å­$ğ‘šğ‘–ğ‘›_{ğº âˆˆ S} ||ğ¼ âˆ’ ğºğ¿||^2_ğ¹$

æœ¬ç¯‡æ–‡ç« æå‡ºäº†ä¸€ä¸ªè¿‡æ»¤çš„æ–°çš„æ–¹æ³•ï¼Œæ–°æ–¹æ³•åŸºäºå¯¹Gçš„è¿‘ä¼¼é¢„å…ˆè®¡ç®—ã€‚To generate this low-cost approximation, we solve Eq. 3 ï¼ˆä¸Šé¢é‚£ä¸ªæœ€å°åŒ–çš„å¼å­ï¼‰via several iterations of the CG method with a relatively high tolerance to obtain an approximate solution.

è¿™ä¸ªcache-friendly extensionå’Œ the new filtration æ–¹æ³• å¯ä»¥ç”¨äºä»»ä½•ä¸€ä¸ªç»™å®šçš„ç¨€ç–çŠ¶æ€

åœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œåœ¨åˆ©ç”¨æœ¬æ–‡æåˆ°çš„ä¼˜åŒ–æ–¹æ³•å¯¹ç¨€ç–çŸ©é˜µè¿›è¡Œä¼˜åŒ–æ—¶ï¼ŒåŸºæœ¬çš„ä¼˜åŒ–è¿‡ç¨‹å¦‚ä¸‹ï¼š

1. åˆ©ç”¨cache-friendly æ–¹æ³•æ¥å¯¹ç»™å®šçš„ç¨€ç–æ¨¡å¼è¿›è¡Œæ‰©å±•ã€‚ æ­¤è¿‡ç¨‹å¯¹åº”äºç®—æ³•2çš„ç¬¬ä¸‰è¡Œã€‚
	- ![image.png|600](https://jsd.cdn.zzko.cn/gh/NEUQer-xing/Markdown_images@master/images-2/20230901161450.png)
2. å¯¹æ‰©å±•æ¨¡å¼ä¸Šçš„Gè¿›è¡Œé¢„è®¡ç®—
3. è¿‡æ»¤æ‰å…·æœ‰è¾ƒä½ç»å¯¹å€¼ç‰¹å¾çš„æ¡ç›®
4. ä½¿ç”¨$ğ‘šğ‘–ğ‘›_{ğº âˆˆ S} ||ğ¼ âˆ’ ğºğ¿||^2_ğ¹$æ–¹ç¨‹æ¥è®¡ç®—ä¸Šé¢æ­¥éª¤ç”Ÿæˆçš„ç¨€ç–æ¨¡å¼Gçš„ç³»æ•°ã€‚ è¯¥æ­¥éª¤å¯¹åº”äºç®—æ³•2çš„ç¬¬äº”è¡Œã€‚
	- ![image.png|600](https://jsd.cdn.zzko.cn/gh/NEUQer-xing/Markdown_images@master/images-2/20230901161450.png)

è¿™äº›æ­¥éª¤åº”ç”¨äºä¸€ä¸ª64x64çš„ç¨€ç–çŸ©é˜µã€‚è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
![image.png](https://jsd.cdn.zzko.cn/gh/NEUQer-xing/Markdown_images@master/images-2/20230901162624.png)

å·¦ä¾§å›¾åƒæ˜¾ç¤ºäº†åˆå§‹çŸ©é˜µçš„ä¸‹ä¸‰è§’éƒ¨åˆ†ã€‚

åœ¨ä¸­é—´çš„å›¾ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ¨¡å¼å¦‚ä½•åœ¨å‡è®¾ç¼“å­˜å¤§å°ä¸º64Bå’Œä»¥åŒç²¾åº¦å­˜å‚¨çš„æ¡ç›®ä¸‹æˆä¸ºç¼“å­˜å‹å¥½çš„æ‰©å±•ã€‚

å³ä¾§å›¾ç‰‡ä¸­çš„çº¢è‰²éƒ¨åˆ†æ˜¾ç¤ºäº†é«˜é€Ÿç¼“å­˜å‹å¥½æ‰©å±•çš„æœ€ç»ˆæ¡ç›®ï¼Œè€Œåˆå§‹æ¡ç›®ä»¥é»‘è‰²è¡¨ç¤ºã€‚

è¯·æ³¨æ„ï¼Œä¸åŒçš„ç¼“å­˜å¤§å°ä¼šå¯¼è‡´ä¸åŒçš„æ‰©å±•æ¨¡å¼ã€‚åœ¨è¿™ä¸ªæ‰©å±•æ¨¡å¼ä¸Šé¢„å…ˆè®¡ç®—å‡ºäº†ä¸€ä¸ªé€†è¿‘ä¼¼å€¼ã€‚åœ¨è¿™ä¸ªæ‰©å±•æ¨¡å¼ä¸Šé¢„å…ˆè®¡ç®—äº†åå‘é€¼è¿‘ã€‚

## Exploiting Spatial and Temporal Locality in the $G_P$ and $G^T_P$ Products

æœ¬èŠ‚ä¸»è¦ä»‹ç»åˆ©ç”¨ $Gp$ å’Œ $G^Tp$ æé«˜ç©ºé—´å±€éƒ¨æ€§å’Œæ—¶é—´å±€éƒ¨æ€§çš„æ–¹æ³•ã€‚

å‰é¢çš„éƒ¨åˆ†ä¸»è¦ä»‹ç»çš„æ˜¯å¦‚ä½•æ‰©å±•ç¨€ç–æ¨¡å¼çŸ©é˜µGä»è€Œå¾—åˆ°ä¸€ä¸ªæ‰©å±•çš„ç¨€ç–æ¨¡å¼çŸ©é˜µ$G_{ext}$ï¼Œè¿™ä¸ªæ‰©å±•çš„ç¨€ç–æ¨¡å¼çŸ©é˜µï¼Œåœ¨ä¸å‘é‡pè¿›è¡Œå‘é‡ä¹˜æ³•æ—¶å€™ï¼Œå¯¼è‡´ç¼“å­˜æœªå‘½ä¸­çš„æ¬¡æ•°ä¸ä¼šå¢åŠ ã€‚è¿™ä¸ªæ‰©å±•åŸºäºGçš„ç©ºé—´å±€éƒ¨æ€§è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶äº§ç”Ÿ$G^T$çš„æ—¶é—´å±€éƒ¨æ€§ä¼˜åŒ–ã€‚

æé«˜FSAIçš„æ—¶é—´å±€éƒ¨æ€§å’Œç©ºé—´å±€éƒ¨æ€§æ–¹æ³•å¦‚ä¸‹ï¼š
1. å°†åˆå§‹çš„ä¸‹ä¸‰è§’çŠ¶æ€ä¸‹çš„çŸ©é˜µSè¿›è¡Œcache-friendlyæ‰©å±•ï¼Œæ‰©å±•åˆ°ç¼“å­˜å‹å¥½çš„æ¡ç›®ï¼Œä»è€Œä¼˜åŒ–Gä¹˜ç§¯ä¸­å¯¹xçš„è®¿é—®ã€‚
2. æå‰è®¡ç®—æ‰©å±•æ¨¡å¼ä¸Šçš„è¿‘ä¼¼å€¼
3. è¿‡æ»¤æ‰æ‰©å±•ä½ç½®çš„æ¡ç›®ï¼Œä»è€Œå¾—åˆ°$S_{ext}$
4. å°†$(S_{ext})^T$ åˆ©ç”¨cache-friendlyè¿›è¡Œæ‰©å±•ï¼Œå¾—åˆ°æ‰©å±•æ¡ç›®ï¼Œä»è€Œä¼˜åŒ–$G^T$ä¹˜ç§¯ä¸­xçš„è®¿é—®ã€‚
5. æå‰è®¡ç®—æ‰©å±•åçš„è¿‘ä¼¼å€¼
6. è¿‡æ»¤æ‰æ‰©å±•ä½ç½®çš„æ¡ç›®ï¼Œä»è€Œå¾—åˆ°$((S_{ext})^T)_{ext}$
7. ä½¿ç”¨è½¬ç½®çš„ç¨€ç–çŠ¶æ€ï¼ˆæ¨¡å¼ï¼‰çŸ©é˜µ$((S_{ext})^T)_{ext}$ æ¥è®¡ç®—æœ€ç»ˆçš„$G$

å…·ä½“çš„ç®—æ³•æ­¥éª¤å¦‚ä¸‹æ‰€ç¤ºï¼š
![image.png|center|800](https://jsd.cdn.zzko.cn/gh/NEUQer-xing/Markdown_images@master/images-2/20230901190351.png)

## Evaluation

### å®éªŒè®¾ç½®

è¯„ä¼°è€ƒè™‘åˆ°äº†æœ€å…ˆè¿›çš„FSAIæ–¹æ³•ï¼Œä»¥åŠä½¿ç”¨ç¼“å­˜å‹å¥½æ–¹æ³•æ‰©å±•ç¨€ç–æ¨¡å¼çš„å¦å¤–ä¸¤ä¸ªé¢„å¤„ç†å™¨ã€‚

1. FSAI - Factorized Sparse Approximate Inverse pre-conditioner.
	- ![image.png|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20230904141947.png)
	- ä¸è¿›è¡Œé˜ˆå€¼å¤„ç†å¹¶ä¸”ä»…ä»…å¯¹ç©ºç™½ç›®å½•è¿›è¡Œè¿‡æ»¤
2. FSAIE(sp) - Factorized Sparse Approximate Inverse preconditioner with a sparse pattern extension exploiting spatial locality.
	- ![image.png|800](https://jsd.cdn.zzko.cn/gh/NEUQer-xing/Markdown_images@master/images-2/20230901190351.png)
	- åœ¨ç®—æ³•4ä¸­ä¸åŒ…å«ç¬¬5å’Œç¬¬6æ­¥éª¤
	- åœ¨ç¬¬ä¸€æ¬¡ä¹˜æ³•æ—¶åªç”¨åˆ°ç©ºé—´å±€éƒ¨æ€§ï¼Œåœ¨ç¬¬äºŒæ¬¡ä¹˜æ³•æ—¶ï¼Œç”¨åˆ°æ—¶é—´å±€éƒ¨æ€§
3. FSAIE(full) -Factorized Sparse Approximate Inverse preconditioner with pattern extension exploiting spatial and temporal locality.
	- ç®—æ³•4
	- åœ¨æ‰€æœ‰çš„ä¹˜æ³•ä¸­ï¼Œéƒ½è¦ç”¨åˆ°ç©ºé—´å±€éƒ¨æ€§å’Œæ—¶é—´å±€éƒ¨æ€§ã€‚

### Performance Improvement on Skylake

è¿™éƒ¨åˆ†ä»‹ç»äº†FSAIEï¼ˆspï¼‰å’ŒFSAIEï¼ˆfullï¼‰ç›¸å¯¹äºFSAIçš„æ€§èƒ½ã€‚

![æˆªå±2023-09-04 14.29.57.png](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/%E6%88%AA%E5%B1%8F2023-09-04%2014.29.57.png)

è¡¨1æ˜¾ç¤ºäº†Skylakeç³»ç»Ÿä¸­ä½¿ç”¨ä¸‰ç§æŠ€æœ¯å’Œ**è¿‡æ»¤å™¨= 0.01**çš„ç»“æœã€‚

ç¬¬6è‡³8åˆ—æŠ¥å‘Šäº†FSAIçš„è®¾ç½®æ—¶é—´ã€æ±‚è§£æ—¶é—´å’Œæ”¶æ•›æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°ã€‚

è¿™äº›æ˜¯æˆ‘ä»¬ç”¨æ¥æ¯”è¾ƒæˆ‘ä»¬çš„æ¨¡å¼æ‰©å±•æ–¹æ³•çš„SkylakeåŸºå‡†ç»“æœã€‚

ç¬¬9è‡³12åˆ—åˆ†åˆ«æŠ¥å‘Šäº†è®¾ç½®æ—¶é—´ã€æ±‚è§£æ—¶é—´ã€æ”¶æ•›è¿­ä»£æ¬¡æ•°ä»¥åŠFSAIEï¼ˆspï¼‰æ·»åŠ åˆ°Açš„ä¸‹ä¸‰è§’å½¢æ¨¡å¼ä¸­çš„æ¡ç›®ç™¾åˆ†æ¯”ã€‚ç¬¬13è‡³16åˆ—æŠ¥å‘Šäº†å…³äºFSAIEï¼ˆfullï¼‰çš„è¿™äº›ç›¸åŒåº¦é‡æŒ‡æ ‡ã€‚

â€œmany cases we observe both FSAIE(sp) and FSAIE(full) to successfully **decrease iteration count and solve tim**e. FSAIE(full) obtains larger pattern extensions than FSAIE(sp), which **produces larger iteration count and solution time decreases**.â€([pdf](zotero://open-pdf/library/items/M7WDJ8JS?page=7&annotation=E8NR63S8))

â€œIn all cases, our new filtration strategy **avoids convergence degradation** while providing a higher degree of robustness to the methodâ€  ([pdf](zotero://open-pdf/library/items/M7WDJ8JS?page=8&annotation=VWPIZAEW))

### Effects on Data Cache Misses and FLOPS

è¯¥éƒ¨åˆ†æè¿°äº†åœ¨Skylakeä¸Šä½¿ç”¨å…¶æ‰©å±•çš„ç¨€ç–æ¨¡å¼æ—¶ï¼ŒFSAIEï¼ˆfullï¼‰æ–¹æ³•åœ¨æ•°æ®ç¼“å­˜æœªå‘½ä¸­å’Œæ¯ç§’æµ®ç‚¹æ“ä½œï¼ˆflop/sï¼‰æ–¹é¢çš„ä¼˜åŠ¿ã€‚

FSAIEï¼ˆFULLï¼‰åœ¨ä¸¤ä¸ªæ–¹é¢ä¸Šæé«˜äº†æ•ˆç‡ï¼š
- é¦–å…ˆï¼Œé€šè¿‡æ‰©å±•é¢„å¤„ç†ç¨€ç–æ¨¡å¼æ¥å®ç°è¿­ä»£æ¬¡æ•°çš„å‡å°‘ï¼›
- å…¶æ¬¡ï¼Œä¿æŒä½¿ç”¨äº†æ¨¡å¼æ‰©å±•ä¹‹åå¸¦æ¥çš„é¢å¤–è¿­ä»£æˆæœ¬ä»ç„¶å¤„äºè¾ƒä½æ°´å¹³ã€‚

7.2èŠ‚å±•ç¤ºäº†ç›¸å¯¹äºSkylakeç³»ç»Ÿä¸­çš„FSAIè€Œè¨€ï¼ŒFSAIE(full)**åœ¨è§£å†³æ—¶é—´å’Œè¿­ä»£æ¬¡æ•°æ–¹é¢**çš„ä¼˜åŠ¿ã€‚

è¿™ä¸€èŠ‚å±•ç¤ºäº†FSAIE(full)åœ¨ç¬¬äºŒä¸ªæ–¹é¢çš„æ•ˆæœã€‚

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20230904153901.png)

â€œThese results clearly indicate how our cache-friendly sparse pattern extensions successfully **minimize the average data cache misses per ğº non-zero entry**.â€

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20230904153917.png)

â€œFigure 4 clearly shows how the cache-aware extensions of the FSAIE(full) method significantly **improve the floating-point throughput achieved** by the sparse patterns of baseline FSAI.â€

### Setup Phase Overhead

å°½ç®¡è¯¥æ–‡ç« æå‡ºçš„æ–¹æ³•æ˜¾è‘—åŠ é€Ÿäº†å…±è½­æ¢¯åº¦æ³•çš„æ±‚è§£é˜¶æ®µï¼Œä½†ç›¸å¯¹äºFSAIï¼Œåœ¨ç¨€ç–æ¨¡å¼çš„æ‰©å±•é˜¶æ®µä¼šå¸¦æ¥ä¸€äº›**é¢å¤–å¼€é”€**ï¼Œè¿™ä¸»è¦æ˜¯ç”±äº**è®¡ç®—GçŸ©é˜µæ¡ç›®çš„æˆæœ¬æ›´é«˜**ã€‚

**Such overhead becomes negligible in a practical numerical simulation context**
- since the **setup phase** is performed **only once** while the **solve phase is repeated** several times for the same matrix. 
- Furthermore, even when the setup is to be repeated on each time step, some of its parts, such as the definition of the final pattern, do not need to be repeated on each time step.

### Evaluation on POWER9

![æˆªå±2023-09-04 16.02.33.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/%E6%88%AA%E5%B1%8F2023-09-04%2016.02.33.png)

â€œFor most of matrices ğ‘“ ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿ = 0.01 is the best option.â€ ([Laut ç­‰, 2021, p. 10](zotero://select/library/items/WHYB98JZ)) ([pdf](zotero://open-pdf/library/items/M7WDJ8JS?page=10&annotation=9LC3CJ79))

### Evaluation on A64FX

![image.png|center|800](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20230904160715.png)

è¿™äº›**å¤§å¹…åº¦çš„è¿­ä»£æ¬¡æ•°å‡å°‘**å¸¦æ¥äº†æ˜æ˜¾çš„æ€§èƒ½æ”¹å–„ã€‚

FSAIEï¼ˆå®Œå…¨ï¼‰æ–¹æ³•åœ¨filter = 0.01æ—¶å¸¦æ¥äº†å¹³å‡æ€§èƒ½æå‡20.08ï¼…ï¼Œä½¿ç”¨æœ€ä½³filter per matrixæ—¶ä¸º22.85ï¼…ã€‚

ä¸è¿‡æ»¤ä»»ä½•æ¡ç›®ï¼Œå³filter = 0.0ï¼Œæ€§èƒ½ä¼šä¸‹é™ï¼Œå› ä¸º**è¿­ä»£æˆæœ¬å¢åŠ è¶…è¿‡äº†è¿­ä»£æ¬¡æ•°çš„å‡å°‘**ã€‚

We show the results of the best performing ğ‘“ ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿ value (blue columns) for each matrix and the results for the best general ğ‘“ ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿ value (orange columns), **which is 0.01**. Many matrices display much larger performance improvements on A64FX than POWER9 and Skylake.

### Comparing Results on Different Architectures

![image.png](https://cdn.jsdelivr.net/gh/NEUQer-xing/Markdown_images@master/images-2/20230904161403.png)


è™½ç„¶ Skylake å’Œ POWER9 çš„ç»“æœæ˜¾ç¤ºå‡ºç±»ä¼¼çš„è¶‹åŠ¿ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰åŸºæœ¬ç›¸åŒçš„æ¨¡å¼æ‰©å±•ï¼Œä½†å½“ FSAIE (full) åº”ç”¨äº A64FX æ—¶ï¼Œå®ƒè·å¾—äº†æ›´ä¸°å¯Œçš„ç¨€ç–æ¨¡å¼ï¼Œè¿™æ˜¾è‘—æé«˜äº†å®éªŒé›†ä¸­æ‰€æœ‰çŸ©é˜µçš„å¹³å‡æ”¹å–„ã€‚

è¿™æ˜¯å› ä¸º A64FX çš„ç¼“å­˜è¡Œå¤§å°æ¯” Skylake æˆ– POWER9 å¤§4å€ã€‚

## Related work on approximate inverse methods

æœ‰å‡ ç§å…ˆå‰æå‡ºçš„æ–¹æ³•æ¥ä¸ºç¨€ç–è¿‘ä¼¼é€†é—®é¢˜ç”Ÿæˆæ¨¡å¼ã€‚æ ¹æ®ç¨€ç–è¿‘ä¼¼é€†çš„æ¨¡å¼å¦‚ä½•è¯„ä¼°ï¼Œå®ƒä»¬è¢«è®¤ä¸ºæ˜¯é™æ€æˆ–åŠ¨æ€æ–¹æ³•ã€‚

### Static approach

å¯¹äºé™æ€æ–¹æ³•ï¼ˆæœ¬æ–‡è€ƒè™‘çš„æ–¹æ³•ï¼‰ï¼Œæ¨¡å¼æ˜¯äº‹å…ˆç¡®å®šçš„ï¼Œå¹¶åœ¨é€†è¿‘ä¼¼è®¡ç®—æœŸé—´ä¿æŒä¸å˜ï¼Œæ— è®ºæ˜¯Mè¿˜æ˜¯Gçš„åˆ†è§£æ–¹æ³•ã€‚

å¸¸è§çš„ä¸€ç§æ˜¯ä½¿ç”¨çŸ©é˜µAçš„ç¨€ç–æ¨¡å¼çš„å¹‚æ¬¡æ–¹ï¼Œé€šå¸¸æ˜¯ $A^2$ æˆ– $^A3$ (10, 16, 20)ã€‚

å…¶ä»–æŠ€æœ¯ä¼šé‡æ–°å¡‘é€ åˆå§‹æ¨¡å¼(23)ã€‚

é€šè¿‡**é˜ˆå€¼åŒ–**å’Œ**åå¤„ç†è¿‡æ»¤**æˆ–**è‡ªé€‚åº”å…¥å£ä¸¢å¼ƒç­–ç•¥**(5, 6, 11, 15, 27, 29)å¯ä»¥å°†ç”Ÿæˆçš„æ¨¡å¼å˜ä¸º**ç¨€ç–æ¨¡å¼**ã€‚

å¯»æ‰¾æœ€ä½³çš„é˜ˆå€¼åŒ–å’Œè¿‡æ»¤å‡†åˆ™é€šå¸¸æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚

### Dynamic approximate inverse methods

åŠ¨æ€çš„è¿‘ä¼¼é€†æ–¹æ³•é€šè¿‡è‡ªé€‚åº”ç¨‹åº**ä»åˆå§‹çŒœæµ‹ï¼ˆä¾‹å¦‚å¯¹è§’çº¿æ¨¡å¼ï¼‰å¼€å§‹ï¼Œå¹¶ä½¿ç”¨æŸç§ç­–ç•¥æ‰©å¤§è¯¥æ¨¡å¼ï¼Œç›´åˆ°æ»¡è¶³ç‰¹å®šçš„æ ‡å‡†**ï¼Œä»è€Œè®¡ç®—å‡ºé€†æ¨¡å¼ã€‚

An example of a dynamic approach, SPAI, was proposed by Grote and Huckle [17]. There is also a factorized formulation, FSPAI [21]. Other dynamic strategies have been developed more recently, such as, its generalization to block form, BSAI [22], and others like PSAI and RSAI [25, 26].

é€šå¸¸ï¼ŒåŠ¨æ€è¿‘ä¼¼é€†æ¯”å…¶é™æ€çš„æ–¹æ³•æ›´ä¸ºæœ‰æ•ˆã€‚ç„¶è€Œï¼Œé«˜æ•ˆåœ°å°†å®ƒä»¬å¹¶è¡ŒåŒ–å¹¶ä¸”å®ƒä»¬çš„é¢„å¤„ç†é˜¶æ®µé€šå¸¸æ¯”é™æ€æ–¹æ³•æ›´æ˜‚è´µå¹¶éæ˜“äº‹ã€‚

### æ€»è¿°

æ‰€æœ‰å¼•ç”¨çš„è¿‘ä¼¼é€†æ–¹æ³•ï¼Œæ— è®ºæ˜¯é™æ€çš„è¿˜æ˜¯åŠ¨æ€çš„ï¼Œå…±åŒçš„ä¸€ä¸ªå› ç´ æ˜¯å®ƒä»¬éƒ½æ²¡æœ‰è€ƒè™‘åˆ°å‡†åˆ™æ¥å®šä¹‰ç¨€ç–æ¨¡å¼ã€‚

åŸºäºç¼“å­˜å‹å¥½æ¨¡å¼æ‰©å±•çš„æ¦‚å¿µï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸æåˆ°çš„ä»»ä½•æ›¿ä»£æ–¹æ³•äº’è¡¥ã€‚æœ€å¸¸è§çš„é™æ€æ–¹æ³•FSAIåœ¨è¿™é‡Œè¢«ç”¨ä½œå‚è€ƒã€‚ç„¶è€Œï¼Œæ— è®ºä½¿ç”¨ä½•ç§å…¶ä»–å…·æœ‰æ•°å€¼è¯„ä¼°æ ‡å‡†çš„æ¨¡å¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•éƒ½èƒ½å¸¦æ¥æ½œåœ¨çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚

## Conclusions

This paper demonstrates the benefits of a **FSAI sparse pattern extension** based on two fundamental concepts:
- ä¸€ä¸ªèƒ½å¤Ÿç”Ÿæˆç¼“å­˜æ„ŸçŸ¥çš„ç¨€ç–æ¨¡å¼æ‰©å±•çš„ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥æ˜¾è‘—é™ä½CGè¿­ä»£æ¬¡æ•°å¹¶ä¸”å¸¦æœ‰è¾ƒä½è¿­ä»£æ—¶é—´å¼€é”€ã€‚
- ä¸€ä¸ªå¼ºå¤§çš„è¿‡æ»¤ç­–ç•¥ï¼Œæœ€å¤§ç¨‹åº¦åœ°åˆ©ç”¨äº†ç¼“å­˜æ„ŸçŸ¥æ‰©å±•çš„å¥½å¤„ã€‚

å°½ç®¡æœ€å…ˆè¿›çš„æ–¹æ³•**ä»…åŸºäºæ•°å€¼è€ƒè™‘æ¥å®šä¹‰ç¨€ç–æ¨¡å¼**ï¼Œä½†æœ¬æ–‡é¦–æ¬¡å±•ç¤ºäº†è€ƒè™‘è®¡ç®—æœºä½“ç³»ç»“æ„æ¦‚å¿µçš„å¥½å¤„ã€‚

å®é™…ä¸Šï¼Œæ‰€æå‡ºçš„ç¼“å­˜æ„ŸçŸ¥æ¨¡å¼æ‰©å±•æ˜¯å¯¹ç”¨äºå®šä¹‰ç¨€ç–æ¨¡å¼çš„ä»»ä½•æ•°å€¼ç­–ç•¥çš„è¡¥å……ã€‚
